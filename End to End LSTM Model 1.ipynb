{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "WARNING:tensorflow:From c:\\users\\edwar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-d33199a52712>:216: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.278865, Accuracy = 0.21266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.7628653049468994, Accuracy = 0.17102137207984924\n",
      "Training iter #30000:   Batch Loss = 1.282398, Accuracy = 0.7946666479110718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6525483131408691, Accuracy = 0.6338649392127991\n",
      "Training iter #60000:   Batch Loss = 0.768922, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.136003017425537, Accuracy = 0.6674584150314331\n",
      "Training iter #90000:   Batch Loss = 0.734059, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.32218074798584, Accuracy = 0.6796742677688599\n",
      "Training iter #120000:   Batch Loss = 0.643850, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.9977433681488037, Accuracy = 0.7488971948623657\n",
      "Training iter #150000:   Batch Loss = 0.616332, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.0734260082244873, Accuracy = 0.7577196955680847\n",
      "Training iter #180000:   Batch Loss = 0.594011, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.2092132568359375, Accuracy = 0.7421106100082397\n",
      "Training iter #210000:   Batch Loss = 0.573314, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.241879940032959, Accuracy = 0.7417712807655334\n",
      "Optimization Finished!\n",
      "Training time is: 178.23791074752808 seconds\n",
      "FINAL RESULT: Batch Loss = 2.224536418914795, Accuracy = 0.7421106100082397\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.901990, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.604672908782959, Accuracy = 0.11265693604946136\n",
      "Training iter #30000:   Batch Loss = 1.306291, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4752986431121826, Accuracy = 0.7024092078208923\n",
      "Training iter #60000:   Batch Loss = 0.905766, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4482555389404297, Accuracy = 0.7590770125389099\n",
      "Training iter #90000:   Batch Loss = 0.745938, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.275655746459961, Accuracy = 0.8109942078590393\n",
      "Training iter #120000:   Batch Loss = 0.695433, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4219043254852295, Accuracy = 0.8316932320594788\n",
      "Training iter #150000:   Batch Loss = 0.678846, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4523353576660156, Accuracy = 0.8076009750366211\n",
      "Training iter #180000:   Batch Loss = 0.670320, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.33436119556427, Accuracy = 0.8350865244865417\n",
      "Training iter #210000:   Batch Loss = 0.632639, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3556742668151855, Accuracy = 0.84764164686203\n",
      "Training iter #240000:   Batch Loss = 0.610388, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3862411975860596, Accuracy = 0.8432304263114929\n",
      "Training iter #270000:   Batch Loss = 0.733399, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1535556316375732, Accuracy = 0.7964031100273132\n",
      "Training iter #300000:   Batch Loss = 0.620877, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2687256336212158, Accuracy = 0.8252460360527039\n",
      "Training iter #330000:   Batch Loss = 0.599768, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2669486999511719, Accuracy = 0.8344078660011292\n",
      "Training iter #360000:   Batch Loss = 0.577930, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2505507469177246, Accuracy = 0.8398371338844299\n",
      "Training iter #390000:   Batch Loss = 0.557007, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2509894371032715, Accuracy = 0.8364438414573669\n",
      "Training iter #420000:   Batch Loss = 0.559729, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.259810209274292, Accuracy = 0.8483203053474426\n",
      "Optimization Finished!\n",
      "Training time is: 313.982515335083 seconds\n",
      "FINAL RESULT: Batch Loss = 1.2638863325119019, Accuracy = 0.8425517678260803\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.046444, Accuracy = 0.2473333328962326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.642885208129883, Accuracy = 0.26433661580085754\n",
      "Training iter #30000:   Batch Loss = 1.329294, Accuracy = 0.6986666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3952462673187256, Accuracy = 0.6565999388694763\n",
      "Training iter #60000:   Batch Loss = 0.976545, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2651147842407227, Accuracy = 0.777061402797699\n",
      "Training iter #90000:   Batch Loss = 0.998760, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1617639064788818, Accuracy = 0.7930098176002502\n",
      "Training iter #120000:   Batch Loss = 0.771819, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.160566806793213, Accuracy = 0.8340685367584229\n",
      "Training iter #150000:   Batch Loss = 0.724173, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1882057189941406, Accuracy = 0.8340685367584229\n",
      "Training iter #180000:   Batch Loss = 0.747934, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1773934364318848, Accuracy = 0.8198167681694031\n",
      "Training iter #210000:   Batch Loss = 0.657285, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.139601230621338, Accuracy = 0.8473023176193237\n",
      "Training iter #240000:   Batch Loss = 0.645894, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.11631178855896, Accuracy = 0.8629114627838135\n",
      "Training iter #270000:   Batch Loss = 0.599876, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1413590908050537, Accuracy = 0.8595181703567505\n",
      "Training iter #300000:   Batch Loss = 0.627541, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.148330569267273, Accuracy = 0.8608754873275757\n",
      "Training iter #330000:   Batch Loss = 0.577598, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0835700035095215, Accuracy = 0.8659653663635254\n",
      "Training iter #360000:   Batch Loss = 0.573319, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2141517400741577, Accuracy = 0.8249067068099976\n",
      "Training iter #390000:   Batch Loss = 0.666384, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9721777439117432, Accuracy = 0.8601968288421631\n",
      "Training iter #420000:   Batch Loss = 0.532200, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1326411962509155, Accuracy = 0.8520529270172119\n",
      "Training iter #450000:   Batch Loss = 0.565804, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0329461097717285, Accuracy = 0.8751272559165955\n",
      "Training iter #480000:   Batch Loss = 0.519753, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0412356853485107, Accuracy = 0.876823902130127\n",
      "Training iter #510000:   Batch Loss = 0.501738, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0604711771011353, Accuracy = 0.861214816570282\n",
      "Training iter #540000:   Batch Loss = 0.513341, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0413837432861328, Accuracy = 0.8693586587905884\n",
      "Training iter #570000:   Batch Loss = 0.466959, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1188960075378418, Accuracy = 0.8618934750556946\n",
      "Training iter #600000:   Batch Loss = 0.495720, Accuracy = 0.9559999704360962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 1.043666124343872, Accuracy = 0.8625721335411072\n",
      "Training iter #630000:   Batch Loss = 0.485018, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9554168581962585, Accuracy = 0.8676620125770569\n",
      "Training iter #660000:   Batch Loss = 0.440648, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9751615524291992, Accuracy = 0.8724126219749451\n",
      "Optimization Finished!\n",
      "Training time is: 539.417266368866 seconds\n",
      "FINAL RESULT: Batch Loss = 0.9736613631248474, Accuracy = 0.873430609703064\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.642318, Accuracy = 0.19333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3602850437164307, Accuracy = 0.2422802895307541\n",
      "Training iter #30000:   Batch Loss = 1.216116, Accuracy = 0.7586666941642761\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6336779594421387, Accuracy = 0.6352222561836243\n",
      "Training iter #60000:   Batch Loss = 0.934565, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4956045150756836, Accuracy = 0.7543264627456665\n",
      "Training iter #90000:   Batch Loss = 0.906763, Accuracy = 0.8773333430290222\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5457671880722046, Accuracy = 0.7913131713867188\n",
      "Training iter #120000:   Batch Loss = 0.763524, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4645920991897583, Accuracy = 0.8164234757423401\n",
      "Training iter #150000:   Batch Loss = 0.654805, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4382667541503906, Accuracy = 0.826603353023529\n",
      "Training iter #180000:   Batch Loss = 0.616510, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4129526615142822, Accuracy = 0.8428910970687866\n",
      "Training iter #210000:   Batch Loss = 0.689186, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3086435794830322, Accuracy = 0.8357651829719543\n",
      "Training iter #240000:   Batch Loss = 0.653278, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2355127334594727, Accuracy = 0.8649473786354065\n",
      "Training iter #270000:   Batch Loss = 0.618280, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2123677730560303, Accuracy = 0.8720732927322388\n",
      "Training iter #300000:   Batch Loss = 0.569023, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.238598108291626, Accuracy = 0.8724126219749451\n",
      "Training iter #330000:   Batch Loss = 0.535209, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2495903968811035, Accuracy = 0.8673226833343506\n",
      "Training iter #360000:   Batch Loss = 0.545581, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2550437450408936, Accuracy = 0.8642687201499939\n",
      "Training iter #390000:   Batch Loss = 0.567453, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2726811170578003, Accuracy = 0.8618934750556946\n",
      "Training iter #420000:   Batch Loss = 0.538863, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2489378452301025, Accuracy = 0.8598574995994568\n",
      "Training iter #450000:   Batch Loss = 0.500341, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1826545000076294, Accuracy = 0.8693586587905884\n",
      "Training iter #480000:   Batch Loss = 0.502024, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4231140613555908, Accuracy = 0.8133695125579834\n",
      "Training iter #510000:   Batch Loss = 0.525359, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2710847854614258, Accuracy = 0.8333898782730103\n",
      "Training iter #540000:   Batch Loss = 0.523772, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.103593349456787, Accuracy = 0.8717339634895325\n",
      "Training iter #570000:   Batch Loss = 0.484095, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1380752325057983, Accuracy = 0.8673226833343506\n",
      "Training iter #600000:   Batch Loss = 0.444831, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.144148349761963, Accuracy = 0.879199206829071\n",
      "Training iter #630000:   Batch Loss = 0.410706, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1418976783752441, Accuracy = 0.8751272559165955\n",
      "Training iter #660000:   Batch Loss = 0.434413, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1883169412612915, Accuracy = 0.8625721335411072\n",
      "Training iter #690000:   Batch Loss = 0.485955, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1120785474777222, Accuracy = 0.8727519512176514\n",
      "Training iter #720000:   Batch Loss = 0.428877, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0914528369903564, Accuracy = 0.873430609703064\n",
      "Training iter #750000:   Batch Loss = 0.398384, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1081503629684448, Accuracy = 0.882253110408783\n",
      "Training iter #780000:   Batch Loss = 0.397033, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.075111746788025, Accuracy = 0.8680013418197632\n",
      "Training iter #810000:   Batch Loss = 0.386872, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.085353970527649, Accuracy = 0.8724126219749451\n",
      "Training iter #840000:   Batch Loss = 0.416909, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.989458441734314, Accuracy = 0.8798778653144836\n",
      "Training iter #870000:   Batch Loss = 0.390937, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.110973596572876, Accuracy = 0.8540889024734497\n",
      "Optimization Finished!\n",
      "Training time is: 817.5040528774261 seconds\n",
      "FINAL RESULT: Batch Loss = 1.2679662704467773, Accuracy = 0.773668110370636\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.929987, Accuracy = 0.1966666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4630985260009766, Accuracy = 0.21547336876392365\n",
      "Training iter #30000:   Batch Loss = 1.344383, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.487957239151001, Accuracy = 0.6430268287658691\n",
      "Training iter #60000:   Batch Loss = 1.018798, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2734794616699219, Accuracy = 0.7573803663253784\n",
      "Training iter #90000:   Batch Loss = 0.846267, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2287626266479492, Accuracy = 0.7824906706809998\n",
      "Training iter #120000:   Batch Loss = 0.844187, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.147903323173523, Accuracy = 0.8303359150886536\n",
      "Training iter #150000:   Batch Loss = 0.673627, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.126946210861206, Accuracy = 0.8435697555541992\n",
      "Training iter #180000:   Batch Loss = 0.704849, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1235239505767822, Accuracy = 0.8527315855026245\n",
      "Training iter #210000:   Batch Loss = 0.668477, Accuracy = 0.9266666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1545627117156982, Accuracy = 0.838819146156311\n",
      "Training iter #240000:   Batch Loss = 0.626257, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1535358428955078, Accuracy = 0.8473023176193237\n",
      "Training iter #270000:   Batch Loss = 0.605419, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1754871606826782, Accuracy = 0.8513742685317993\n",
      "Training iter #300000:   Batch Loss = 0.591964, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2313868999481201, Accuracy = 0.8486596345901489\n",
      "Training iter #330000:   Batch Loss = 0.545605, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2508940696716309, Accuracy = 0.8564642071723938\n",
      "Training iter #360000:   Batch Loss = 0.734290, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0323327779769897, Accuracy = 0.8194774389266968\n",
      "Training iter #390000:   Batch Loss = 0.592102, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9802997708320618, Accuracy = 0.8598574995994568\n",
      "Training iter #420000:   Batch Loss = 0.551883, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0621188879013062, Accuracy = 0.8673226833343506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #450000:   Batch Loss = 0.527809, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1117403507232666, Accuracy = 0.8601968288421631\n",
      "Training iter #480000:   Batch Loss = 0.538709, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1557722091674805, Accuracy = 0.8595181703567505\n",
      "Training iter #510000:   Batch Loss = 0.530490, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.088542103767395, Accuracy = 0.8571428656578064\n",
      "Training iter #540000:   Batch Loss = 0.516378, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.072484016418457, Accuracy = 0.8574821949005127\n",
      "Training iter #570000:   Batch Loss = 0.484049, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1348122358322144, Accuracy = 0.8547675609588623\n",
      "Training iter #600000:   Batch Loss = 0.496381, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1630327701568604, Accuracy = 0.8520529270172119\n",
      "Training iter #630000:   Batch Loss = 0.459814, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1506257057189941, Accuracy = 0.854428231716156\n",
      "Training iter #660000:   Batch Loss = 0.458974, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1557198762893677, Accuracy = 0.8530709147453308\n",
      "Training iter #690000:   Batch Loss = 0.421722, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1826766729354858, Accuracy = 0.8442484140396118\n",
      "Training iter #720000:   Batch Loss = 0.424779, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2065837383270264, Accuracy = 0.8401764631271362\n",
      "Training iter #750000:   Batch Loss = 0.406803, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2340177297592163, Accuracy = 0.8381404876708984\n",
      "Training iter #780000:   Batch Loss = 0.509101, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.165069818496704, Accuracy = 0.838819146156311\n",
      "Training iter #810000:   Batch Loss = 0.392280, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0698851346969604, Accuracy = 0.8469629883766174\n",
      "Training iter #840000:   Batch Loss = 0.406358, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1109827756881714, Accuracy = 0.8496776223182678\n",
      "Training iter #870000:   Batch Loss = 0.397382, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0857982635498047, Accuracy = 0.8374618291854858\n",
      "Training iter #900000:   Batch Loss = 0.356219, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.083851933479309, Accuracy = 0.8462843298912048\n",
      "Training iter #930000:   Batch Loss = 0.379202, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1051723957061768, Accuracy = 0.8289785981178284\n",
      "Training iter #960000:   Batch Loss = 0.349777, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1247889995574951, Accuracy = 0.8469629883766174\n",
      "Training iter #990000:   Batch Loss = 0.332437, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.128961205482483, Accuracy = 0.8286392688751221\n",
      "Training iter #1020000:   Batch Loss = 0.350008, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0906856060028076, Accuracy = 0.8218527436256409\n",
      "Training iter #1050000:   Batch Loss = 0.383481, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9733668565750122, Accuracy = 0.8313539028167725\n",
      "Training iter #1080000:   Batch Loss = 0.367242, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9917972087860107, Accuracy = 0.8303359150886536\n",
      "Optimization Finished!\n",
      "Training time is: 1406.6220235824585 seconds\n",
      "FINAL RESULT: Batch Loss = 0.9182753562927246, Accuracy = 0.8564642071723938\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.813584, Accuracy = 0.2280000001192093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5125937461853027, Accuracy = 0.18425516784191132\n",
      "Training iter #30000:   Batch Loss = 1.411716, Accuracy = 0.621999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.516455888748169, Accuracy = 0.626060426235199\n",
      "Training iter #60000:   Batch Loss = 1.224463, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5118155479431152, Accuracy = 0.6589752435684204\n",
      "Training iter #90000:   Batch Loss = 1.068418, Accuracy = 0.8059999942779541\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3445507287979126, Accuracy = 0.7251442074775696\n",
      "Training iter #120000:   Batch Loss = 0.936605, Accuracy = 0.8546666502952576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2613524198532104, Accuracy = 0.7567017078399658\n",
      "Training iter #150000:   Batch Loss = 0.870472, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2420200109481812, Accuracy = 0.7919918298721313\n",
      "Training iter #180000:   Batch Loss = 0.903733, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.281057596206665, Accuracy = 0.7872412800788879\n",
      "Training iter #210000:   Batch Loss = 0.709506, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0856027603149414, Accuracy = 0.8439090847969055\n",
      "Training iter #240000:   Batch Loss = 0.752207, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.095942497253418, Accuracy = 0.84764164686203\n",
      "Training iter #270000:   Batch Loss = 0.660361, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0008466243743896, Accuracy = 0.8622328042984009\n",
      "Training iter #300000:   Batch Loss = 0.673875, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.02482271194458, Accuracy = 0.8585001826286316\n",
      "Training iter #330000:   Batch Loss = 0.644222, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0224871635437012, Accuracy = 0.870037317276001\n",
      "Training iter #360000:   Batch Loss = 0.587215, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1180099248886108, Accuracy = 0.8489989638328552\n",
      "Training iter #390000:   Batch Loss = 0.630782, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9964431524276733, Accuracy = 0.8744485974311829\n",
      "Training iter #420000:   Batch Loss = 0.567851, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.973527193069458, Accuracy = 0.876823902130127\n",
      "Training iter #450000:   Batch Loss = 0.561430, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0153416395187378, Accuracy = 0.8676620125770569\n",
      "Training iter #480000:   Batch Loss = 0.593248, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0523498058319092, Accuracy = 0.854428231716156\n",
      "Training iter #510000:   Batch Loss = 0.620584, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.95404052734375, Accuracy = 0.8459450006484985\n",
      "Training iter #540000:   Batch Loss = 0.596376, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9233791828155518, Accuracy = 0.8652867078781128\n",
      "Training iter #570000:   Batch Loss = 0.521848, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9117903709411621, Accuracy = 0.879199206829071\n",
      "Training iter #600000:   Batch Loss = 0.522460, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9130538702011108, Accuracy = 0.8839497566223145\n",
      "Training iter #630000:   Batch Loss = 0.504715, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9403135776519775, Accuracy = 0.882253110408783\n",
      "Training iter #660000:   Batch Loss = 0.532314, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0062199831008911, Accuracy = 0.8724126219749451\n",
      "Training iter #690000:   Batch Loss = 0.541938, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.887590765953064, Accuracy = 0.8825924396514893\n",
      "Training iter #720000:   Batch Loss = 0.466808, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8909343481063843, Accuracy = 0.876823902130127\n",
      "Training iter #750000:   Batch Loss = 0.488816, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8858764171600342, Accuracy = 0.8673226833343506\n",
      "Training iter #780000:   Batch Loss = 0.507041, Accuracy = 0.9480000138282776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8553009033203125, Accuracy = 0.8775025606155396\n",
      "Training iter #810000:   Batch Loss = 0.476468, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8600220680236816, Accuracy = 0.8717339634895325\n",
      "Training iter #840000:   Batch Loss = 0.461365, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8308981657028198, Accuracy = 0.88802170753479\n",
      "Training iter #870000:   Batch Loss = 0.417453, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8344125747680664, Accuracy = 0.8808958530426025\n",
      "Training iter #900000:   Batch Loss = 0.406693, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8574891090393066, Accuracy = 0.8778418898582458\n",
      "Training iter #930000:   Batch Loss = 0.560783, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0396809577941895, Accuracy = 0.8011537194252014\n",
      "Training iter #960000:   Batch Loss = 0.432650, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7275803089141846, Accuracy = 0.8931116461753845\n",
      "Training iter #990000:   Batch Loss = 0.445843, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.778849184513092, Accuracy = 0.8832710981369019\n",
      "Training iter #1020000:   Batch Loss = 0.391868, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8019857406616211, Accuracy = 0.8819137811660767\n",
      "Training iter #1050000:   Batch Loss = 0.362047, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7831844091415405, Accuracy = 0.8812351822853088\n",
      "Training iter #1080000:   Batch Loss = 0.409250, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7818635702133179, Accuracy = 0.8842890858650208\n",
      "Training iter #1110000:   Batch Loss = 0.361380, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7943685054779053, Accuracy = 0.8832710981369019\n",
      "Training iter #1140000:   Batch Loss = 0.416753, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7706009149551392, Accuracy = 0.8771632313728333\n",
      "Training iter #1170000:   Batch Loss = 0.386854, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7911052703857422, Accuracy = 0.8618934750556946\n",
      "Training iter #1200000:   Batch Loss = 0.356994, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7564429640769958, Accuracy = 0.8859857320785522\n",
      "Training iter #1230000:   Batch Loss = 0.364716, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7742096185684204, Accuracy = 0.8761452436447144\n",
      "Training iter #1260000:   Batch Loss = 0.324109, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7758499979972839, Accuracy = 0.8808958530426025\n",
      "Training iter #1290000:   Batch Loss = 0.371204, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7763857841491699, Accuracy = 0.8747879266738892\n",
      "Training iter #1320000:   Batch Loss = 0.317709, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7959848642349243, Accuracy = 0.876823902130127\n",
      "Optimization Finished!\n",
      "Training time is: 2110.9557995796204 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7930967807769775, Accuracy = 0.8744485974311829\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.045358, Accuracy = 0.1926666647195816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6787900924682617, Accuracy = 0.17916525900363922\n",
      "Training iter #30000:   Batch Loss = 1.261225, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3933395147323608, Accuracy = 0.6918900609016418\n",
      "Training iter #60000:   Batch Loss = 1.152386, Accuracy = 0.7706666588783264\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2836472988128662, Accuracy = 0.7478792071342468\n",
      "Training iter #90000:   Batch Loss = 0.930563, Accuracy = 0.8513333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2604230642318726, Accuracy = 0.7678995728492737\n",
      "Training iter #120000:   Batch Loss = 0.833079, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2192546129226685, Accuracy = 0.810654878616333\n",
      "Training iter #150000:   Batch Loss = 0.672196, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2113536596298218, Accuracy = 0.8177807927131653\n",
      "Training iter #180000:   Batch Loss = 0.625766, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0827224254608154, Accuracy = 0.8527315855026245\n",
      "Training iter #210000:   Batch Loss = 0.696533, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0457452535629272, Accuracy = 0.8605361580848694\n",
      "Training iter #240000:   Batch Loss = 0.783784, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2242528200149536, Accuracy = 0.7848659753799438\n",
      "Training iter #270000:   Batch Loss = 0.625094, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9773433208465576, Accuracy = 0.8554462194442749\n",
      "Training iter #300000:   Batch Loss = 0.588157, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.069197416305542, Accuracy = 0.8456056714057922\n",
      "Training iter #330000:   Batch Loss = 0.558387, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9810609221458435, Accuracy = 0.8761452436447144\n",
      "Training iter #360000:   Batch Loss = 0.561495, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0651965141296387, Accuracy = 0.8344078660011292\n",
      "Training iter #390000:   Batch Loss = 0.629375, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9046831727027893, Accuracy = 0.8863250613212585\n",
      "Training iter #420000:   Batch Loss = 0.597570, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9462209343910217, Accuracy = 0.8741092681884766\n",
      "Training iter #450000:   Batch Loss = 0.542522, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0063449144363403, Accuracy = 0.8686800003051758\n",
      "Training iter #480000:   Batch Loss = 0.508740, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0075103044509888, Accuracy = 0.8428910970687866\n",
      "Training iter #510000:   Batch Loss = 0.471376, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9931972622871399, Accuracy = 0.8656260371208191\n",
      "Training iter #540000:   Batch Loss = 0.478531, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9193859100341797, Accuracy = 0.8798778653144836\n",
      "Training iter #570000:   Batch Loss = 0.536724, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0013599395751953, Accuracy = 0.8551068902015686\n",
      "Training iter #600000:   Batch Loss = 0.515284, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3339568376541138, Accuracy = 0.7746860980987549\n",
      "Training iter #630000:   Batch Loss = 0.601764, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9079408645629883, Accuracy = 0.8252460360527039\n",
      "Training iter #660000:   Batch Loss = 0.446457, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8368532061576843, Accuracy = 0.8785205483436584\n",
      "Training iter #690000:   Batch Loss = 0.429828, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8166199922561646, Accuracy = 0.8941296339035034\n",
      "Training iter #720000:   Batch Loss = 0.420538, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8141202926635742, Accuracy = 0.882253110408783\n",
      "Training iter #750000:   Batch Loss = 0.457152, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8109031319618225, Accuracy = 0.8832710981369019\n",
      "Training iter #780000:   Batch Loss = 0.436065, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.889186680316925, Accuracy = 0.8825924396514893\n",
      "Training iter #810000:   Batch Loss = 0.439284, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8131493330001831, Accuracy = 0.8887003660202026\n",
      "Training iter #840000:   Batch Loss = 0.400480, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7491453886032104, Accuracy = 0.8985409140586853\n",
      "Training iter #870000:   Batch Loss = 0.392078, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7709861993789673, Accuracy = 0.8907363414764404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #900000:   Batch Loss = 0.376699, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7825782299041748, Accuracy = 0.8927723169326782\n",
      "Training iter #930000:   Batch Loss = 0.414649, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7908875942230225, Accuracy = 0.8887003660202026\n",
      "Training iter #960000:   Batch Loss = 0.361012, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7476741075515747, Accuracy = 0.8934509754180908\n",
      "Training iter #990000:   Batch Loss = 0.368358, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7894287109375, Accuracy = 0.8866643905639648\n",
      "Training iter #1020000:   Batch Loss = 0.383858, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8137689828872681, Accuracy = 0.8778418898582458\n",
      "Training iter #1050000:   Batch Loss = 0.366832, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8245559930801392, Accuracy = 0.8737699389457703\n",
      "Training iter #1080000:   Batch Loss = 0.378090, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8114827871322632, Accuracy = 0.879199206829071\n",
      "Training iter #1110000:   Batch Loss = 0.378282, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8187223672866821, Accuracy = 0.882253110408783\n",
      "Training iter #1140000:   Batch Loss = 0.313467, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8333108425140381, Accuracy = 0.879199206829071\n",
      "Training iter #1170000:   Batch Loss = 0.367773, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7919655442237854, Accuracy = 0.8819137811660767\n",
      "Training iter #1200000:   Batch Loss = 0.346636, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7361717820167542, Accuracy = 0.8659653663635254\n",
      "Training iter #1230000:   Batch Loss = 0.328516, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8174788951873779, Accuracy = 0.8713946342468262\n",
      "Training iter #1260000:   Batch Loss = 0.368562, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7019765377044678, Accuracy = 0.8920936584472656\n",
      "Training iter #1290000:   Batch Loss = 0.364205, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7022467851638794, Accuracy = 0.8968442678451538\n",
      "Training iter #1320000:   Batch Loss = 0.317302, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7100275158882141, Accuracy = 0.8778418898582458\n",
      "Training iter #1350000:   Batch Loss = 0.334047, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7623984217643738, Accuracy = 0.8808958530426025\n",
      "Training iter #1380000:   Batch Loss = 0.344540, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6668419241905212, Accuracy = 0.8707159757614136\n",
      "Training iter #1410000:   Batch Loss = 0.291368, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6993354558944702, Accuracy = 0.8805565237998962\n",
      "Training iter #1440000:   Batch Loss = 0.316783, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7425564527511597, Accuracy = 0.88802170753479\n",
      "Training iter #1470000:   Batch Loss = 0.318685, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7905888557434082, Accuracy = 0.8618934750556946\n",
      "Training iter #1500000:   Batch Loss = 0.319850, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.632042407989502, Accuracy = 0.876823902130127\n",
      "Training iter #1530000:   Batch Loss = 0.302811, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6624980568885803, Accuracy = 0.8968442678451538\n",
      "Optimization Finished!\n",
      "Training time is: 2844.805278778076 seconds\n",
      "FINAL RESULT: Batch Loss = 0.68813556432724, Accuracy = 0.8998982310295105\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.940863, Accuracy = 0.2173333317041397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.545644760131836, Accuracy = 0.31048524379730225\n",
      "Training iter #30000:   Batch Loss = 1.429850, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.609425663948059, Accuracy = 0.5965388417243958\n",
      "Training iter #60000:   Batch Loss = 1.264576, Accuracy = 0.7206666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4731359481811523, Accuracy = 0.6667797565460205\n",
      "Training iter #90000:   Batch Loss = 1.167950, Accuracy = 0.7820000052452087\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.368347406387329, Accuracy = 0.733627438545227\n",
      "Training iter #120000:   Batch Loss = 1.057126, Accuracy = 0.8299999833106995\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3656340837478638, Accuracy = 0.7472005486488342\n",
      "Training iter #150000:   Batch Loss = 0.879349, Accuracy = 0.9020000100135803\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2794811725616455, Accuracy = 0.8232100605964661\n",
      "Training iter #180000:   Batch Loss = 0.859262, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2025822401046753, Accuracy = 0.8540889024734497\n",
      "Training iter #210000:   Batch Loss = 0.796798, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1216697692871094, Accuracy = 0.8608754873275757\n",
      "Training iter #240000:   Batch Loss = 0.691521, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1032038927078247, Accuracy = 0.8724126219749451\n",
      "Training iter #270000:   Batch Loss = 0.654743, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.139106035232544, Accuracy = 0.8795385360717773\n",
      "Training iter #300000:   Batch Loss = 0.632353, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1034482717514038, Accuracy = 0.8853070735931396\n",
      "Training iter #330000:   Batch Loss = 0.644248, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1098623275756836, Accuracy = 0.8764845728874207\n",
      "Training iter #360000:   Batch Loss = 0.690331, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0391335487365723, Accuracy = 0.8839497566223145\n",
      "Training iter #390000:   Batch Loss = 0.668118, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0801072120666504, Accuracy = 0.8639293909072876\n",
      "Training iter #420000:   Batch Loss = 0.600147, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0283474922180176, Accuracy = 0.8917543292045593\n",
      "Training iter #450000:   Batch Loss = 0.571171, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.020754098892212, Accuracy = 0.891414999961853\n",
      "Training iter #480000:   Batch Loss = 0.618878, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0370206832885742, Accuracy = 0.8937903046607971\n",
      "Training iter #510000:   Batch Loss = 0.610569, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0440831184387207, Accuracy = 0.8968442678451538\n",
      "Training iter #540000:   Batch Loss = 0.526000, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0065665245056152, Accuracy = 0.8863250613212585\n",
      "Training iter #570000:   Batch Loss = 0.638780, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0133484601974487, Accuracy = 0.8775025606155396\n",
      "Training iter #600000:   Batch Loss = 0.516453, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9822902083396912, Accuracy = 0.8870037198066711\n",
      "Training iter #630000:   Batch Loss = 0.476995, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9430735111236572, Accuracy = 0.9019341468811035\n",
      "Training iter #660000:   Batch Loss = 0.485121, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9530519247055054, Accuracy = 0.9005768299102783\n",
      "Training iter #690000:   Batch Loss = 0.515703, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9673326015472412, Accuracy = 0.8975229263305664\n",
      "Training iter #720000:   Batch Loss = 0.490853, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9442479610443115, Accuracy = 0.9060060977935791\n",
      "Training iter #750000:   Batch Loss = 0.450160, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9443525075912476, Accuracy = 0.8900576829910278\n",
      "Training iter #780000:   Batch Loss = 0.635703, Accuracy = 0.9473333358764648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9550886750221252, Accuracy = 0.8985409140586853\n",
      "Training iter #810000:   Batch Loss = 0.559837, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8896909952163696, Accuracy = 0.8944689631462097\n",
      "Training iter #840000:   Batch Loss = 0.443038, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.873776912689209, Accuracy = 0.8988802433013916\n",
      "Training iter #870000:   Batch Loss = 0.428528, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8090881109237671, Accuracy = 0.9026128053665161\n",
      "Training iter #900000:   Batch Loss = 0.409683, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8416591882705688, Accuracy = 0.8995589017868042\n",
      "Training iter #930000:   Batch Loss = 0.386477, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8585647344589233, Accuracy = 0.9015948176383972\n",
      "Training iter #960000:   Batch Loss = 0.451091, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8935544490814209, Accuracy = 0.8764845728874207\n",
      "Training iter #990000:   Batch Loss = 0.494880, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8594986200332642, Accuracy = 0.9012554883956909\n",
      "Training iter #1020000:   Batch Loss = 0.407586, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8499965667724609, Accuracy = 0.8931116461753845\n",
      "Training iter #1050000:   Batch Loss = 0.376518, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8301610350608826, Accuracy = 0.8968442678451538\n",
      "Training iter #1080000:   Batch Loss = 0.429036, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8313720226287842, Accuracy = 0.9012554883956909\n",
      "Training iter #1110000:   Batch Loss = 0.416485, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8071842789649963, Accuracy = 0.9039701223373413\n",
      "Training iter #1140000:   Batch Loss = 0.359156, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8244813680648804, Accuracy = 0.9019341468811035\n",
      "Training iter #1170000:   Batch Loss = 0.376580, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7748124599456787, Accuracy = 0.9029521346092224\n",
      "Training iter #1200000:   Batch Loss = 0.348589, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7242794036865234, Accuracy = 0.9134713411331177\n",
      "Training iter #1230000:   Batch Loss = 0.329394, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7749423980712891, Accuracy = 0.8951476216316223\n",
      "Training iter #1260000:   Batch Loss = 0.470229, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8972046375274658, Accuracy = 0.8282999396324158\n",
      "Training iter #1290000:   Batch Loss = 0.469757, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6752976775169373, Accuracy = 0.8832710981369019\n",
      "Training iter #1320000:   Batch Loss = 0.366674, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7064354419708252, Accuracy = 0.9066847562789917\n",
      "Training iter #1350000:   Batch Loss = 0.332337, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7344518303871155, Accuracy = 0.9053274393081665\n",
      "Training iter #1380000:   Batch Loss = 0.384691, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7482796311378479, Accuracy = 0.9090600609779358\n",
      "Training iter #1410000:   Batch Loss = 0.365892, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7734351754188538, Accuracy = 0.9029521346092224\n",
      "Training iter #1440000:   Batch Loss = 0.322206, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7807703614234924, Accuracy = 0.9022734761238098\n",
      "Training iter #1470000:   Batch Loss = 0.368047, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.009250521659851, Accuracy = 0.8293179273605347\n",
      "Training iter #1500000:   Batch Loss = 0.339827, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7133688926696777, Accuracy = 0.8944689631462097\n",
      "Training iter #1530000:   Batch Loss = 0.291012, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7245904207229614, Accuracy = 0.8944689631462097\n",
      "Training iter #1560000:   Batch Loss = 0.295353, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7217884063720703, Accuracy = 0.910417377948761\n",
      "Training iter #1590000:   Batch Loss = 0.327602, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.720352292060852, Accuracy = 0.9077027440071106\n",
      "Training iter #1620000:   Batch Loss = 0.309844, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7265744209289551, Accuracy = 0.9032914638519287\n",
      "Training iter #1650000:   Batch Loss = 0.277019, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7132658958435059, Accuracy = 0.8995589017868042\n",
      "Training iter #1680000:   Batch Loss = 0.317238, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7081800699234009, Accuracy = 0.8978622555732727\n",
      "Training iter #1710000:   Batch Loss = 0.314809, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7136083245277405, Accuracy = 0.900237500667572\n",
      "Training iter #1740000:   Batch Loss = 0.266946, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7149314284324646, Accuracy = 0.9026128053665161\n",
      "Optimization Finished!\n",
      "Training time is: 3911.613085746765 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7720768451690674, Accuracy = 0.8364438414573669\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 4.092761, Accuracy = 0.06866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.2172367572784424, Accuracy = 0.22192059457302094\n",
      "Training iter #30000:   Batch Loss = 1.714663, Accuracy = 0.5606666803359985\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6650910377502441, Accuracy = 0.5914489030838013\n",
      "Training iter #60000:   Batch Loss = 1.193545, Accuracy = 0.7693333625793457\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4104903936386108, Accuracy = 0.6912114024162292\n",
      "Training iter #90000:   Batch Loss = 1.158389, Accuracy = 0.7726666927337646\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3348901271820068, Accuracy = 0.7417712807655334\n",
      "Training iter #120000:   Batch Loss = 0.956459, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2458001375198364, Accuracy = 0.8028503656387329\n",
      "Training iter #150000:   Batch Loss = 1.010532, Accuracy = 0.8360000252723694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.215700387954712, Accuracy = 0.8221920728683472\n",
      "Training iter #180000:   Batch Loss = 0.812530, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.103891372680664, Accuracy = 0.8561248779296875\n",
      "Training iter #210000:   Batch Loss = 0.686604, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.109634280204773, Accuracy = 0.8608754873275757\n",
      "Training iter #240000:   Batch Loss = 0.816244, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1460787057876587, Accuracy = 0.8333898782730103\n",
      "Training iter #270000:   Batch Loss = 0.693410, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0294297933578491, Accuracy = 0.870037317276001\n",
      "Training iter #300000:   Batch Loss = 0.683187, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0412309169769287, Accuracy = 0.8832710981369019\n",
      "Training iter #330000:   Batch Loss = 0.627915, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.010772705078125, Accuracy = 0.8859857320785522\n",
      "Training iter #360000:   Batch Loss = 0.632220, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0037668943405151, Accuracy = 0.8883610367774963\n",
      "Training iter #390000:   Batch Loss = 0.615867, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0166378021240234, Accuracy = 0.8720732927322388\n",
      "Training iter #420000:   Batch Loss = 0.679739, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0034005641937256, Accuracy = 0.8917543292045593\n",
      "Training iter #450000:   Batch Loss = 0.633265, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0096235275268555, Accuracy = 0.8693586587905884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #480000:   Batch Loss = 0.712615, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0060139894485474, Accuracy = 0.8785205483436584\n",
      "Training iter #510000:   Batch Loss = 0.580648, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9565333127975464, Accuracy = 0.8958262801170349\n",
      "Training iter #540000:   Batch Loss = 0.606179, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9645205140113831, Accuracy = 0.8903970122337341\n",
      "Training iter #570000:   Batch Loss = 0.576234, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.966318666934967, Accuracy = 0.8866643905639648\n",
      "Training iter #600000:   Batch Loss = 0.581735, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0088659524917603, Accuracy = 0.8849677443504333\n",
      "Training iter #630000:   Batch Loss = 0.563320, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1261179447174072, Accuracy = 0.8527315855026245\n",
      "Training iter #660000:   Batch Loss = 0.560055, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9564567804336548, Accuracy = 0.8411944508552551\n",
      "Training iter #690000:   Batch Loss = 0.611628, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8839424848556519, Accuracy = 0.8863250613212585\n",
      "Training iter #720000:   Batch Loss = 0.506173, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9369031190872192, Accuracy = 0.8866643905639648\n",
      "Training iter #750000:   Batch Loss = 0.533268, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9066485166549683, Accuracy = 0.8931116461753845\n",
      "Training iter #780000:   Batch Loss = 0.491269, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8972582221031189, Accuracy = 0.8920936584472656\n",
      "Training iter #810000:   Batch Loss = 0.481259, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9095161557197571, Accuracy = 0.8853070735931396\n",
      "Training iter #840000:   Batch Loss = 0.442294, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9008617401123047, Accuracy = 0.8870037198066711\n",
      "Training iter #870000:   Batch Loss = 0.510324, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8756205439567566, Accuracy = 0.8887003660202026\n",
      "Training iter #900000:   Batch Loss = 0.450595, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8849042654037476, Accuracy = 0.8866643905639648\n",
      "Training iter #930000:   Batch Loss = 0.484787, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8710705637931824, Accuracy = 0.8866643905639648\n",
      "Training iter #960000:   Batch Loss = 0.436234, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.869540810585022, Accuracy = 0.885646402835846\n",
      "Training iter #990000:   Batch Loss = 0.471438, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.871926486492157, Accuracy = 0.882253110408783\n",
      "Training iter #1020000:   Batch Loss = 0.451508, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.853769838809967, Accuracy = 0.8917543292045593\n",
      "Training iter #1050000:   Batch Loss = 0.435961, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9238236546516418, Accuracy = 0.8778418898582458\n",
      "Training iter #1080000:   Batch Loss = 0.413307, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8097648620605469, Accuracy = 0.8903970122337341\n",
      "Training iter #1110000:   Batch Loss = 0.401757, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8213499784469604, Accuracy = 0.8944689631462097\n",
      "Training iter #1140000:   Batch Loss = 0.431542, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.81703782081604, Accuracy = 0.8873430490493774\n",
      "Training iter #1170000:   Batch Loss = 0.410303, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9293408393859863, Accuracy = 0.8608754873275757\n",
      "Training iter #1200000:   Batch Loss = 0.437003, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9426059722900391, Accuracy = 0.8690193295478821\n",
      "Training iter #1230000:   Batch Loss = 0.482621, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.792049765586853, Accuracy = 0.8866643905639648\n",
      "Training iter #1260000:   Batch Loss = 0.392491, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8029618263244629, Accuracy = 0.88802170753479\n",
      "Training iter #1290000:   Batch Loss = 0.353166, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.803607702255249, Accuracy = 0.8890396952629089\n",
      "Training iter #1320000:   Batch Loss = 0.407083, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8113046288490295, Accuracy = 0.8934509754180908\n",
      "Training iter #1350000:   Batch Loss = 0.427642, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8242300152778625, Accuracy = 0.8717339634895325\n",
      "Training iter #1380000:   Batch Loss = 0.417284, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.750097393989563, Accuracy = 0.8839497566223145\n",
      "Training iter #1410000:   Batch Loss = 0.369593, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7867196798324585, Accuracy = 0.8900576829910278\n",
      "Training iter #1440000:   Batch Loss = 0.391054, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7721778750419617, Accuracy = 0.8992195725440979\n",
      "Training iter #1470000:   Batch Loss = 0.374612, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7873332500457764, Accuracy = 0.8900576829910278\n",
      "Training iter #1500000:   Batch Loss = 0.416144, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7497313022613525, Accuracy = 0.8615541458129883\n",
      "Training iter #1530000:   Batch Loss = 0.436598, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.794759213924408, Accuracy = 0.8795385360717773\n",
      "Training iter #1560000:   Batch Loss = 0.321896, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7854396104812622, Accuracy = 0.8839497566223145\n",
      "Training iter #1590000:   Batch Loss = 0.359192, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7841631174087524, Accuracy = 0.8805565237998962\n",
      "Training iter #1620000:   Batch Loss = 0.325036, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8053666353225708, Accuracy = 0.8781812191009521\n",
      "Training iter #1650000:   Batch Loss = 0.346359, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7860525846481323, Accuracy = 0.8747879266738892\n",
      "Training iter #1680000:   Batch Loss = 0.308629, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7971630096435547, Accuracy = 0.8778418898582458\n",
      "Training iter #1710000:   Batch Loss = 0.302655, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7727391123771667, Accuracy = 0.8744485974311829\n",
      "Training iter #1740000:   Batch Loss = 0.331725, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7702173590660095, Accuracy = 0.8785205483436584\n",
      "Training iter #1770000:   Batch Loss = 0.347436, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7389588356018066, Accuracy = 0.8808958530426025\n",
      "Training iter #1800000:   Batch Loss = 0.316903, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7947970032691956, Accuracy = 0.8588395118713379\n",
      "Training iter #1830000:   Batch Loss = 0.343571, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6857577562332153, Accuracy = 0.8995589017868042\n",
      "Training iter #1860000:   Batch Loss = 0.282085, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7175503373146057, Accuracy = 0.8788598775863647\n",
      "Training iter #1890000:   Batch Loss = 0.304058, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7683405876159668, Accuracy = 0.8764845728874207\n",
      "Training iter #1920000:   Batch Loss = 0.400152, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7975074648857117, Accuracy = 0.876823902130127\n",
      "Training iter #1950000:   Batch Loss = 0.347455, Accuracy = 0.9466666579246521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6977745294570923, Accuracy = 0.8669833540916443\n",
      "Training iter #1980000:   Batch Loss = 0.321337, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8137949705123901, Accuracy = 0.8527315855026245\n",
      "Optimization Finished!\n",
      "Training time is: 4653.333568572998 seconds\n",
      "FINAL RESULT: Batch Loss = 0.746449887752533, Accuracy = 0.8693586587905884\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.672680, Accuracy = 0.009333333000540733\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3910863399505615, Accuracy = 0.39972853660583496\n",
      "Training iter #30000:   Batch Loss = 1.320608, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3748526573181152, Accuracy = 0.6674584150314331\n",
      "Training iter #60000:   Batch Loss = 1.195260, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3409698009490967, Accuracy = 0.7197149395942688\n",
      "Training iter #90000:   Batch Loss = 1.017835, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.296147108078003, Accuracy = 0.7488971948623657\n",
      "Training iter #120000:   Batch Loss = 0.846108, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1424702405929565, Accuracy = 0.8123515248298645\n",
      "Training iter #150000:   Batch Loss = 0.751359, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1291148662567139, Accuracy = 0.8357651829719543\n",
      "Training iter #180000:   Batch Loss = 0.822873, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1831645965576172, Accuracy = 0.8245673775672913\n",
      "Training iter #210000:   Batch Loss = 1.003831, Accuracy = 0.8453333377838135\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0284284353256226, Accuracy = 0.8496776223182678\n",
      "Training iter #240000:   Batch Loss = 0.686074, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9320317506790161, Accuracy = 0.8808958530426025\n",
      "Training iter #270000:   Batch Loss = 0.646739, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9030975103378296, Accuracy = 0.8870037198066711\n",
      "Training iter #300000:   Batch Loss = 0.616929, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8761288523674011, Accuracy = 0.8944689631462097\n",
      "Training iter #330000:   Batch Loss = 0.645358, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8904867768287659, Accuracy = 0.8900576829910278\n",
      "Training iter #360000:   Batch Loss = 0.651384, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8604128360748291, Accuracy = 0.8924329876899719\n",
      "Training iter #390000:   Batch Loss = 0.649048, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8771060109138489, Accuracy = 0.8910756707191467\n",
      "Training iter #420000:   Batch Loss = 0.625655, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9040838479995728, Accuracy = 0.8839497566223145\n",
      "Training iter #450000:   Batch Loss = 0.588054, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9508566856384277, Accuracy = 0.8693586587905884\n",
      "Training iter #480000:   Batch Loss = 0.617199, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8073979616165161, Accuracy = 0.898201584815979\n",
      "Training iter #510000:   Batch Loss = 0.533859, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7949622273445129, Accuracy = 0.907024085521698\n",
      "Training iter #540000:   Batch Loss = 0.566814, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7912269830703735, Accuracy = 0.9015948176383972\n",
      "Training iter #570000:   Batch Loss = 0.588956, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7888948917388916, Accuracy = 0.9073634147644043\n",
      "Training iter #600000:   Batch Loss = 0.557694, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8319028615951538, Accuracy = 0.8941296339035034\n",
      "Training iter #630000:   Batch Loss = 0.487704, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7972182035446167, Accuracy = 0.898201584815979\n",
      "Training iter #660000:   Batch Loss = 0.484716, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7351876497268677, Accuracy = 0.9060060977935791\n",
      "Training iter #690000:   Batch Loss = 0.443167, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7603453993797302, Accuracy = 0.9012554883956909\n",
      "Training iter #720000:   Batch Loss = 0.520571, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7469139099121094, Accuracy = 0.9019341468811035\n",
      "Training iter #750000:   Batch Loss = 0.538589, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9017194509506226, Accuracy = 0.866644024848938\n",
      "Training iter #780000:   Batch Loss = 0.482627, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7591853737831116, Accuracy = 0.8917543292045593\n",
      "Training iter #810000:   Batch Loss = 0.467403, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7625685930252075, Accuracy = 0.8873430490493774\n",
      "Training iter #840000:   Batch Loss = 0.503140, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.774433970451355, Accuracy = 0.8595181703567505\n",
      "Training iter #870000:   Batch Loss = 0.451142, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7148510217666626, Accuracy = 0.8893790245056152\n",
      "Training iter #900000:   Batch Loss = 0.413695, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.740594208240509, Accuracy = 0.8937903046607971\n",
      "Training iter #930000:   Batch Loss = 0.471827, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7304849624633789, Accuracy = 0.8944689631462097\n",
      "Training iter #960000:   Batch Loss = 0.451420, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.754367470741272, Accuracy = 0.8832710981369019\n",
      "Training iter #990000:   Batch Loss = 0.431955, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1242939233779907, Accuracy = 0.8177807927131653\n",
      "Training iter #1020000:   Batch Loss = 0.449446, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8771546483039856, Accuracy = 0.8527315855026245\n",
      "Training iter #1050000:   Batch Loss = 0.383769, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7386056184768677, Accuracy = 0.8829317688941956\n",
      "Training iter #1080000:   Batch Loss = 0.469259, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8305737972259521, Accuracy = 0.8316932320594788\n",
      "Training iter #1110000:   Batch Loss = 0.472066, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7238199710845947, Accuracy = 0.8890396952629089\n",
      "Training iter #1140000:   Batch Loss = 0.451930, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7053269147872925, Accuracy = 0.8941296339035034\n",
      "Training iter #1170000:   Batch Loss = 0.375612, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7252234220504761, Accuracy = 0.8890396952629089\n",
      "Training iter #1200000:   Batch Loss = 0.374340, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7305071949958801, Accuracy = 0.8870037198066711\n",
      "Training iter #1230000:   Batch Loss = 0.369633, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7421547174453735, Accuracy = 0.884628415107727\n",
      "Training iter #1260000:   Batch Loss = 0.343331, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7441720366477966, Accuracy = 0.8808958530426025\n",
      "Training iter #1290000:   Batch Loss = 0.405687, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6780905723571777, Accuracy = 0.8870037198066711\n",
      "Training iter #1320000:   Batch Loss = 0.404666, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7245226502418518, Accuracy = 0.8713946342468262\n",
      "Training iter #1350000:   Batch Loss = 0.375330, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7455146908760071, Accuracy = 0.8707159757614136\n",
      "Training iter #1380000:   Batch Loss = 0.338162, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7509548664093018, Accuracy = 0.8618934750556946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1410000:   Batch Loss = 0.366369, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7184369564056396, Accuracy = 0.8652867078781128\n",
      "Training iter #1440000:   Batch Loss = 0.349151, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.628272294998169, Accuracy = 0.8924329876899719\n",
      "Training iter #1470000:   Batch Loss = 0.410931, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6402058601379395, Accuracy = 0.8836104273796082\n",
      "Training iter #1500000:   Batch Loss = 0.393191, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6870869994163513, Accuracy = 0.8805565237998962\n",
      "Training iter #1530000:   Batch Loss = 0.314105, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.688265323638916, Accuracy = 0.8839497566223145\n",
      "Training iter #1560000:   Batch Loss = 0.323550, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7356846332550049, Accuracy = 0.8703766465187073\n",
      "Training iter #1590000:   Batch Loss = 0.359477, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6718965768814087, Accuracy = 0.885646402835846\n",
      "Training iter #1620000:   Batch Loss = 0.334310, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6298720836639404, Accuracy = 0.8859857320785522\n",
      "Training iter #1650000:   Batch Loss = 0.352001, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7331726551055908, Accuracy = 0.8730912804603577\n",
      "Training iter #1680000:   Batch Loss = 0.369852, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7162476181983948, Accuracy = 0.8778418898582458\n",
      "Training iter #1710000:   Batch Loss = 0.330820, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6222022771835327, Accuracy = 0.8917543292045593\n",
      "Training iter #1740000:   Batch Loss = 0.290185, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6817381381988525, Accuracy = 0.8771632313728333\n",
      "Training iter #1770000:   Batch Loss = 0.276023, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7810052633285522, Accuracy = 0.8629114627838135\n",
      "Training iter #1800000:   Batch Loss = 0.314233, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6345404386520386, Accuracy = 0.8839497566223145\n",
      "Training iter #1830000:   Batch Loss = 0.362197, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6273298859596252, Accuracy = 0.8853070735931396\n",
      "Training iter #1860000:   Batch Loss = 0.373333, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5694057941436768, Accuracy = 0.8747879266738892\n",
      "Training iter #1890000:   Batch Loss = 0.302488, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5406874418258667, Accuracy = 0.9019341468811035\n",
      "Training iter #1920000:   Batch Loss = 0.284168, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5663753747940063, Accuracy = 0.8988802433013916\n",
      "Training iter #1950000:   Batch Loss = 0.287420, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5839771032333374, Accuracy = 0.8968442678451538\n",
      "Training iter #1980000:   Batch Loss = 0.292421, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5866069793701172, Accuracy = 0.898201584815979\n",
      "Training iter #2010000:   Batch Loss = 0.261640, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5992119908332825, Accuracy = 0.8988802433013916\n",
      "Training iter #2040000:   Batch Loss = 0.316045, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5884578227996826, Accuracy = 0.8992195725440979\n",
      "Training iter #2070000:   Batch Loss = 0.314066, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5889240503311157, Accuracy = 0.8995589017868042\n",
      "Training iter #2100000:   Batch Loss = 0.232600, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5982279181480408, Accuracy = 0.8995589017868042\n",
      "Training iter #2130000:   Batch Loss = 0.295770, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5360977053642273, Accuracy = 0.8629114627838135\n",
      "Training iter #2160000:   Batch Loss = 0.246905, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5890440940856934, Accuracy = 0.8883610367774963\n",
      "Training iter #2190000:   Batch Loss = 0.264307, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5376263856887817, Accuracy = 0.9019341468811035\n",
      "Optimization Finished!\n",
      "Training time is: 5938.139092683792 seconds\n",
      "FINAL RESULT: Batch Loss = 0.5876399278640747, Accuracy = 0.8934509754180908\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[178.23890900611877, 313.98351311683655, 539.417266368866, 817.5050506591797, 1406.6220235824585, 2110.9557995796204, 2844.805278778076, 3911.613085746765, 4653.333568572998, 5938.139092683792]\n",
      "[0.7421106, 0.84255177, 0.8734306, 0.7736681, 0.8564642, 0.8744486, 0.89989823, 0.83644384, 0.86935866, 0.893451]\n",
      "[2.2245364, 1.2638863, 0.97366136, 1.2679663, 0.91827536, 0.7930968, 0.68813556, 0.77207685, 0.7464499, 0.5876399]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 of End-to-end training with the baseline model model 1.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset under same number of training iterations.\n",
    "# Based on the code of Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "# Functions to load the input data\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array so when we test the model with different datasize, \n",
    "# we can have a roughly evenly distributed classes of data\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "# Define LSTM and other helper functions.\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32 # Hidden layer num of features\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "    # Training the neural net\n",
    "\n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    # Graph weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.993079, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.1482207775115967, Accuracy = 0.18221920728683472\n",
      "Training iter #300000:   Batch Loss = 0.673940, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9710876941680908, Accuracy = 0.8724126219749451\n",
      "Training iter #600000:   Batch Loss = 0.611907, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.989443302154541, Accuracy = 0.8693586587905884\n",
      "Training iter #900000:   Batch Loss = 0.468934, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.786361575126648, Accuracy = 0.8849677443504333\n",
      "Training iter #1200000:   Batch Loss = 0.439657, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7811511754989624, Accuracy = 0.8863250613212585\n",
      "Training iter #1500000:   Batch Loss = 0.405759, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6536039113998413, Accuracy = 0.8839497566223145\n",
      "Training iter #1800000:   Batch Loss = 0.353126, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6498178243637085, Accuracy = 0.8713946342468262\n",
      "Training iter #2100000:   Batch Loss = 0.274733, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6838410496711731, Accuracy = 0.8788598775863647\n",
      "Training iter #2400000:   Batch Loss = 0.315537, Accuracy = 0.9353333115577698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6387608647346497, Accuracy = 0.8741092681884766\n",
      "Training iter #2700000:   Batch Loss = 0.300500, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5957967042922974, Accuracy = 0.8910756707191467\n",
      "Optimization Finished!\n",
      "Training time is: 2665.7584557533264 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6376276612281799, Accuracy = 0.8859857320785522\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment 2 Part 1 of End-to-end training with the baseline model model 1.\n",
    "# Finding the optimal number of iterations for Model 1\n",
    "# Based on the code of Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "# Define LSTM and other helper functions\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32 # Hidden layer num of features\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 400  # Loop 400 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "\n",
    "    # Training the neural net\n",
    "\n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    # Graph weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "    \n",
    "    # plot the result to find the optimal number of iterations\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(np.arange(0,len(train_losses),1), train_losses, 'r', np.arange(0,len(test_losses),1), test_losses, 'b')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(np.arange(0,len(train_accuracies),1), train_accuracies, 'r', np.arange(0,len(test_accuracies),1), test_accuracies, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save above results for manuscript plotting later.\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[train_accuracies[i], test_accuracies[i],train_losses[i],test_losses[i]] for i in range(0,len(train_accuracies))])\n",
    "sheet.save_as(\"LSTM_Training_ACC_LOSS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "WARNING:tensorflow:From c:\\users\\edwar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-19a589d1404a>:216: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.126858, Accuracy = 0.20133332908153534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.7522950172424316, Accuracy = 0.15710891783237457\n",
      "Optimization Finished!\n",
      "Training time is: 201.77709484100342 seconds\n",
      "FINAL RESULT: Batch Loss = 2.467919111251831, Accuracy = 0.6728876829147339\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.760080, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.817190647125244, Accuracy = 0.22904649376869202\n",
      "Training iter #300000:   Batch Loss = 0.603584, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.22968327999115, Accuracy = 0.8364438414573669\n",
      "Optimization Finished!\n",
      "Training time is: 340.67320585250854 seconds\n",
      "FINAL RESULT: Batch Loss = 1.270267367362976, Accuracy = 0.826603353023529\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.880519, Accuracy = 0.14733333885669708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.417586326599121, Accuracy = 0.16932474076747894\n",
      "Training iter #300000:   Batch Loss = 0.550390, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.209153175354004, Accuracy = 0.8411944508552551\n",
      "Training iter #600000:   Batch Loss = 0.422704, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.125903606414795, Accuracy = 0.8364438414573669\n",
      "Optimization Finished!\n",
      "Training time is: 577.2064092159271 seconds\n",
      "FINAL RESULT: Batch Loss = 0.9237062335014343, Accuracy = 0.7936884760856628\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.111403, Accuracy = 0.20800000429153442\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6771621704101562, Accuracy = 0.20223957300186157\n",
      "Training iter #300000:   Batch Loss = 0.552901, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3093185424804688, Accuracy = 0.8364438414573669\n",
      "Training iter #600000:   Batch Loss = 0.438325, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1303954124450684, Accuracy = 0.8506956100463867\n",
      "Optimization Finished!\n",
      "Training time is: 1101.1747999191284 seconds\n",
      "FINAL RESULT: Batch Loss = 0.9066397547721863, Accuracy = 0.8333898782730103\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.128616, Accuracy = 0.0560000017285347\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5829052925109863, Accuracy = 0.192059725522995\n",
      "Training iter #300000:   Batch Loss = 0.580622, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8992571830749512, Accuracy = 0.8683406710624695\n",
      "Training iter #600000:   Batch Loss = 0.493309, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8711111545562744, Accuracy = 0.8568035364151001\n",
      "Training iter #900000:   Batch Loss = 0.346776, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7686811089515686, Accuracy = 0.898201584815979\n",
      "Optimization Finished!\n",
      "Training time is: 1800.5067710876465 seconds\n",
      "FINAL RESULT: Batch Loss = 0.83069908618927, Accuracy = 0.8540889024734497\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.712560, Accuracy = 0.1693333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5073843002319336, Accuracy = 0.198167622089386\n",
      "Training iter #300000:   Batch Loss = 0.644526, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.963869571685791, Accuracy = 0.9066847562789917\n",
      "Training iter #600000:   Batch Loss = 0.521762, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.894994854927063, Accuracy = 0.8727519512176514\n",
      "Training iter #900000:   Batch Loss = 0.621836, Accuracy = 0.9006666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9756653308868408, Accuracy = 0.8456056714057922\n",
      "Training iter #1200000:   Batch Loss = 0.351885, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8462973833084106, Accuracy = 0.8673226833343506\n",
      "Optimization Finished!\n",
      "Training time is: 2391.5787525177 seconds\n",
      "FINAL RESULT: Batch Loss = 0.8612674474716187, Accuracy = 0.8310145735740662\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.533360, Accuracy = 0.046666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9243950843811035, Accuracy = 0.12996266782283783\n",
      "Training iter #300000:   Batch Loss = 0.668665, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9307831525802612, Accuracy = 0.8853070735931396\n",
      "Training iter #600000:   Batch Loss = 0.584637, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8232210874557495, Accuracy = 0.8741092681884766\n",
      "Training iter #900000:   Batch Loss = 0.442945, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7275118827819824, Accuracy = 0.9110960364341736\n",
      "Training iter #1200000:   Batch Loss = 0.377252, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6346343755722046, Accuracy = 0.9165253043174744\n",
      "Training iter #1500000:   Batch Loss = 0.346991, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6384512186050415, Accuracy = 0.8978622555732727\n",
      "Optimization Finished!\n",
      "Training time is: 3418.066894054413 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6310596466064453, Accuracy = 0.8934509754180908\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.730992, Accuracy = 0.10066666454076767\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5577521324157715, Accuracy = 0.1879877895116806\n",
      "Training iter #300000:   Batch Loss = 0.621689, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.109106421470642, Accuracy = 0.8635901212692261\n",
      "Training iter #600000:   Batch Loss = 0.470522, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7825039625167847, Accuracy = 0.9046487808227539\n",
      "Training iter #900000:   Batch Loss = 0.375279, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7409261465072632, Accuracy = 0.8985409140586853\n",
      "Training iter #1200000:   Batch Loss = 0.355753, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6562833786010742, Accuracy = 0.9063454270362854\n",
      "Training iter #1500000:   Batch Loss = 0.326581, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.634225606918335, Accuracy = 0.9039701223373413\n",
      "Optimization Finished!\n",
      "Training time is: 4313.305387973785 seconds\n",
      "FINAL RESULT: Batch Loss = 0.616037905216217, Accuracy = 0.8870037198066711\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.140202, Accuracy = 0.014666666276752949\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.69270920753479, Accuracy = 0.20054292678833008\n",
      "Training iter #300000:   Batch Loss = 0.623390, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9067165851593018, Accuracy = 0.8934509754180908\n",
      "Training iter #600000:   Batch Loss = 0.529670, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7904893159866333, Accuracy = 0.9161859750747681\n",
      "Training iter #900000:   Batch Loss = 0.430408, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7366747260093689, Accuracy = 0.9009161591529846\n",
      "Training iter #1200000:   Batch Loss = 0.380439, Accuracy = 0.9573333263397217\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6727476119995117, Accuracy = 0.9022734761238098\n",
      "Training iter #1500000:   Batch Loss = 0.357254, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6237324476242065, Accuracy = 0.9049881100654602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1800000:   Batch Loss = 0.338701, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6614893078804016, Accuracy = 0.8747879266738892\n",
      "Optimization Finished!\n",
      "Training time is: 5219.629273891449 seconds\n",
      "FINAL RESULT: Batch Loss = 0.5385497212409973, Accuracy = 0.907024085521698\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.826427, Accuracy = 0.1326666623353958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.1035983562469482, Accuracy = 0.11435358226299286\n",
      "Training iter #300000:   Batch Loss = 0.615872, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0522265434265137, Accuracy = 0.8839497566223145\n",
      "Training iter #600000:   Batch Loss = 0.598796, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.147977590560913, Accuracy = 0.8371224999427795\n",
      "Training iter #900000:   Batch Loss = 0.432453, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8450181484222412, Accuracy = 0.8849677443504333\n",
      "Training iter #1200000:   Batch Loss = 0.390088, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8595696687698364, Accuracy = 0.8859857320785522\n",
      "Training iter #1500000:   Batch Loss = 0.358597, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8822178244590759, Accuracy = 0.8642687201499939\n",
      "Training iter #1800000:   Batch Loss = 0.328816, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5702968835830688, Accuracy = 0.8873430490493774\n",
      "Training iter #2100000:   Batch Loss = 0.256542, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.641180694103241, Accuracy = 0.8941296339035034\n",
      "Optimization Finished!\n",
      "Training time is: 7049.133264064789 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6167262196540833, Accuracy = 0.9026128053665161\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[201.77709484100342, 340.67320585250854, 577.2064092159271, 1101.1747999191284, 1800.5067710876465, 2391.5787525177, 3418.066894054413, 4313.305387973785, 5219.629273891449, 7049.133264064789]\n",
      "[0.6728877, 0.82660335, 0.7936885, 0.8333899, 0.8540889, 0.8310146, 0.893451, 0.8870037, 0.9070241, 0.9026128]\n",
      "[2.467919, 1.2702674, 0.92370623, 0.90663975, 0.8306991, 0.86126745, 0.63105965, 0.6160379, 0.5385497, 0.6167262]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 Part 2 of End-to-end training with the baseline model model 1.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset \n",
    "# under different number of training iterations.\n",
    "# Based on the code of Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "# Define LSTM and other helper functions\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32 # Hidden layer num of features\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 350  # Loop 350 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "\n",
    "    # Training the neural net\n",
    "\n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    # Graph weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final list for manuscript ploting later\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[runing_time_list[i], accuracy_list[i],loss_list[i]] for i in range(0,len(runing_time_list))])\n",
    "sheet.save_as(\"LSTM_Mod_Time_ACC_LOSS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "WARNING:tensorflow:From c:\\users\\edwar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-7a56f3e44648>:215: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.836260, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6535658836364746, Accuracy = 0.18221920728683472\n",
      "Training iter #300000:   Batch Loss = 0.595414, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8986382484436035, Accuracy = 0.8893790245056152\n",
      "Training iter #600000:   Batch Loss = 0.509351, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8072294592857361, Accuracy = 0.9019341468811035\n",
      "Training iter #900000:   Batch Loss = 0.589843, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8774027824401855, Accuracy = 0.7980997562408447\n",
      "Training iter #1200000:   Batch Loss = 0.363674, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.616554319858551, Accuracy = 0.9100780487060547\n",
      "Training iter #1500000:   Batch Loss = 0.316098, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6069691777229309, Accuracy = 0.8893790245056152\n",
      "Training iter #1800000:   Batch Loss = 0.281656, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47027331590652466, Accuracy = 0.9239904880523682\n",
      "Training iter #2100000:   Batch Loss = 0.218557, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46420300006866455, Accuracy = 0.9158466458320618\n",
      "Training iter #2400000:   Batch Loss = 0.307638, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48463284969329834, Accuracy = 0.8992195725440979\n",
      "Training iter #2700000:   Batch Loss = 0.245386, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5490131378173828, Accuracy = 0.8887003660202026\n",
      "Optimization Finished!\n",
      "Training time is: 2688.795155763626 seconds\n",
      "FINAL RESULT: Batch Loss = 0.47376710176467896, Accuracy = 0.8805565237998962\n",
      "Testing Accuracy: 88.05565237998962%\n",
      "\n",
      "Precision: 88.34442250709057%\n",
      "Recall: 88.05564981336953%\n",
      "f1_score: 88.05398036391259%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[479  15   2   0   0   0]\n",
      " [ 12 434  25   0   0   0]\n",
      " [ 18  12 390   0   0   0]\n",
      " [  0   0   0 406  85   0]\n",
      " [  2   0   0 154 376   0]\n",
      " [  0  27   0   0   0 510]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[16.253817    0.5089922   0.06786563  0.          0.          0.        ]\n",
      " [ 0.40719375 14.726841    0.8483203   0.          0.          0.        ]\n",
      " [ 0.6107906   0.40719375 13.233797    0.          0.          0.        ]\n",
      " [ 0.          0.          0.         13.776723    2.884289    0.        ]\n",
      " [ 0.06786563  0.          0.          5.225653   12.758738    0.        ]\n",
      " [ 0.          0.916186    0.          0.          0.         17.305735  ]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix to take a look\n",
    "# Based on the code of Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "# Define LSTM and other helper functions\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset, some of the credits goes to \n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32 # Hidden layer num of features\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 400  # Loop 400 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "\n",
    "    # Training the neural net\n",
    "\n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    # Graph weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "    \n",
    "    predictions = one_hot_predictions.argmax(1)\n",
    "    \n",
    "    print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    print(confusion_matrix)\n",
    "    normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "    print(normalised_confusion_matrix)\n",
    "    print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "    print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "    \n",
    "    # Plot Results: \n",
    "    width = 12\n",
    "    height = 12\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(\n",
    "        normalised_confusion_matrix, \n",
    "        interpolation='nearest', \n",
    "        cmap=plt.cm.rainbow\n",
    "    )\n",
    "    plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(n_classes)\n",
    "    plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "    plt.yticks(tick_marks, LABELS)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "f = plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "f.savefig(\"lstm_conf.pdf\", bbox_inches='tight')\n",
    "f.savefig(\"lstm_conf.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
