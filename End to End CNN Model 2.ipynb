{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.995123, Accuracy = 0.2160000056028366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9745306968688965, Accuracy = 0.331523597240448\n",
      "Training iter #30000:   Batch Loss = 2.593151, Accuracy = 0.36266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6241605281829834, Accuracy = 0.3362741768360138\n",
      "Training iter #60000:   Batch Loss = 2.228961, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3118982315063477, Accuracy = 0.554462194442749\n",
      "Training iter #90000:   Batch Loss = 1.897295, Accuracy = 0.7879999876022339\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.049118995666504, Accuracy = 0.6769596338272095\n",
      "Training iter #120000:   Batch Loss = 1.582014, Accuracy = 0.8506666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.8196260929107666, Accuracy = 0.6735663414001465\n",
      "Training iter #150000:   Batch Loss = 1.332594, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.663738489151001, Accuracy = 0.6813709139823914\n",
      "Training iter #180000:   Batch Loss = 1.155768, Accuracy = 0.9306666851043701\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5724763870239258, Accuracy = 0.6817102432250977\n",
      "Training iter #210000:   Batch Loss = 1.005552, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5180566310882568, Accuracy = 0.6878181099891663\n",
      "Optimization Finished!\n",
      "Training time is: 77.07421851158142 seconds\n",
      "FINAL RESULT: Batch Loss = 1.5015966892242432, Accuracy = 0.689854085445404\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.007003, Accuracy = 0.07133333384990692\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.967952013015747, Accuracy = 0.16694943606853485\n",
      "Training iter #30000:   Batch Loss = 2.608605, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6478047370910645, Accuracy = 0.390227347612381\n",
      "Training iter #60000:   Batch Loss = 2.275801, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3430325984954834, Accuracy = 0.61588054895401\n",
      "Training iter #90000:   Batch Loss = 1.895329, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.994086503982544, Accuracy = 0.6868001222610474\n",
      "Training iter #120000:   Batch Loss = 1.590231, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7269370555877686, Accuracy = 0.7098744511604309\n",
      "Training iter #150000:   Batch Loss = 1.399417, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5598013401031494, Accuracy = 0.7363420724868774\n",
      "Training iter #180000:   Batch Loss = 1.257051, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4366556406021118, Accuracy = 0.7529691457748413\n",
      "Training iter #210000:   Batch Loss = 1.116835, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3387346267700195, Accuracy = 0.7712928652763367\n",
      "Training iter #240000:   Batch Loss = 1.005113, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2614936828613281, Accuracy = 0.7774007320404053\n",
      "Training iter #270000:   Batch Loss = 0.907678, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2005126476287842, Accuracy = 0.7852053046226501\n",
      "Training iter #300000:   Batch Loss = 0.822850, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1495251655578613, Accuracy = 0.791652500629425\n",
      "Training iter #330000:   Batch Loss = 0.756451, Accuracy = 0.9293333292007446\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1108849048614502, Accuracy = 0.7933491468429565\n",
      "Training iter #360000:   Batch Loss = 0.683820, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.075364351272583, Accuracy = 0.7997964024543762\n",
      "Training iter #390000:   Batch Loss = 0.626504, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0446134805679321, Accuracy = 0.801832377910614\n",
      "Training iter #420000:   Batch Loss = 0.579319, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0166795253753662, Accuracy = 0.8038683533668518\n",
      "Optimization Finished!\n",
      "Training time is: 149.82840013504028 seconds\n",
      "FINAL RESULT: Batch Loss = 0.9991534948348999, Accuracy = 0.8065829873085022\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.998817, Accuracy = 0.16866666078567505\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.961399555206299, Accuracy = 0.17645062506198883\n",
      "Training iter #30000:   Batch Loss = 2.626046, Accuracy = 0.34599998593330383\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.650026321411133, Accuracy = 0.30641329288482666\n",
      "Training iter #60000:   Batch Loss = 2.348788, Accuracy = 0.5986666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3706212043762207, Accuracy = 0.5751611590385437\n",
      "Training iter #90000:   Batch Loss = 1.993973, Accuracy = 0.656000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.047285318374634, Accuracy = 0.6121479272842407\n",
      "Training iter #120000:   Batch Loss = 1.759759, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7724692821502686, Accuracy = 0.6308109760284424\n",
      "Training iter #150000:   Batch Loss = 1.540453, Accuracy = 0.7293333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6026514768600464, Accuracy = 0.6599932312965393\n",
      "Training iter #180000:   Batch Loss = 1.403799, Accuracy = 0.8053333163261414\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4813563823699951, Accuracy = 0.7183576226234436\n",
      "Training iter #210000:   Batch Loss = 1.293171, Accuracy = 0.8113333582878113\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3795415163040161, Accuracy = 0.7356634140014648\n",
      "Training iter #240000:   Batch Loss = 1.191408, Accuracy = 0.8213333487510681\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.288130521774292, Accuracy = 0.7583983540534973\n",
      "Training iter #270000:   Batch Loss = 1.103013, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2061913013458252, Accuracy = 0.7729894518852234\n",
      "Training iter #300000:   Batch Loss = 1.020095, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1312072277069092, Accuracy = 0.7919918298721313\n",
      "Training iter #330000:   Batch Loss = 0.919694, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0629668235778809, Accuracy = 0.7987784147262573\n",
      "Training iter #360000:   Batch Loss = 0.846314, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0025714635849, Accuracy = 0.8072616457939148\n",
      "Training iter #390000:   Batch Loss = 0.815568, Accuracy = 0.8899999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9482072591781616, Accuracy = 0.8177807927131653\n",
      "Training iter #420000:   Batch Loss = 0.735967, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9031851291656494, Accuracy = 0.8201560974121094\n",
      "Training iter #450000:   Batch Loss = 0.705698, Accuracy = 0.9160000085830688\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8631553053855896, Accuracy = 0.8259246945381165\n",
      "Training iter #480000:   Batch Loss = 0.639244, Accuracy = 0.9253333210945129\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8290613889694214, Accuracy = 0.8340685367584229\n",
      "Training iter #510000:   Batch Loss = 0.582662, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8010833859443665, Accuracy = 0.8381404876708984\n",
      "Training iter #540000:   Batch Loss = 0.592782, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7758210897445679, Accuracy = 0.8425517678260803\n",
      "Training iter #570000:   Batch Loss = 0.523513, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7582112550735474, Accuracy = 0.8439090847969055\n",
      "Training iter #600000:   Batch Loss = 0.527175, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.740959644317627, Accuracy = 0.8517135977745056\n",
      "Training iter #630000:   Batch Loss = 0.480407, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7268134355545044, Accuracy = 0.8564642071723938\n",
      "Training iter #660000:   Batch Loss = 0.435244, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7153093814849854, Accuracy = 0.8595181703567505\n",
      "Optimization Finished!\n",
      "Training time is: 224.99076628684998 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7142071723937988, Accuracy = 0.8574821949005127\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.009429, Accuracy = 0.1340000033378601\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9790096282958984, Accuracy = 0.30878859758377075\n",
      "Training iter #30000:   Batch Loss = 2.556627, Accuracy = 0.3440000116825104\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5376811027526855, Accuracy = 0.36070579290390015\n",
      "Training iter #60000:   Batch Loss = 2.192877, Accuracy = 0.7573333382606506\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.222568988800049, Accuracy = 0.6549032926559448\n",
      "Training iter #90000:   Batch Loss = 1.876377, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.959097981452942, Accuracy = 0.7309128046035767\n",
      "Training iter #120000:   Batch Loss = 1.593055, Accuracy = 0.8679999709129333\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7387723922729492, Accuracy = 0.7373600006103516\n",
      "Training iter #150000:   Batch Loss = 1.396731, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5809762477874756, Accuracy = 0.7468612194061279\n",
      "Training iter #180000:   Batch Loss = 1.214224, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.470005750656128, Accuracy = 0.7621309757232666\n",
      "Training iter #210000:   Batch Loss = 1.121963, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3869812488555908, Accuracy = 0.7655242681503296\n",
      "Training iter #240000:   Batch Loss = 1.053009, Accuracy = 0.8706666827201843\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.314798355102539, Accuracy = 0.7719715237617493\n",
      "Training iter #270000:   Batch Loss = 0.918904, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2519285678863525, Accuracy = 0.783847987651825\n",
      "Training iter #300000:   Batch Loss = 0.847082, Accuracy = 0.921999990940094\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1966667175292969, Accuracy = 0.7991177439689636\n",
      "Training iter #330000:   Batch Loss = 0.753303, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1446917057037354, Accuracy = 0.810654878616333\n",
      "Training iter #360000:   Batch Loss = 0.723316, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1004236936569214, Accuracy = 0.8184594511985779\n",
      "Training iter #390000:   Batch Loss = 0.711695, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0604064464569092, Accuracy = 0.8289785981178284\n",
      "Training iter #420000:   Batch Loss = 0.621813, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0242271423339844, Accuracy = 0.8357651829719543\n",
      "Training iter #450000:   Batch Loss = 0.580789, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9923509955406189, Accuracy = 0.8398371338844299\n",
      "Training iter #480000:   Batch Loss = 0.523802, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9621879458427429, Accuracy = 0.8459450006484985\n",
      "Training iter #510000:   Batch Loss = 0.519022, Accuracy = 0.940666675567627\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9370179176330566, Accuracy = 0.8527315855026245\n",
      "Training iter #540000:   Batch Loss = 0.526241, Accuracy = 0.9273333549499512\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9150948524475098, Accuracy = 0.8530709147453308\n",
      "Training iter #570000:   Batch Loss = 0.460207, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8951379060745239, Accuracy = 0.8551068902015686\n",
      "Training iter #600000:   Batch Loss = 0.430786, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8774740695953369, Accuracy = 0.8568035364151001\n",
      "Training iter #630000:   Batch Loss = 0.392505, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8600974082946777, Accuracy = 0.8588395118713379\n",
      "Training iter #660000:   Batch Loss = 0.400335, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8460496068000793, Accuracy = 0.8608754873275757\n",
      "Training iter #690000:   Batch Loss = 0.417099, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8349220752716064, Accuracy = 0.8618934750556946\n",
      "Training iter #720000:   Batch Loss = 0.359374, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8240440487861633, Accuracy = 0.8635901212692261\n",
      "Training iter #750000:   Batch Loss = 0.342529, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8145398497581482, Accuracy = 0.8652867078781128\n",
      "Training iter #780000:   Batch Loss = 0.311532, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8043585419654846, Accuracy = 0.8680013418197632\n",
      "Training iter #810000:   Batch Loss = 0.327130, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.796936571598053, Accuracy = 0.8690193295478821\n",
      "Training iter #840000:   Batch Loss = 0.349441, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7921712398529053, Accuracy = 0.8686800003051758\n",
      "Training iter #870000:   Batch Loss = 0.300712, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7863782644271851, Accuracy = 0.870037317276001\n",
      "Optimization Finished!\n",
      "Training time is: 297.03854608535767 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7825644016265869, Accuracy = 0.8717339634895325\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.995916, Accuracy = 0.18133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.968111991882324, Accuracy = 0.16966407001018524\n",
      "Training iter #30000:   Batch Loss = 2.643365, Accuracy = 0.39133334159851074\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.625164747238159, Accuracy = 0.34679335355758667\n",
      "Training iter #60000:   Batch Loss = 2.409868, Accuracy = 0.44333332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4230010509490967, Accuracy = 0.40006786584854126\n",
      "Training iter #90000:   Batch Loss = 2.174486, Accuracy = 0.6506666541099548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.2005200386047363, Accuracy = 0.6046827435493469\n",
      "Training iter #120000:   Batch Loss = 1.921643, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.9630699157714844, Accuracy = 0.6766203045845032\n",
      "Training iter #150000:   Batch Loss = 1.743679, Accuracy = 0.7353333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7761354446411133, Accuracy = 0.7013912200927734\n",
      "Training iter #180000:   Batch Loss = 1.609507, Accuracy = 0.7379999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6008508205413818, Accuracy = 0.7139464020729065\n",
      "Training iter #210000:   Batch Loss = 1.412439, Accuracy = 0.777999997138977\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4187207221984863, Accuracy = 0.7258228659629822\n",
      "Training iter #240000:   Batch Loss = 1.217281, Accuracy = 0.7933333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2563724517822266, Accuracy = 0.742449939250946\n",
      "Training iter #270000:   Batch Loss = 1.056330, Accuracy = 0.843999981880188\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1366558074951172, Accuracy = 0.7753647565841675\n",
      "Training iter #300000:   Batch Loss = 0.936429, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0464611053466797, Accuracy = 0.8092975616455078\n",
      "Training iter #330000:   Batch Loss = 0.884702, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9725341796875, Accuracy = 0.8228707313537598\n",
      "Training iter #360000:   Batch Loss = 0.859296, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9076983332633972, Accuracy = 0.8313539028167725\n",
      "Training iter #390000:   Batch Loss = 0.799745, Accuracy = 0.8920000195503235\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8512265682220459, Accuracy = 0.8374618291854858\n",
      "Training iter #420000:   Batch Loss = 0.717612, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8011204600334167, Accuracy = 0.8408551216125488\n",
      "Training iter #450000:   Batch Loss = 0.630232, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7555627822875977, Accuracy = 0.8500169515609741\n",
      "Training iter #480000:   Batch Loss = 0.591303, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7173832654953003, Accuracy = 0.8496776223182678\n",
      "Training iter #510000:   Batch Loss = 0.546382, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6827079057693481, Accuracy = 0.8503562808036804\n",
      "Training iter #540000:   Batch Loss = 0.584634, Accuracy = 0.8953333497047424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6525393724441528, Accuracy = 0.8534102439880371\n",
      "Training iter #570000:   Batch Loss = 0.564692, Accuracy = 0.8793333172798157\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6275976896286011, Accuracy = 0.8551068902015686\n",
      "Training iter #600000:   Batch Loss = 0.507116, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6044090390205383, Accuracy = 0.8581608533859253\n",
      "Training iter #630000:   Batch Loss = 0.432245, Accuracy = 0.9366666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.584756076335907, Accuracy = 0.861214816570282\n",
      "Training iter #660000:   Batch Loss = 0.425847, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5676097869873047, Accuracy = 0.8622328042984009\n",
      "Training iter #690000:   Batch Loss = 0.403249, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5514774322509766, Accuracy = 0.8632507920265198\n",
      "Training iter #720000:   Batch Loss = 0.432176, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5377441048622131, Accuracy = 0.8625721335411072\n",
      "Training iter #750000:   Batch Loss = 0.449656, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5242502093315125, Accuracy = 0.8635901212692261\n",
      "Training iter #780000:   Batch Loss = 0.406175, Accuracy = 0.9039999842643738\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5126897096633911, Accuracy = 0.8652867078781128\n",
      "Training iter #810000:   Batch Loss = 0.339863, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5023577213287354, Accuracy = 0.8659653663635254\n",
      "Training iter #840000:   Batch Loss = 0.342327, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49162301421165466, Accuracy = 0.8656260371208191\n",
      "Training iter #870000:   Batch Loss = 0.323064, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4841405153274536, Accuracy = 0.8642687201499939\n",
      "Training iter #900000:   Batch Loss = 0.315413, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4759681224822998, Accuracy = 0.8663046956062317\n",
      "Training iter #930000:   Batch Loss = 0.387426, Accuracy = 0.8926666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46941667795181274, Accuracy = 0.866644024848938\n",
      "Training iter #960000:   Batch Loss = 0.345209, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46277350187301636, Accuracy = 0.8690193295478821\n",
      "Training iter #990000:   Batch Loss = 0.275398, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4551924169063568, Accuracy = 0.8690193295478821\n",
      "Training iter #1020000:   Batch Loss = 0.286102, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4498606324195862, Accuracy = 0.870037317276001\n",
      "Training iter #1050000:   Batch Loss = 0.289388, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44465792179107666, Accuracy = 0.870037317276001\n",
      "Training iter #1080000:   Batch Loss = 0.271924, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.440631240606308, Accuracy = 0.8707159757614136\n",
      "Optimization Finished!\n",
      "Training time is: 392.9003310203552 seconds\n",
      "FINAL RESULT: Batch Loss = 0.4360034167766571, Accuracy = 0.8717339634895325\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.996255, Accuracy = 0.2913333475589752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9692392349243164, Accuracy = 0.18187987804412842\n",
      "Training iter #30000:   Batch Loss = 2.644791, Accuracy = 0.3766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6170780658721924, Accuracy = 0.40549710392951965\n",
      "Training iter #60000:   Batch Loss = 2.254333, Accuracy = 0.687333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.291278839111328, Accuracy = 0.6325076222419739\n",
      "Training iter #90000:   Batch Loss = 1.919355, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.973315715789795, Accuracy = 0.6834068298339844\n",
      "Training iter #120000:   Batch Loss = 1.584038, Accuracy = 0.8653333187103271\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7059160470962524, Accuracy = 0.7750254273414612\n",
      "Training iter #150000:   Batch Loss = 1.413280, Accuracy = 0.809333324432373\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5102503299713135, Accuracy = 0.8137088418006897\n",
      "Training iter #180000:   Batch Loss = 1.317343, Accuracy = 0.8253333568572998\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3673474788665771, Accuracy = 0.8313539028167725\n",
      "Training iter #210000:   Batch Loss = 1.119320, Accuracy = 0.9300000071525574\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2582809925079346, Accuracy = 0.8384798169136047\n",
      "Training iter #240000:   Batch Loss = 1.024608, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.17138671875, Accuracy = 0.8411944508552551\n",
      "Training iter #270000:   Batch Loss = 0.881080, Accuracy = 0.9346666932106018\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.098597764968872, Accuracy = 0.8405157923698425\n",
      "Training iter #300000:   Batch Loss = 0.906297, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0354292392730713, Accuracy = 0.8432304263114929\n",
      "Training iter #330000:   Batch Loss = 0.878132, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9790847897529602, Accuracy = 0.8439090847969055\n",
      "Training iter #360000:   Batch Loss = 0.719873, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9306049346923828, Accuracy = 0.8439090847969055\n",
      "Training iter #390000:   Batch Loss = 0.699408, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8871016502380371, Accuracy = 0.8456056714057922\n",
      "Training iter #420000:   Batch Loss = 0.609724, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.849379301071167, Accuracy = 0.8462843298912048\n",
      "Training iter #450000:   Batch Loss = 0.664144, Accuracy = 0.8993333578109741\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8146041631698608, Accuracy = 0.84764164686203\n",
      "Training iter #480000:   Batch Loss = 0.655942, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7808869481086731, Accuracy = 0.8459450006484985\n",
      "Training iter #510000:   Batch Loss = 0.524614, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7530876994132996, Accuracy = 0.8459450006484985\n",
      "Training iter #540000:   Batch Loss = 0.526795, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7280275821685791, Accuracy = 0.8469629883766174\n",
      "Training iter #570000:   Batch Loss = 0.465273, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7064864635467529, Accuracy = 0.8479809761047363\n",
      "Training iter #600000:   Batch Loss = 0.523376, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6853170394897461, Accuracy = 0.8486596345901489\n",
      "Training iter #630000:   Batch Loss = 0.525738, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.665256917476654, Accuracy = 0.8489989638328552\n",
      "Training iter #660000:   Batch Loss = 0.409508, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6481172442436218, Accuracy = 0.8517135977745056\n",
      "Training iter #690000:   Batch Loss = 0.425514, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6324869990348816, Accuracy = 0.8534102439880371\n",
      "Training iter #720000:   Batch Loss = 0.379150, Accuracy = 0.9466666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6202207207679749, Accuracy = 0.8530709147453308\n",
      "Training iter #750000:   Batch Loss = 0.437091, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6075361967086792, Accuracy = 0.8537495732307434\n",
      "Training iter #780000:   Batch Loss = 0.447765, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5948444604873657, Accuracy = 0.8534102439880371\n",
      "Training iter #810000:   Batch Loss = 0.338350, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.583854079246521, Accuracy = 0.8534102439880371\n",
      "Training iter #840000:   Batch Loss = 0.362120, Accuracy = 0.9393333196640015\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5746017098426819, Accuracy = 0.8537495732307434\n",
      "Training iter #870000:   Batch Loss = 0.321871, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5661973357200623, Accuracy = 0.8540889024734497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #900000:   Batch Loss = 0.359405, Accuracy = 0.9286666512489319\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5578607320785522, Accuracy = 0.8547675609588623\n",
      "Training iter #930000:   Batch Loss = 0.393513, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5502657294273376, Accuracy = 0.8540889024734497\n",
      "Training iter #960000:   Batch Loss = 0.290420, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5428791046142578, Accuracy = 0.8561248779296875\n",
      "Training iter #990000:   Batch Loss = 0.321981, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.536733865737915, Accuracy = 0.8554462194442749\n",
      "Training iter #1020000:   Batch Loss = 0.286892, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5311964750289917, Accuracy = 0.8547675609588623\n",
      "Training iter #1050000:   Batch Loss = 0.285419, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5254560708999634, Accuracy = 0.8557855486869812\n",
      "Training iter #1080000:   Batch Loss = 0.356795, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5202904939651489, Accuracy = 0.8561248779296875\n",
      "Training iter #1110000:   Batch Loss = 0.252115, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5159251093864441, Accuracy = 0.8571428656578064\n",
      "Training iter #1140000:   Batch Loss = 0.293931, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5105959177017212, Accuracy = 0.8574821949005127\n",
      "Training iter #1170000:   Batch Loss = 0.260978, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5080922842025757, Accuracy = 0.8571428656578064\n",
      "Training iter #1200000:   Batch Loss = 0.241439, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.503623366355896, Accuracy = 0.8574821949005127\n",
      "Training iter #1230000:   Batch Loss = 0.329345, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5004968643188477, Accuracy = 0.8581608533859253\n",
      "Training iter #1260000:   Batch Loss = 0.226419, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49728381633758545, Accuracy = 0.8585001826286316\n",
      "Training iter #1290000:   Batch Loss = 0.271013, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4929550588130951, Accuracy = 0.8591788411140442\n",
      "Training iter #1320000:   Batch Loss = 0.238597, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49130672216415405, Accuracy = 0.8585001826286316\n",
      "Optimization Finished!\n",
      "Training time is: 461.91923928260803 seconds\n",
      "FINAL RESULT: Batch Loss = 0.49003249406814575, Accuracy = 0.8574821949005127\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.998719, Accuracy = 0.1599999964237213\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9758880138397217, Accuracy = 0.16898541152477264\n",
      "Training iter #30000:   Batch Loss = 2.686248, Accuracy = 0.3720000088214874\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.652369976043701, Accuracy = 0.3814048171043396\n",
      "Training iter #60000:   Batch Loss = 2.372555, Accuracy = 0.6426666378974915\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3655993938446045, Accuracy = 0.5948421955108643\n",
      "Training iter #90000:   Batch Loss = 1.968276, Accuracy = 0.6653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.0376944541931152, Accuracy = 0.6152018904685974\n",
      "Training iter #120000:   Batch Loss = 1.686755, Accuracy = 0.7260000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7482755184173584, Accuracy = 0.6613505482673645\n",
      "Training iter #150000:   Batch Loss = 1.445535, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.552819013595581, Accuracy = 0.7376993298530579\n",
      "Training iter #180000:   Batch Loss = 1.253559, Accuracy = 0.8586666584014893\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.407122015953064, Accuracy = 0.7678995728492737\n",
      "Training iter #210000:   Batch Loss = 1.224863, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2878082990646362, Accuracy = 0.801832377910614\n",
      "Training iter #240000:   Batch Loss = 1.125235, Accuracy = 0.8740000128746033\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.185497522354126, Accuracy = 0.8184594511985779\n",
      "Training iter #270000:   Batch Loss = 0.972808, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0975581407546997, Accuracy = 0.8303359150886536\n",
      "Training iter #300000:   Batch Loss = 0.907097, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0204355716705322, Accuracy = 0.838819146156311\n",
      "Training iter #330000:   Batch Loss = 0.774545, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9529393315315247, Accuracy = 0.8415337800979614\n",
      "Training iter #360000:   Batch Loss = 0.726523, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8939781188964844, Accuracy = 0.8517135977745056\n",
      "Training iter #390000:   Batch Loss = 0.783973, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8430855870246887, Accuracy = 0.8530709147453308\n",
      "Training iter #420000:   Batch Loss = 0.708183, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7992793321609497, Accuracy = 0.8557855486869812\n",
      "Training iter #450000:   Batch Loss = 0.618773, Accuracy = 0.9380000233650208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7613702416419983, Accuracy = 0.8557855486869812\n",
      "Training iter #480000:   Batch Loss = 0.586971, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7280505895614624, Accuracy = 0.8551068902015686\n",
      "Training iter #510000:   Batch Loss = 0.484852, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6989330649375916, Accuracy = 0.8537495732307434\n",
      "Training iter #540000:   Batch Loss = 0.489795, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6738471984863281, Accuracy = 0.8591788411140442\n",
      "Training iter #570000:   Batch Loss = 0.557775, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6516632437705994, Accuracy = 0.8568035364151001\n",
      "Training iter #600000:   Batch Loss = 0.472022, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6322706937789917, Accuracy = 0.8568035364151001\n",
      "Training iter #630000:   Batch Loss = 0.445882, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6146219968795776, Accuracy = 0.8591788411140442\n",
      "Training iter #660000:   Batch Loss = 0.412357, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5990481376647949, Accuracy = 0.8618934750556946\n",
      "Training iter #690000:   Batch Loss = 0.338752, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.584942102432251, Accuracy = 0.8622328042984009\n",
      "Training iter #720000:   Batch Loss = 0.381337, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5727563500404358, Accuracy = 0.8605361580848694\n",
      "Training iter #750000:   Batch Loss = 0.453700, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5616257190704346, Accuracy = 0.8632507920265198\n",
      "Training iter #780000:   Batch Loss = 0.363849, Accuracy = 0.9566666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.551314651966095, Accuracy = 0.8649473786354065\n",
      "Training iter #810000:   Batch Loss = 0.359606, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5419572591781616, Accuracy = 0.8659653663635254\n",
      "Training iter #840000:   Batch Loss = 0.324475, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5332992076873779, Accuracy = 0.870037317276001\n",
      "Training iter #870000:   Batch Loss = 0.288113, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5258415341377258, Accuracy = 0.8713946342468262\n",
      "Training iter #900000:   Batch Loss = 0.346212, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5185332894325256, Accuracy = 0.8724126219749451\n",
      "Training iter #930000:   Batch Loss = 0.396024, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5120959281921387, Accuracy = 0.8727519512176514\n",
      "Training iter #960000:   Batch Loss = 0.291454, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5060284733772278, Accuracy = 0.8737699389457703\n",
      "Training iter #990000:   Batch Loss = 0.312499, Accuracy = 0.949999988079071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5001451373100281, Accuracy = 0.8724126219749451\n",
      "Training iter #1020000:   Batch Loss = 0.270975, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4948287308216095, Accuracy = 0.8747879266738892\n",
      "Training iter #1050000:   Batch Loss = 0.245308, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4905329942703247, Accuracy = 0.8754665851593018\n",
      "Training iter #1080000:   Batch Loss = 0.350678, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4851522743701935, Accuracy = 0.8758059144020081\n",
      "Training iter #1110000:   Batch Loss = 0.356566, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48116618394851685, Accuracy = 0.8781812191009521\n",
      "Training iter #1140000:   Batch Loss = 0.258902, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4772776663303375, Accuracy = 0.876823902130127\n",
      "Training iter #1170000:   Batch Loss = 0.278091, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4738537669181824, Accuracy = 0.879199206829071\n",
      "Training iter #1200000:   Batch Loss = 0.245096, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46965163946151733, Accuracy = 0.8785205483436584\n",
      "Training iter #1230000:   Batch Loss = 0.233101, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4668598175048828, Accuracy = 0.8798778653144836\n",
      "Training iter #1260000:   Batch Loss = 0.340000, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46303415298461914, Accuracy = 0.8785205483436584\n",
      "Training iter #1290000:   Batch Loss = 0.335837, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46040692925453186, Accuracy = 0.8798778653144836\n",
      "Training iter #1320000:   Batch Loss = 0.242539, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45763736963272095, Accuracy = 0.8802171945571899\n",
      "Training iter #1350000:   Batch Loss = 0.243733, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4559078812599182, Accuracy = 0.8798778653144836\n",
      "Training iter #1380000:   Batch Loss = 0.220624, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4530794620513916, Accuracy = 0.8805565237998962\n",
      "Training iter #1410000:   Batch Loss = 0.215287, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4509039521217346, Accuracy = 0.8802171945571899\n",
      "Training iter #1440000:   Batch Loss = 0.324961, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44816893339157104, Accuracy = 0.8795385360717773\n",
      "Training iter #1470000:   Batch Loss = 0.308177, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4472348988056183, Accuracy = 0.8802171945571899\n",
      "Training iter #1500000:   Batch Loss = 0.228106, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44489631056785583, Accuracy = 0.879199206829071\n",
      "Training iter #1530000:   Batch Loss = 0.228809, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4439154267311096, Accuracy = 0.8781812191009521\n",
      "Optimization Finished!\n",
      "Training time is: 516.0612478256226 seconds\n",
      "FINAL RESULT: Batch Loss = 0.44256389141082764, Accuracy = 0.8788598775863647\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.996274, Accuracy = 0.1313333362340927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.971630096435547, Accuracy = 0.17509330809116364\n",
      "Training iter #30000:   Batch Loss = 2.631880, Accuracy = 0.3479999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6079728603363037, Accuracy = 0.36138445138931274\n",
      "Training iter #60000:   Batch Loss = 2.307384, Accuracy = 0.5853333473205566\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.30930233001709, Accuracy = 0.5955208539962769\n",
      "Training iter #90000:   Batch Loss = 2.041254, Accuracy = 0.6006666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.047224998474121, Accuracy = 0.6355615854263306\n",
      "Training iter #120000:   Batch Loss = 1.822065, Accuracy = 0.5979999899864197\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.8135055303573608, Accuracy = 0.6464200615882874\n",
      "Training iter #150000:   Batch Loss = 1.676322, Accuracy = 0.6186666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6378490924835205, Accuracy = 0.649474024772644\n",
      "Training iter #180000:   Batch Loss = 1.513945, Accuracy = 0.6713333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.506842017173767, Accuracy = 0.6538853049278259\n",
      "Training iter #210000:   Batch Loss = 1.371554, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4042303562164307, Accuracy = 0.6613505482673645\n",
      "Training iter #240000:   Batch Loss = 1.223758, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.312408685684204, Accuracy = 0.6820495128631592\n",
      "Training iter #270000:   Batch Loss = 1.168170, Accuracy = 0.7559999823570251\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.233839511871338, Accuracy = 0.6901934146881104\n",
      "Training iter #300000:   Batch Loss = 1.098031, Accuracy = 0.7760000228881836\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.16237211227417, Accuracy = 0.7058025002479553\n",
      "Training iter #330000:   Batch Loss = 1.055882, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0964996814727783, Accuracy = 0.7180183529853821\n",
      "Training iter #360000:   Batch Loss = 0.946160, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0382587909698486, Accuracy = 0.7281981706619263\n",
      "Training iter #390000:   Batch Loss = 0.945020, Accuracy = 0.7986666560173035\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9882464408874512, Accuracy = 0.7360027432441711\n",
      "Training iter #420000:   Batch Loss = 0.857991, Accuracy = 0.8293333053588867\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9377392530441284, Accuracy = 0.7505938410758972\n",
      "Training iter #450000:   Batch Loss = 0.815196, Accuracy = 0.828000009059906\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8960763216018677, Accuracy = 0.7611129879951477\n",
      "Training iter #480000:   Batch Loss = 0.802198, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8543980121612549, Accuracy = 0.7777400612831116\n",
      "Training iter #510000:   Batch Loss = 0.763273, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8217236995697021, Accuracy = 0.7841873168945312\n",
      "Training iter #540000:   Batch Loss = 0.682393, Accuracy = 0.8619999885559082\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7893524169921875, Accuracy = 0.7923311591148376\n",
      "Training iter #570000:   Batch Loss = 0.685803, Accuracy = 0.8339999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7620335817337036, Accuracy = 0.7970817685127258\n",
      "Training iter #600000:   Batch Loss = 0.665833, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7342246770858765, Accuracy = 0.8008143901824951\n",
      "Training iter #630000:   Batch Loss = 0.637097, Accuracy = 0.8846666812896729\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7095019817352295, Accuracy = 0.8076009750366211\n",
      "Training iter #660000:   Batch Loss = 0.570790, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6849884390830994, Accuracy = 0.8225314021110535\n",
      "Training iter #690000:   Batch Loss = 0.594988, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6641746759414673, Accuracy = 0.8272819519042969\n",
      "Training iter #720000:   Batch Loss = 0.525435, Accuracy = 0.9113333225250244\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6457663774490356, Accuracy = 0.8269426822662354\n",
      "Training iter #750000:   Batch Loss = 0.519957, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6297957897186279, Accuracy = 0.8327112197875977\n",
      "Training iter #780000:   Batch Loss = 0.557284, Accuracy = 0.8686666488647461\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6128911972045898, Accuracy = 0.8364438414573669\n",
      "Training iter #810000:   Batch Loss = 0.530131, Accuracy = 0.8519999980926514\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6019856929779053, Accuracy = 0.8323718905448914\n",
      "Training iter #840000:   Batch Loss = 0.436849, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5918478965759277, Accuracy = 0.8361045122146606\n",
      "Training iter #870000:   Batch Loss = 0.438777, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5808364152908325, Accuracy = 0.838819146156311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #900000:   Batch Loss = 0.440753, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5735955238342285, Accuracy = 0.8398371338844299\n",
      "Training iter #930000:   Batch Loss = 0.425936, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5658952593803406, Accuracy = 0.8452664017677307\n",
      "Training iter #960000:   Batch Loss = 0.387774, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5573554039001465, Accuracy = 0.8500169515609741\n",
      "Training iter #990000:   Batch Loss = 0.422610, Accuracy = 0.9233333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5512704253196716, Accuracy = 0.8527315855026245\n",
      "Training iter #1020000:   Batch Loss = 0.369652, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5466711521148682, Accuracy = 0.854428231716156\n",
      "Training iter #1050000:   Batch Loss = 0.369719, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5424104928970337, Accuracy = 0.8561248779296875\n",
      "Training iter #1080000:   Batch Loss = 0.452690, Accuracy = 0.8600000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5356722474098206, Accuracy = 0.8574821949005127\n",
      "Training iter #1110000:   Batch Loss = 0.431162, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5338924527168274, Accuracy = 0.8554462194442749\n",
      "Training iter #1140000:   Batch Loss = 0.315786, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5302832126617432, Accuracy = 0.8574821949005127\n",
      "Training iter #1170000:   Batch Loss = 0.316602, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5251652002334595, Accuracy = 0.8585001826286316\n",
      "Training iter #1200000:   Batch Loss = 0.322462, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5218827128410339, Accuracy = 0.8585001826286316\n",
      "Training iter #1230000:   Batch Loss = 0.315321, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5195292830467224, Accuracy = 0.8581608533859253\n",
      "Training iter #1260000:   Batch Loss = 0.303269, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5140737891197205, Accuracy = 0.8622328042984009\n",
      "Training iter #1290000:   Batch Loss = 0.334567, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5115244388580322, Accuracy = 0.861214816570282\n",
      "Training iter #1320000:   Batch Loss = 0.295684, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5085911750793457, Accuracy = 0.8632507920265198\n",
      "Training iter #1350000:   Batch Loss = 0.294314, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5062990188598633, Accuracy = 0.8646080493927002\n",
      "Training iter #1380000:   Batch Loss = 0.395527, Accuracy = 0.8960000276565552\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5017470121383667, Accuracy = 0.8656260371208191\n",
      "Training iter #1410000:   Batch Loss = 0.377329, Accuracy = 0.9053333401679993\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5013737082481384, Accuracy = 0.8659653663635254\n",
      "Training iter #1440000:   Batch Loss = 0.258814, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4987613558769226, Accuracy = 0.8663046956062317\n",
      "Training iter #1470000:   Batch Loss = 0.261870, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49652090668678284, Accuracy = 0.8673226833343506\n",
      "Training iter #1500000:   Batch Loss = 0.266105, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49381139874458313, Accuracy = 0.8683406710624695\n",
      "Training iter #1530000:   Batch Loss = 0.257691, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4933217167854309, Accuracy = 0.8663046956062317\n",
      "Training iter #1560000:   Batch Loss = 0.259984, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4894063472747803, Accuracy = 0.8663046956062317\n",
      "Training iter #1590000:   Batch Loss = 0.290971, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48855069279670715, Accuracy = 0.866644024848938\n",
      "Training iter #1620000:   Batch Loss = 0.259687, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4855131506919861, Accuracy = 0.8673226833343506\n",
      "Training iter #1650000:   Batch Loss = 0.255608, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48537659645080566, Accuracy = 0.8673226833343506\n",
      "Training iter #1680000:   Batch Loss = 0.359775, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4822554588317871, Accuracy = 0.8680013418197632\n",
      "Training iter #1710000:   Batch Loss = 0.341238, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4828338921070099, Accuracy = 0.8652867078781128\n",
      "Training iter #1740000:   Batch Loss = 0.224472, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4799564480781555, Accuracy = 0.8669833540916443\n",
      "Optimization Finished!\n",
      "Training time is: 589.104679107666 seconds\n",
      "FINAL RESULT: Batch Loss = 0.47948235273361206, Accuracy = 0.8673226833343506\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.007509, Accuracy = 0.27000001072883606\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.980076789855957, Accuracy = 0.17916525900363922\n",
      "Training iter #30000:   Batch Loss = 2.682520, Accuracy = 0.3006666600704193\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.646097183227539, Accuracy = 0.3705463111400604\n",
      "Training iter #60000:   Batch Loss = 2.385845, Accuracy = 0.7039999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.384511709213257, Accuracy = 0.6464200615882874\n",
      "Training iter #90000:   Batch Loss = 2.076464, Accuracy = 0.7086666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.077143907546997, Accuracy = 0.693247377872467\n",
      "Training iter #120000:   Batch Loss = 1.728127, Accuracy = 0.8133333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7692291736602783, Accuracy = 0.7207329273223877\n",
      "Training iter #150000:   Batch Loss = 1.513634, Accuracy = 0.7753333449363708\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5528316497802734, Accuracy = 0.7709535360336304\n",
      "Training iter #180000:   Batch Loss = 1.285124, Accuracy = 0.8693333268165588\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4082608222961426, Accuracy = 0.7960637807846069\n",
      "Training iter #210000:   Batch Loss = 1.150169, Accuracy = 0.8646666407585144\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2983037233352661, Accuracy = 0.8048863410949707\n",
      "Training iter #240000:   Batch Loss = 1.064940, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2024283409118652, Accuracy = 0.8157448172569275\n",
      "Training iter #270000:   Batch Loss = 0.982678, Accuracy = 0.8913333415985107\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1229887008666992, Accuracy = 0.820834755897522\n",
      "Training iter #300000:   Batch Loss = 0.898466, Accuracy = 0.8859999775886536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0510166883468628, Accuracy = 0.8272819519042969\n",
      "Training iter #330000:   Batch Loss = 0.841771, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9921365976333618, Accuracy = 0.8316932320594788\n",
      "Training iter #360000:   Batch Loss = 0.795660, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9403046369552612, Accuracy = 0.8327112197875977\n",
      "Training iter #390000:   Batch Loss = 0.722943, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8965703845024109, Accuracy = 0.8333898782730103\n",
      "Training iter #420000:   Batch Loss = 0.733606, Accuracy = 0.8799999952316284\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8513875007629395, Accuracy = 0.8394978046417236\n",
      "Training iter #450000:   Batch Loss = 0.691129, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.81784987449646, Accuracy = 0.8415337800979614\n",
      "Training iter #480000:   Batch Loss = 0.694242, Accuracy = 0.887333333492279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7824817895889282, Accuracy = 0.8452664017677307\n",
      "Training iter #510000:   Batch Loss = 0.556578, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.752352237701416, Accuracy = 0.8473023176193237\n",
      "Training iter #540000:   Batch Loss = 0.611500, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.731278121471405, Accuracy = 0.8466236591339111\n",
      "Training iter #570000:   Batch Loss = 0.534106, Accuracy = 0.9359999895095825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7093895673751831, Accuracy = 0.8483203053474426\n",
      "Training iter #600000:   Batch Loss = 0.554449, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6845669746398926, Accuracy = 0.8513742685317993\n",
      "Training iter #630000:   Batch Loss = 0.475618, Accuracy = 0.9359999895095825\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6640064120292664, Accuracy = 0.8534102439880371\n",
      "Training iter #660000:   Batch Loss = 0.417502, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6462591886520386, Accuracy = 0.8571428656578064\n",
      "Training iter #690000:   Batch Loss = 0.435571, Accuracy = 0.9413333535194397\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6253464221954346, Accuracy = 0.8591788411140442\n",
      "Training iter #720000:   Batch Loss = 0.397259, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6126502752304077, Accuracy = 0.8601968288421631\n",
      "Training iter #750000:   Batch Loss = 0.405744, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6002942323684692, Accuracy = 0.8618934750556946\n",
      "Training iter #780000:   Batch Loss = 0.377583, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5877907872200012, Accuracy = 0.8618934750556946\n",
      "Training iter #810000:   Batch Loss = 0.382956, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.579047441482544, Accuracy = 0.8625721335411072\n",
      "Training iter #840000:   Batch Loss = 0.344880, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5701000094413757, Accuracy = 0.8639293909072876\n",
      "Training iter #870000:   Batch Loss = 0.433444, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5541106462478638, Accuracy = 0.8649473786354065\n",
      "Training iter #900000:   Batch Loss = 0.393224, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5446675419807434, Accuracy = 0.8656260371208191\n",
      "Training iter #930000:   Batch Loss = 0.435106, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5355868339538574, Accuracy = 0.8659653663635254\n",
      "Training iter #960000:   Batch Loss = 0.334286, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5266185402870178, Accuracy = 0.8656260371208191\n",
      "Training iter #990000:   Batch Loss = 0.411051, Accuracy = 0.909333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5210087299346924, Accuracy = 0.8669833540916443\n",
      "Training iter #1020000:   Batch Loss = 0.352365, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5187331438064575, Accuracy = 0.8676620125770569\n",
      "Training iter #1050000:   Batch Loss = 0.383079, Accuracy = 0.9079999923706055\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5096537470817566, Accuracy = 0.8693586587905884\n",
      "Training iter #1080000:   Batch Loss = 0.287817, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.503045380115509, Accuracy = 0.8696979880332947\n",
      "Training iter #1110000:   Batch Loss = 0.281009, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49758434295654297, Accuracy = 0.8707159757614136\n",
      "Training iter #1140000:   Batch Loss = 0.304329, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.488821417093277, Accuracy = 0.8741092681884766\n",
      "Training iter #1170000:   Batch Loss = 0.275508, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4848428964614868, Accuracy = 0.8741092681884766\n",
      "Training iter #1200000:   Batch Loss = 0.292683, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4817044138908386, Accuracy = 0.8754665851593018\n",
      "Training iter #1230000:   Batch Loss = 0.262507, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47677797079086304, Accuracy = 0.8764845728874207\n",
      "Training iter #1260000:   Batch Loss = 0.278016, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47497302293777466, Accuracy = 0.876823902130127\n",
      "Training iter #1290000:   Batch Loss = 0.245378, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4726598262786865, Accuracy = 0.8747879266738892\n",
      "Training iter #1320000:   Batch Loss = 0.350694, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4661411941051483, Accuracy = 0.879199206829071\n",
      "Training iter #1350000:   Batch Loss = 0.297517, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4612249433994293, Accuracy = 0.8788598775863647\n",
      "Training iter #1380000:   Batch Loss = 0.356135, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.457629919052124, Accuracy = 0.8802171945571899\n",
      "Training iter #1410000:   Batch Loss = 0.267898, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4538261890411377, Accuracy = 0.8785205483436584\n",
      "Training iter #1440000:   Batch Loss = 0.343509, Accuracy = 0.9133333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4515395760536194, Accuracy = 0.8812351822853088\n",
      "Training iter #1470000:   Batch Loss = 0.294721, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45265257358551025, Accuracy = 0.8808958530426025\n",
      "Training iter #1500000:   Batch Loss = 0.315760, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4490084946155548, Accuracy = 0.8802171945571899\n",
      "Training iter #1530000:   Batch Loss = 0.235743, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4464449882507324, Accuracy = 0.8819137811660767\n",
      "Training iter #1560000:   Batch Loss = 0.222037, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44322261214256287, Accuracy = 0.8819137811660767\n",
      "Training iter #1590000:   Batch Loss = 0.258017, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43967342376708984, Accuracy = 0.8808958530426025\n",
      "Training iter #1620000:   Batch Loss = 0.231291, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43741413950920105, Accuracy = 0.8819137811660767\n",
      "Training iter #1650000:   Batch Loss = 0.236697, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4362390637397766, Accuracy = 0.884628415107727\n",
      "Training iter #1680000:   Batch Loss = 0.218635, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43449416756629944, Accuracy = 0.8839497566223145\n",
      "Training iter #1710000:   Batch Loss = 0.233684, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43447694182395935, Accuracy = 0.8839497566223145\n",
      "Training iter #1740000:   Batch Loss = 0.246130, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4335734248161316, Accuracy = 0.8808958530426025\n",
      "Training iter #1770000:   Batch Loss = 0.320769, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4299813508987427, Accuracy = 0.8842890858650208\n",
      "Training iter #1800000:   Batch Loss = 0.256791, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4275413155555725, Accuracy = 0.884628415107727\n",
      "Training iter #1830000:   Batch Loss = 0.318320, Accuracy = 0.918666660785675\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4257187247276306, Accuracy = 0.8825924396514893\n",
      "Training iter #1860000:   Batch Loss = 0.239627, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42405152320861816, Accuracy = 0.882253110408783\n",
      "Training iter #1890000:   Batch Loss = 0.310784, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4223628342151642, Accuracy = 0.884628415107727\n",
      "Training iter #1920000:   Batch Loss = 0.272011, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42343229055404663, Accuracy = 0.8839497566223145\n",
      "Training iter #1950000:   Batch Loss = 0.235879, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42172449827194214, Accuracy = 0.8849677443504333\n",
      "Training iter #1980000:   Batch Loss = 0.218075, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42101961374282837, Accuracy = 0.884628415107727\n",
      "Optimization Finished!\n",
      "Training time is: 670.169819355011 seconds\n",
      "FINAL RESULT: Batch Loss = 0.4194723963737488, Accuracy = 0.8849677443504333\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.000058, Accuracy = 0.1979999989271164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.985668182373047, Accuracy = 0.17407533526420593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #30000:   Batch Loss = 2.724892, Accuracy = 0.512666642665863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.723933696746826, Accuracy = 0.45945027470588684\n",
      "Training iter #60000:   Batch Loss = 2.429998, Accuracy = 0.6006666421890259\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4557600021362305, Accuracy = 0.5595520734786987\n",
      "Training iter #90000:   Batch Loss = 2.113374, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.1497673988342285, Accuracy = 0.6589752435684204\n",
      "Training iter #120000:   Batch Loss = 1.775899, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.8389086723327637, Accuracy = 0.7376993298530579\n",
      "Training iter #150000:   Batch Loss = 1.538904, Accuracy = 0.7893333435058594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6045042276382446, Accuracy = 0.792670488357544\n",
      "Training iter #180000:   Batch Loss = 1.427059, Accuracy = 0.7093333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4367036819458008, Accuracy = 0.810654878616333\n",
      "Training iter #210000:   Batch Loss = 1.271996, Accuracy = 0.7326666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3049631118774414, Accuracy = 0.826603353023529\n",
      "Training iter #240000:   Batch Loss = 1.048792, Accuracy = 0.8513333201408386\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1947426795959473, Accuracy = 0.8367831707000732\n",
      "Training iter #270000:   Batch Loss = 0.959855, Accuracy = 0.8886666893959045\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1022965908050537, Accuracy = 0.8425517678260803\n",
      "Training iter #300000:   Batch Loss = 0.888566, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.021730661392212, Accuracy = 0.8462843298912048\n",
      "Training iter #330000:   Batch Loss = 0.884754, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9503433704376221, Accuracy = 0.8554462194442749\n",
      "Training iter #360000:   Batch Loss = 0.757895, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8900879621505737, Accuracy = 0.8571428656578064\n",
      "Training iter #390000:   Batch Loss = 0.722503, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8386926054954529, Accuracy = 0.8588395118713379\n",
      "Training iter #420000:   Batch Loss = 0.647536, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7938733100891113, Accuracy = 0.8622328042984009\n",
      "Training iter #450000:   Batch Loss = 0.626577, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7540735006332397, Accuracy = 0.8669833540916443\n",
      "Training iter #480000:   Batch Loss = 0.591134, Accuracy = 0.9246666431427002\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7193447351455688, Accuracy = 0.870037317276001\n",
      "Training iter #510000:   Batch Loss = 0.540410, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6893428564071655, Accuracy = 0.8703766465187073\n",
      "Training iter #540000:   Batch Loss = 0.603421, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6645291447639465, Accuracy = 0.8703766465187073\n",
      "Training iter #570000:   Batch Loss = 0.592926, Accuracy = 0.8539999723434448\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.642382025718689, Accuracy = 0.8737699389457703\n",
      "Training iter #600000:   Batch Loss = 0.556121, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6205439567565918, Accuracy = 0.8727519512176514\n",
      "Training iter #630000:   Batch Loss = 0.427191, Accuracy = 0.9319999814033508\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6009635925292969, Accuracy = 0.8744485974311829\n",
      "Training iter #660000:   Batch Loss = 0.422472, Accuracy = 0.9599999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5843346118927002, Accuracy = 0.8761452436447144\n",
      "Training iter #690000:   Batch Loss = 0.383115, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.57077956199646, Accuracy = 0.879199206829071\n",
      "Training iter #720000:   Batch Loss = 0.422216, Accuracy = 0.9513333439826965\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5572757720947266, Accuracy = 0.8812351822853088\n",
      "Training iter #750000:   Batch Loss = 0.403442, Accuracy = 0.9426666498184204\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5449535846710205, Accuracy = 0.8825924396514893\n",
      "Training iter #780000:   Batch Loss = 0.343728, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5313090085983276, Accuracy = 0.8795385360717773\n",
      "Training iter #810000:   Batch Loss = 0.366174, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5226031541824341, Accuracy = 0.8815745115280151\n",
      "Training iter #840000:   Batch Loss = 0.379611, Accuracy = 0.9340000152587891\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5145668387413025, Accuracy = 0.8836104273796082\n",
      "Training iter #870000:   Batch Loss = 0.337417, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5064582824707031, Accuracy = 0.8853070735931396\n",
      "Training iter #900000:   Batch Loss = 0.335829, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4992606043815613, Accuracy = 0.8839497566223145\n",
      "Training iter #930000:   Batch Loss = 0.428705, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4916529655456543, Accuracy = 0.8836104273796082\n",
      "Training iter #960000:   Batch Loss = 0.416499, Accuracy = 0.8713333606719971\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4860512614250183, Accuracy = 0.884628415107727\n",
      "Training iter #990000:   Batch Loss = 0.302668, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48052269220352173, Accuracy = 0.8849677443504333\n",
      "Training iter #1020000:   Batch Loss = 0.305912, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4752421975135803, Accuracy = 0.8859857320785522\n",
      "Training iter #1050000:   Batch Loss = 0.282320, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4689166843891144, Accuracy = 0.8853070735931396\n",
      "Training iter #1080000:   Batch Loss = 0.318797, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4638075828552246, Accuracy = 0.8859857320785522\n",
      "Training iter #1110000:   Batch Loss = 0.323064, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46046650409698486, Accuracy = 0.8863250613212585\n",
      "Training iter #1140000:   Batch Loss = 0.317520, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45628637075424194, Accuracy = 0.8866643905639648\n",
      "Training iter #1170000:   Batch Loss = 0.283679, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45159563422203064, Accuracy = 0.88802170753479\n",
      "Training iter #1200000:   Batch Loss = 0.294496, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4483974874019623, Accuracy = 0.8893790245056152\n",
      "Training iter #1230000:   Batch Loss = 0.273554, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44533324241638184, Accuracy = 0.8890396952629089\n",
      "Training iter #1260000:   Batch Loss = 0.264814, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4431184232234955, Accuracy = 0.8890396952629089\n",
      "Training iter #1290000:   Batch Loss = 0.348281, Accuracy = 0.903333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44014835357666016, Accuracy = 0.8890396952629089\n",
      "Training iter #1320000:   Batch Loss = 0.335686, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4373171031475067, Accuracy = 0.8887003660202026\n",
      "Training iter #1350000:   Batch Loss = 0.309478, Accuracy = 0.9106666445732117\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4352829158306122, Accuracy = 0.8897183537483215\n",
      "Training iter #1380000:   Batch Loss = 0.251546, Accuracy = 0.9520000219345093\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4335694909095764, Accuracy = 0.8887003660202026\n",
      "Training iter #1410000:   Batch Loss = 0.235812, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4305204749107361, Accuracy = 0.8897183537483215\n",
      "Training iter #1440000:   Batch Loss = 0.261990, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4272385537624359, Accuracy = 0.891414999961853\n",
      "Training iter #1470000:   Batch Loss = 0.278280, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.424727201461792, Accuracy = 0.8917543292045593\n",
      "Training iter #1500000:   Batch Loss = 0.288924, Accuracy = 0.9446666836738586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4235099256038666, Accuracy = 0.8931116461753845\n",
      "Training iter #1530000:   Batch Loss = 0.231608, Accuracy = 0.9593333601951599\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42176443338394165, Accuracy = 0.8934509754180908\n",
      "Training iter #1560000:   Batch Loss = 0.248149, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4213612377643585, Accuracy = 0.8924329876899719\n",
      "Training iter #1590000:   Batch Loss = 0.271436, Accuracy = 0.9386666417121887\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41901838779449463, Accuracy = 0.8941296339035034\n",
      "Training iter #1620000:   Batch Loss = 0.243724, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41738784313201904, Accuracy = 0.894808292388916\n",
      "Training iter #1650000:   Batch Loss = 0.299520, Accuracy = 0.9026666879653931\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4170142114162445, Accuracy = 0.8937903046607971\n",
      "Training iter #1680000:   Batch Loss = 0.325130, Accuracy = 0.8973333239555359\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4163416028022766, Accuracy = 0.8924329876899719\n",
      "Training iter #1710000:   Batch Loss = 0.332269, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4137687087059021, Accuracy = 0.8958262801170349\n",
      "Training iter #1740000:   Batch Loss = 0.234557, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41161149740219116, Accuracy = 0.8978622555732727\n",
      "Training iter #1770000:   Batch Loss = 0.235387, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41028159856796265, Accuracy = 0.8971835970878601\n",
      "Training iter #1800000:   Batch Loss = 0.212666, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41016244888305664, Accuracy = 0.8971835970878601\n",
      "Training iter #1830000:   Batch Loss = 0.276187, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40926676988601685, Accuracy = 0.8931116461753845\n",
      "Training iter #1860000:   Batch Loss = 0.274848, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4079364836215973, Accuracy = 0.8961656093597412\n",
      "Training iter #1890000:   Batch Loss = 0.232526, Accuracy = 0.9606666564941406\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40532413125038147, Accuracy = 0.8961656093597412\n",
      "Training iter #1920000:   Batch Loss = 0.230592, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4054628312587738, Accuracy = 0.8975229263305664\n",
      "Training iter #1950000:   Batch Loss = 0.243711, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4057922065258026, Accuracy = 0.8985409140586853\n",
      "Training iter #1980000:   Batch Loss = 0.242857, Accuracy = 0.9446666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40672823786735535, Accuracy = 0.8975229263305664\n",
      "Training iter #2010000:   Batch Loss = 0.215827, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40625566244125366, Accuracy = 0.8971835970878601\n",
      "Training iter #2040000:   Batch Loss = 0.309072, Accuracy = 0.9013333320617676\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40292319655418396, Accuracy = 0.8978622555732727\n",
      "Training iter #2070000:   Batch Loss = 0.301755, Accuracy = 0.8946666717529297\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4016713500022888, Accuracy = 0.8971835970878601\n",
      "Training iter #2100000:   Batch Loss = 0.218383, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40223413705825806, Accuracy = 0.8954869508743286\n",
      "Training iter #2130000:   Batch Loss = 0.221240, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4012601375579834, Accuracy = 0.8968442678451538\n",
      "Training iter #2160000:   Batch Loss = 0.196697, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4002722501754761, Accuracy = 0.8985409140586853\n",
      "Training iter #2190000:   Batch Loss = 0.216584, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.39804449677467346, Accuracy = 0.8968442678451538\n",
      "Optimization Finished!\n",
      "Training time is: 744.1731033325195 seconds\n",
      "FINAL RESULT: Batch Loss = 0.3991881012916565, Accuracy = 0.8975229263305664\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[77.07521629333496, 149.82939767837524, 224.99076628684998, 297.03854608535767, 392.90132427215576, 461.9202370643616, 516.0622463226318, 589.104679107666, 670.169819355011, 744.1731033325195]\n",
      "[0.6898541, 0.806583, 0.8574822, 0.87173396, 0.87173396, 0.8574822, 0.8788599, 0.8673227, 0.88496774, 0.8975229]\n",
      "[1.5015967, 0.9991535, 0.7142072, 0.7825644, 0.43600342, 0.4900325, 0.4425639, 0.47948235, 0.4194724, 0.3991881]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 of End-to-end training with the baseline model model 2.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset under same number of training iterations.\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def conv_net(_X):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "    fc1 = tf.layers.dense(fc1, 1024)\n",
    "    out = tf.layers.dense(fc1, n_classes)\n",
    "    return out\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0001\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 30000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    pred = conv_net(x)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.003272, Accuracy = 0.14399999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.984107494354248, Accuracy = 0.1462504267692566\n",
      "Training iter #300000:   Batch Loss = 1.002973, Accuracy = 0.8579999804496765\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0494939088821411, Accuracy = 0.8008143901824951\n",
      "Training iter #600000:   Batch Loss = 0.544462, Accuracy = 0.8486666679382324\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6430554389953613, Accuracy = 0.8639293909072876\n",
      "Training iter #900000:   Batch Loss = 0.343167, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5333853960037231, Accuracy = 0.8829317688941956\n",
      "Training iter #1200000:   Batch Loss = 0.299628, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4841468334197998, Accuracy = 0.885646402835846\n",
      "Training iter #1500000:   Batch Loss = 0.295844, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45608067512512207, Accuracy = 0.8863250613212585\n",
      "Training iter #1800000:   Batch Loss = 0.215669, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43841344118118286, Accuracy = 0.8907363414764404\n",
      "Training iter #2100000:   Batch Loss = 0.209294, Accuracy = 0.9586666822433472\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42970025539398193, Accuracy = 0.8910756707191467\n",
      "Training iter #2400000:   Batch Loss = 0.278057, Accuracy = 0.9146666526794434\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4267202913761139, Accuracy = 0.891414999961853\n",
      "Training iter #2700000:   Batch Loss = 0.226071, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4253566861152649, Accuracy = 0.8910756707191467\n",
      "Optimization Finished!\n",
      "Training time is: 1499.136292219162 seconds\n",
      "FINAL RESULT: Batch Loss = 0.41599199175834656, Accuracy = 0.8951476216316223\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment 2 Part 1 of End-to-end training with the baseline model model 2.\n",
    "# Finding the optimal number of iterations for Model 2\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def conv_net(_X):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "    fc1 = tf.layers.dense(fc1, 1024)\n",
    "    out = tf.layers.dense(fc1, n_classes)\n",
    "    return out\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0001\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 400  # Loop 300 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    pred = conv_net(x)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(np.arange(0,len(train_losses),1), train_losses, 'r', np.arange(0,len(test_losses),1), test_losses, 'b')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(np.arange(0,len(train_accuracies),1), train_accuracies, 'r', np.arange(0,len(test_accuracies),1), test_accuracies, 'b')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save above results for manuscript plotting later.\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[train_accuracies[i], test_accuracies[i],train_losses[i],test_losses[i]] for i in range(0,len(train_accuracies))])\n",
    "sheet.save_as(\"CNN_Training_ACC_LOSS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.990879, Accuracy = 0.19066666066646576\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.974717855453491, Accuracy = 0.16830675303936005\n",
      "Optimization Finished!\n",
      "Training time is: 62.569679737091064 seconds\n",
      "FINAL RESULT: Batch Loss = 1.555046558380127, Accuracy = 0.6365795731544495\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.006721, Accuracy = 0.14133332669734955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9726974964141846, Accuracy = 0.16864608228206635\n",
      "Training iter #300000:   Batch Loss = 0.818948, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2973253726959229, Accuracy = 0.789616584777832\n",
      "Optimization Finished!\n",
      "Training time is: 127.06933259963989 seconds\n",
      "FINAL RESULT: Batch Loss = 1.1909375190734863, Accuracy = 0.7960637807846069\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.008080, Accuracy = 0.2613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.965513229370117, Accuracy = 0.31455716490745544\n",
      "Training iter #300000:   Batch Loss = 0.878370, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1847506761550903, Accuracy = 0.8316932320594788\n",
      "Optimization Finished!\n",
      "Training time is: 182.46383213996887 seconds\n",
      "FINAL RESULT: Batch Loss = 0.90030837059021, Accuracy = 0.8445877432823181\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.009841, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.981715202331543, Accuracy = 0.16762809455394745\n",
      "Training iter #300000:   Batch Loss = 0.898325, Accuracy = 0.937333345413208\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9827059507369995, Accuracy = 0.8401764631271362\n",
      "Training iter #600000:   Batch Loss = 0.421602, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7122462391853333, Accuracy = 0.8659653663635254\n",
      "Optimization Finished!\n",
      "Training time is: 233.86011505126953 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6859724521636963, Accuracy = 0.8676620125770569\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.010785, Accuracy = 0.12733332812786102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9785027503967285, Accuracy = 0.17712928354740143\n",
      "Training iter #300000:   Batch Loss = 0.919702, Accuracy = 0.9126666784286499\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2105293273925781, Accuracy = 0.7818120121955872\n",
      "Training iter #600000:   Batch Loss = 0.611150, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8147709965705872, Accuracy = 0.8167628049850464\n",
      "Training iter #900000:   Batch Loss = 0.359119, Accuracy = 0.9486666917800903\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6663527488708496, Accuracy = 0.8293179273605347\n",
      "Optimization Finished!\n",
      "Training time is: 294.6156749725342 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6606416702270508, Accuracy = 0.8293179273605347\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.007323, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9818177223205566, Accuracy = 0.30607396364212036\n",
      "Training iter #300000:   Batch Loss = 1.088885, Accuracy = 0.8066666722297668\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1516454219818115, Accuracy = 0.777061402797699\n",
      "Training iter #600000:   Batch Loss = 0.613304, Accuracy = 0.8826666474342346\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.710476279258728, Accuracy = 0.8598574995994568\n",
      "Training iter #900000:   Batch Loss = 0.429309, Accuracy = 0.9046666622161865\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5593151450157166, Accuracy = 0.8720732927322388\n",
      "Optimization Finished!\n",
      "Training time is: 358.1142716407776 seconds\n",
      "FINAL RESULT: Batch Loss = 0.5074204206466675, Accuracy = 0.8741092681884766\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.004683, Accuracy = 0.15666666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.981189727783203, Accuracy = 0.3369528353214264\n",
      "Training iter #300000:   Batch Loss = 0.921503, Accuracy = 0.8766666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1182284355163574, Accuracy = 0.7835086584091187\n",
      "Training iter #600000:   Batch Loss = 0.526866, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7228975296020508, Accuracy = 0.8347471952438354\n",
      "Training iter #900000:   Batch Loss = 0.378147, Accuracy = 0.9073333144187927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.572771430015564, Accuracy = 0.8517135977745056\n",
      "Training iter #1200000:   Batch Loss = 0.284468, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.509132981300354, Accuracy = 0.8659653663635254\n",
      "Optimization Finished!\n",
      "Training time is: 418.9163839817047 seconds\n",
      "FINAL RESULT: Batch Loss = 0.4980301856994629, Accuracy = 0.8693586587905884\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.010568, Accuracy = 0.12133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9797325134277344, Accuracy = 0.35086527466773987\n",
      "Training iter #300000:   Batch Loss = 0.894577, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1260578632354736, Accuracy = 0.8232100605964661\n",
      "Training iter #600000:   Batch Loss = 0.521363, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7511534094810486, Accuracy = 0.8629114627838135\n",
      "Training iter #900000:   Batch Loss = 0.363726, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6074494123458862, Accuracy = 0.8713946342468262\n",
      "Training iter #1200000:   Batch Loss = 0.282653, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.541078507900238, Accuracy = 0.8788598775863647\n",
      "Optimization Finished!\n",
      "Training time is: 479.86902952194214 seconds\n",
      "FINAL RESULT: Batch Loss = 0.5091965198516846, Accuracy = 0.8829317688941956\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.004734, Accuracy = 0.1860000044107437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.980036735534668, Accuracy = 0.17441466450691223\n",
      "Training iter #300000:   Batch Loss = 1.009961, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.061513900756836, Accuracy = 0.7865626215934753\n",
      "Training iter #600000:   Batch Loss = 0.563941, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6703886985778809, Accuracy = 0.8452664017677307\n",
      "Training iter #900000:   Batch Loss = 0.426214, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5618013143539429, Accuracy = 0.8656260371208191\n",
      "Training iter #1200000:   Batch Loss = 0.321898, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5085399746894836, Accuracy = 0.8751272559165955\n",
      "Training iter #1500000:   Batch Loss = 0.334305, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47147613763809204, Accuracy = 0.876823902130127\n",
      "Optimization Finished!\n",
      "Training time is: 538.2433726787567 seconds\n",
      "FINAL RESULT: Batch Loss = 0.45721226930618286, Accuracy = 0.8781812191009521\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.011638, Accuracy = 0.3086666762828827\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9809956550598145, Accuracy = 0.16694943606853485\n",
      "Training iter #300000:   Batch Loss = 1.040145, Accuracy = 0.871999979019165\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.119292974472046, Accuracy = 0.8059043288230896\n",
      "Training iter #600000:   Batch Loss = 0.573210, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.669464647769928, Accuracy = 0.8778418898582458\n",
      "Training iter #900000:   Batch Loss = 0.397585, Accuracy = 0.9453333616256714\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5214450359344482, Accuracy = 0.8900576829910278\n",
      "Training iter #1200000:   Batch Loss = 0.310848, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4548030197620392, Accuracy = 0.8924329876899719\n",
      "Training iter #1500000:   Batch Loss = 0.313312, Accuracy = 0.9419999718666077\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42112529277801514, Accuracy = 0.8954869508743286\n",
      "Training iter #1800000:   Batch Loss = 0.223302, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40246421098709106, Accuracy = 0.8971835970878601\n",
      "Optimization Finished!\n",
      "Training time is: 596.057407617569 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: Batch Loss = 0.4003257751464844, Accuracy = 0.8978622555732727\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[62.569679737091064, 127.06933259963989, 182.46383213996887, 233.86011505126953, 294.6156749725342, 358.1142716407776, 418.9163839817047, 479.87002754211426, 538.2433726787567, 596.057407617569]\n",
      "[0.6365796, 0.7960638, 0.84458774, 0.867662, 0.8293179, 0.87410927, 0.86935866, 0.88293177, 0.8781812, 0.89786226]\n",
      "[1.5550466, 1.1909375, 0.9003084, 0.68597245, 0.6606417, 0.5074204, 0.4980302, 0.5091965, 0.45721227, 0.40032578]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 Part 2 of End-to-end training with the baseline model model 2.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset \n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def conv_net(_X):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "    fc1 = tf.layers.dense(fc1, 1024)\n",
    "    out = tf.layers.dense(fc1, n_classes)\n",
    "    return out\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0001\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 250  # Loop 250 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    pred = conv_net(x)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save above results for manuscript plotting later.\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[runing_time_list[i], accuracy_list[i],loss_list[i]] for i in range(0,len(runing_time_list))])\n",
    "sheet.save_as(\"CNN_Mod_Time_ACC_LOSS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.008119, Accuracy = 0.10533333569765091\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.987088680267334, Accuracy = 0.15371564030647278\n",
      "Training iter #300000:   Batch Loss = 1.073339, Accuracy = 0.8460000157356262\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1988846063613892, Accuracy = 0.7529691457748413\n",
      "Training iter #600000:   Batch Loss = 0.645700, Accuracy = 0.8633333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7853507399559021, Accuracy = 0.8347471952438354\n",
      "Training iter #900000:   Batch Loss = 0.406397, Accuracy = 0.9526666402816772\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6307153105735779, Accuracy = 0.8561248779296875\n",
      "Training iter #1200000:   Batch Loss = 0.341885, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5546013712882996, Accuracy = 0.8646080493927002\n",
      "Training iter #1500000:   Batch Loss = 0.326803, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5095065832138062, Accuracy = 0.8690193295478821\n",
      "Training iter #1800000:   Batch Loss = 0.247026, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4820232391357422, Accuracy = 0.8707159757614136\n",
      "Optimization Finished!\n",
      "Training time is: 1095.2533919811249 seconds\n",
      "FINAL RESULT: Batch Loss = 0.47828513383865356, Accuracy = 0.8696979880332947\n",
      "Testing Accuracy: 86.96979880332947%\n",
      "\n",
      "Precision: 87.17312115269013%\n",
      "Recall: 86.96979979640312%\n",
      "f1_score: 87.01732173718422%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[441  27  28   0   0   0]\n",
      " [ 39 417  15   0   0   0]\n",
      " [ 14  31 372   3   0   0]\n",
      " [  0   5   0 379 107   0]\n",
      " [  0   7   0  81 444   0]\n",
      " [  0  27   0   0   0 510]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[14.964371    0.916186    0.9501188   0.          0.          0.        ]\n",
      " [ 1.3233796  14.149983    0.5089922   0.          0.          0.        ]\n",
      " [ 0.4750594   1.0519172  12.623006    0.10179844  0.          0.        ]\n",
      " [ 0.          0.16966406  0.         12.860537    3.630811    0.        ]\n",
      " [ 0.          0.2375297   0.          2.748558   15.066169    0.        ]\n",
      " [ 0.          0.916186    0.          0.          0.         17.305735  ]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix to take a look\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN model is written by myself\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def conv_net(_X):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "    fc1 = tf.layers.dense(fc1, 1024)\n",
    "    out = tf.layers.dense(fc1, n_classes)\n",
    "    return out\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0001\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 250  # Loop 300 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    pred = conv_net(x)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    predictions = one_hot_predictions.argmax(1)\n",
    "    \n",
    "    print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    print(confusion_matrix)\n",
    "    normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "    print(normalised_confusion_matrix)\n",
    "    print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "    print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "    \n",
    "    # Plot Results: \n",
    "    width = 12\n",
    "    height = 12\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(\n",
    "        normalised_confusion_matrix, \n",
    "        interpolation='nearest', \n",
    "        cmap=plt.cm.rainbow\n",
    "    )\n",
    "    plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(n_classes)\n",
    "    plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "    plt.yticks(tick_marks, LABELS)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAM+CAYAAAAjKy8vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XmYZVV59/3vrxsnZBAZFFBBDSIigthOJFGMmmCCU6IvIhpwCImJmoDGCS5FIxIHHMH4oCIOiFM0r6IGSfLggIA0yuiMiAwi3bQgqCB03c8fe1c8FDWc6u6qU4vz/VzXufqcvdde6967Cj13rXuvnapCkiRJklq1bNQBSJIkSdL6MKmRJEmS1DSTGkmSJElNM6mRJEmS1DSTGkmSJElNM6mRJEmS1DSTGkmSJElNM6mRJEmS1DSTGkmSJElNM6mRJEmS1LSNRh2AJEmSpHW3T1KrRx3EDM6BU6pqn4Uex6RGkiRJathqYOWog5hBYKvFGMfyM0mSJElNc6ZGkiRJat3yJTpXsXZiUYZZomcvSZIkScMxqZEkSZLUNMvPJEmSpJYFWJ5RRzG9tYszjDM1kiRJkppmUiNJkiSpaZafSZIkSU3L0l39bJHqz5bq2UuSJEnSUExqJEmSJDXN8jNJkiSpZQE2WqKrny0SZ2okSZIkNc2kRpIkSVLTLD+TJEmSWhaW8Opni2O8z16SJElS80xqJEmSJDXN8jNJkiSpdctd/UySJEmSmmVSI0mSJKlplp9JkiRJLUtc/WzUAUiSJEnS+jCpkSRJktQ0y88kSZKklvnwTWdqJEmSJLXNpEaSJElS0yw/kyRJklrnwzclSZIkqV0mNZIkSZKaZvmZJEmS1DIfvulMjSRJkqS2mdRIkiRJaprlZ5IkSVLrXP1MkiRJktplUiNJkiSpaZafSZIkSS0LsNF4z1WM99lLkiRJap5JjSRJkqSmWX4mSZIktSxx9bNRByBJkiRJ68OkRpIkSVLTLD+TJEmSWrd8vOcqxvvsJUmSJDXPpEaSJElS0yw/kyRJkloWLD8bdQCSJEmStD5MaiRJkiQ1zfIzSZIkqWU+fNOZGkmSJEltM6mRJEmS1DTLzyRJkqTWufqZJEmSJLXLpEaSJElS00xqJEmSpJaFbvWzpfiaK/Tk+CRXJ7lwyvaXJPlBkouSvGWufkxqJEmSJI3KCcA+gxuSPA54KvCQqtoVeNtcnZjUSJIkSRqJqvoasGbK5hcB/1pVN/Vtrp6rH1c/kyRJkpqW29vqZw8A/jjJkcCNwMur6uzZDjCpkSRJkrRQtkqycuDzcVV13BzHbARsATwKeDjwqST3q6qa7QBJkiRJWgirq2rFPI+5HPhsn8R8K8kEsBWwaqYDTGokSZKklk2ufnb78R/AnwCnJXkAcEdg9WwHmNRIkiRJGokkJwF705WpXQ68DjgeOL5f5vl3wIGzlZ6BSY0kSZKkEamq/WfY9Zz59GNSI0mSJLUs3N5WP5u38T57SZIkSc0zqZEkSZLUNMvPJEmSpNbdvlY/mzdnaiRJkiQ1zaRGkrTBJLlLki8kuS7Jp9ejnwOSfGVDxjYqSf44yQ9GHYck3Z5ZfiZJYyjJs4FDgQcC1wPnAkdW1TfWs+tnAPcAtqyqW9a1k6o6EThxPWNZcEkK2KmqfjxTm6r6OrDz4kUlaewkrn426gAkSYsryaHAO4E30SUg9wHeCzx1A3S/A/DD9Ulobk+S+MdDSVoEJjWSNEaSbA68AfiHqvpsVf26qm6uqi9U1T/3be6U5J1Jruxf70xyp37f3kkuT/KyJFcn+XmS5/X7Xg+8FtgvyQ1JXpDkiCQfGxh/xyQ1+WU/yUFJfpLk+iSXJDlgYPs3Bo7bK8nZfVnb2Un2Gth3WpJ/SXJ6389Xkmw1w/lPxv+KgfifluTPk/wwyZokrxlo/4gkZyS5tm97TJI79vu+1jc7rz/f/Qb6f2WSq4APTW7rj7l/P8ae/eftkqxOsvd6/WAlacyZ1EjSeHk0cGfgc7O0OQx4FLAHsDvwCODwgf33BDYHtgdeABybZIuqeh3d7M8nq2qTqvrgbIEkuSvwbuBJVbUpsBddGdzUdncHvti33RJ4O/DFJFsONHs28DxgG+COwMtnGfqedNdge7ok7P10T65+GPDHwGuT3K9vuxY4BNiK7to9Hvh7gKp6TN9m9/58PznQ/93pZq0OHhy4qi4GXgmcmGRj4EPACVV12izxStLcli9bmq9FYlIjSeNlS2D1HOVhBwBvqKqrq2oV8HrguQP7b+7331xVXwJuYN3vGZkAHpzkLlX186q6aJo2fwH8qKo+WlW3VNVJwPeBJw+0+VBV/bCqfgt8ii4hm8nNdPcP3Qx8gi5heVdVXd+PfxHwEICqOqeqzuzH/Snwf4DHDnFOr6uqm/p4bqWq3g/8CDgL2JYuiZQkrQeTGkkaL9cAW81xr8d2wKUDny/tt/1vH1OSot8Am8w3kKr6NbAf8HfAz5N8MckDh4hnMqbtBz5fNY94rqmqtf37yaTjFwP7fzt5fJIHJDk5yVVJfkU3EzVtaduAVVV14xxt3g88GHhPVd00R1tJ0hxMaiRpvJwB3Ag8bZY2V9KVTk26T79tXfwa2Hjg8z0Hd1bVKVX1RLoZi+/TfdmfK57JmK5Yx5jm49/o4tqpqjYDXgPM9YS7mm1nkk3oFmr4IHBEX14nSesudA/fXIqvRWJSI0ljpKquo7uP5Nj+BvmNk9whyZOSvKVvdhJweJKt+xvuXwt8bKY+53Au8Jgk9+kXKXj15I4k90jylP7empvoytjWTtPHl4AHJHl2ko2S7Ac8CDh5HWOaj02BXwE39LNIL5qy/xfA/W5z1OzeBZxTVS+ku1fofesdpSSNOZMaSRozVfV2umfUHA6sAi4DXgz8R9/kjcBK4HzgAuDb/bZ1GetU4JN9X+dw60RkGfAyupmYNXT3qvz9NH1cA+zbt70GeAWwb1WtXpeY5unldIsQXE83i/TJKfuPAD7cr472/83VWZKnAvvQldxB93PYc3LVN0nSuknVrLPkkiRJkpawFVvcpVbufd9RhzGt/Mf3zqmqFQs9jjM1kiRJkppmUiNJkiSpabMt6SlJkiSpBYu40thS5EyNJEmSpKaZ1EjSHJIcleSfRh3HXJLsmKQmH6yZ5MtJDtzAYxyU5Bsbss/F0C8f/bUk1yc5egTjL9nrluS0JC9coL7fnuTv5m4pSevHpEaSZpFka+Cvgf8z6ljmq6qeVFUfXqzxpiZV63D8vZOcmWTN1MQjyX8mWZ/Vcw4GVgObVdXLphn7hCRDL1s93/Zz9LVe122h4pqh/58mecI8DnkrcFiSOy5UTJLoH765bGm+FolJjSTN7iDgS1X12w3d8Yb4Ens782rgw8B9gadNJjH9wzZ/UlUr16PvHYDvls8xWFRV9XPg+8BTRh2LpNs3kxpJmt2TgK9Ofkiyd5LLk7wsydVJfp7keQP7N0/ykSSrklya5PAky/p9ByU5Pck7kqwBjpiy7dokP0myV7/9sn6MAwf6/4sk30nyq37/ETMFPlhWlOQPknw1yXVJVif55EC7ByY5tZ8h+cHgQySTbJnk8/143wLuP8u1+lr/77VJbkjy6CTL+mtwaX8uH0my+QzH3xf4n6q6DjgbuF+SzYBXAa+ZZdzJWPdKcnZ/jmcn2avffgJwIPCKPq4nTDnuYOCAgf1f6Lfv0l/Da5NclOQpc7R/VZKL+xK37yZ5+lwxz3Td+v6en+R7SX6Z5JQkO/Tb0/++XN2f6/lJHjxTXNNcpycm+X5/7DF0f+Od3Hf/JP+T5Jr+9+TEJHfr930UuA/whb7/V/TbP53kqr6/ryXZdcqQpwF/MeS1kKR1YlIjSbPbDfjBlG33BDYHtgdeABybZIt+33v6ffcDHktXuva8gWMfCfwE2AY4cmDb+cCWwMeBTwAPB/4AeA5wTJJN+ra/7vu8G90XxRcledoQ5/EvwFeALYB79XGS5K7Aqf242wD7A+8d+GJ6LHAjsC3w/P41k8f0/96tqjapqjPoZroOAh7XX5NNgGNmOP5C4In9l+gVwHf7uN9ZVdfOdnJJ7g58EXg33XV8O/DFJFtW1UHAicBb+rj+a/DYqjpuyv4nJ7kD8AW6a7YN8BLgxCQ7T9e+7+pi4I/pfv6vBz6WZNvZ4u7d5rr1P9PXAH8JbA18HTipb/en/TEPoPs92A+4Zpa4Bq/TVsC/A4cDW/Ux/+FgE+AoYDtgF+DewBH9dXou8DPgyX3/b+mP+TKwU3+dvt3HMOh7wO5DXAdJ6yrpVj9biq9FYlIjSbO7G3D9lG03A2+oqpur6kvADcDOSZbTfcF8dVVdX1U/BY4Gnjtw7JVV9Z6qumWgpO2SqvpQVa0FPkn3RfINVXVTVX0F+B1dgkNVnVZVF1TVRFWdT/dF97FDnMfNdCVY21XVjVU1edP6vsBP+/Fvqapv033pfUZ/Pn8FvLaqfl1VF9KVh83HAcDbq+onVXUDXYnZszJ96d1RdEnBV+mSqTsAD6GbGfh4Pwvw4hnG+QvgR1X10f48TqIre7rNF/shPYouAfvXqvpdVf0PcDJd0jetqvp0VV3Z/2w+CfwIeMQ6jv+3wFFV9b2qugV4E7BHP1tzM7Ap8EAgfZufD9nvn9OV4X2mqm4G3glcNXAOP66qU/vfvVV0yeGsv19VdXz/+34TXQK0+5TZuOvp/juSpAVjUiNJs/sl3RfIQdf0XzQn/YbuC/BWwB2BSwf2XUo3ozPpsmnG+MXA+98CVNXUbZsAJHlkkv+brrztOuDv+nHn8gq6v8J/qy+lmpxx2QF4ZF9idW2Sa+kSkXvSzRBsNCXmwXMbxnbc9npsBNxjasOqWlNV+1XV7sC76GaTXkJXfnYh8ATg75I8aIhxJsfafpq2w8Z9WVVNDNtfkr9Ocu7AdXwww/1sprMD8K6BvtbQ/fy27xOsY+gSv18kOa4v0xvGdgz8PPt7jP73c5JtknwiyRVJfgV8bLZzSLI8yb/2ZXe/An7a7xo8ZlNg1pk2SVpfJjWSNLvz6cp8hrGa38+ITLoPcMXA5/W9Uf3jwOeBe1fV5sD7GLgnYiZVdVVV/U1VbUc3C/DeJH9A94X2q1V1t4HXJlX1ImAVcAvdzNHg+cw4zDTbruS21+MWbp3ITedg4Mx+dmg3YGVV/Q64gC5ZmGucybGumKbtdKbGfiVw7/T3Q03T363a9zMo7wdeDGxZVXejS8SGqb2Y7rpdBvztlJ/LXarqmwBV9e6qehiwK93v5z/P0tegnzPw80wSbv3zParv4yFVtRld+ePgOUzt/9nAU+kSzs2BHSe7HmizC3DeHHFJWl+jXuXM1c8kaUn7EsOVd9GXj30KODLJpv0X3UPp/tq9oWwKrKmqG5M8gu5L5ZySPDPJvfqPv6T7crqWrqTqAUmem+QO/evhSXbpz+ezdAsabNzPkMz23JtVwATdvTOTTgIOSXLf/r6gNwGfnDLTNTXWbYB/oL+XA7gEeFx//Aq6e5Km+lJ/Hs9OslG6FdMe1J/fMH4xJe6z6O5fekV/TfamK2X7xAzt70p3TVf15/A8pk++pjPddXsf8OrJe5vSLUDxzP79w/sZuzv0Md5I97OcLq6pvgjsmuQv+xLAl9LNyk3alK6c8tok2/P7ZGnS1P43BW4CrgE2pvv5TvVYuvtuJGnBmNRI0uw+Avx5krsM2f4ldF80fwJ8g25m5fgNGM/fA29Icj3wWrokahgPB85KcgPdTM8/VtUlVXU93Y3nz6KbnbgKeDNwp/64F9OVvl0FnAB8aKYBquo3dIsfnN6XTT2K7tw/SrfC1yV0X8BfMkesb6O7p+iG/vNRwJ/QzV58frqlnavqGrr7g15G9wX7FcC+VbV6jrEmfRB4UB/3f/SzQk+hW/1uNfBe4K+r6vsztP8u3f1TZ9B98d8NOH2Ygae7blX1Obqfwyf6sq4L+1gANqObFfolXUncNXTX7DZxTTPWauCZwL/2x+00Jc7XA3sC19ElQJ+d0sVRwOF9/y+n++/jUroZrO8CZw427hdKeBBwm1gkaUOKS/ZL0uySvAm4uqreOepYpJake4jqxVX13lHHIt2erdhq41q5786jDmNa+fC551TV+jw8eSg++E2S5lBVcz4jRdJtVdXLRh2DpPFg+ZkkSZKkpjlTI0mSJDUti7rS2FI03mcvSZIkqXkmNZIkSZKaZvmZNqit7rxR7bjpneZuqA3u0ht2GXUI0kjc4cZRRyBp3FzLT/lNrR7m4bqLI8DypRPOKJjUaIPacdM7sfKvdh11GGPp4K+eNeoQxtrE8lFHML7uddF4/x+5pMV3HAu+QrHmyfIzSZIkSU1zpkaSJElqWXD1s1EHIEmSJEnrw6RGkiRJUtMsP5MkSZKa5sM3x/vsJUmSJDXPpEaSJElS0yw/kyRJkloWYNl4P7PLmRpJkiRJTTOpkSRJktQ0y88kSZKk1rn6mSRJkiS1y6RGkiRJUtMsP5MkSZJaFmC5q59JkiRJUrNMaiRJkiQ1zfIzSZIkqWlx9bNRByBJkiRJ68OkRpIkSVLTLD+TJEmSWubqZ87USJIkSWqbSY0kSZKkpll+JkmSJLVu2XjPVYz32UuSJElqnkmNJEmSpKZZfiZJkiS1LHH1s1EHIEmSJEnrw6RGkiRJUtMsP5MkSZJaFmD5eM9VjPfZS5IkSWqeSY0kSZKkpll+JkmSJLXO1c8kSZIkqV0mNZIkSZKaZvmZJEmS1LIElo33XMV4n70kSZKk5pnUSJIkSWqa5WeSJElS61z9TJIkSZLaZVIjSZIkqWmWn0mSJEktC7B8vOcqxvvsRyzJO5L808DnU5J8YODz0UkO7d8fkuTGJJsP7N87ycnT9HtakhX9+x2T/CjJnw22T3JQkokkDxk47sIkO/bvN0nyb0kuTvKdJOck+ZsNfxUkSZKk9WNSM1rfBPYCSLIM2ArYdWD/XsDp/fv9gbOBpw/beZJ7AacAL6uqU6Zpcjlw2AyHfwD4JbBTVT0U2Ae4+7BjS5IkSXNJcnySq5NcOM2+lyepJFvN1Y9JzWidTp/U0CUzFwLXJ9kiyZ2AXYDvJLk/sAlwOF1yM4x7Al8BDq+qz8/Q5mRg1yQ7D27sx3tEf+wEQFWtqqo3D39qkiRJWjTLsjRfczuB7o/nt5Lk3sATgZ8NdfrzuVbasKrqSuCWJPehS27OAM4CHg2sAM6vqt/RJTInAV8Hdk6yzRDdfwQ4pqo+PUubCeAtwGumbN8VOG8yoZEkSZIWQlV9DVgzza53AK8Aaph+TGpGb3K2ZjKpOWPg8zf7Ns8CPtEnGZ8FnjlEv/8FPDfJxnO0+zjwqCT3nalBksOSnJvkyhn2H5xkZZKVq268ZYjQJEmSpOkleQpwRVWdN+wxrn42epP31exGV352GfAy4FfA8f2N/DsBpyYBuCPwE+DYOfp9C/Ac4NNJnlpV02YbVXVLkqOBVw5s/i6we5JlVTVRVUcCRya5YYY+jgOOA1ix9V2HyqYlSZK0gSRLefWzrZKsHPh8XP/dcVr9H+QPA/50PoMs2bMfI6cD+wJrqmptVa0B7kZXgnYGXenZEVW1Y//aDtg+yQ5D9H0IXXL0wfQZ0QxOAJ4AbA1QVT8GVgJvTLIcIMmd6RYMlCRJkoa1uqpWDLxmTGh69wfuC5yX5KfAvYBvJ7nnbAeZ1IzeBXSrnp05Zdt1VbWarvTsc1OO+Vy/HeDxSS4feD16slFVFXAgsC3dzM20+vt23g0M3qvzQmBL4MdJzqErZ3vlNIdLkiRJG0RVXVBV20z+QZ9utd49q+qq2Y6z/GzEqmotsNmUbQcNvL/NvS5VdejAx7tM0+3eA21/x62n707rt59AN0Mz2e7ddInN5OdfAX87xClIkiRp1IZbaWzJSXIS3XfXrZJcDryuqj44335MaiRJkiSNRFXN+riSfrZmTpafSZIkSWqaSY0kSZKkpll+JkmSJLUsLOUlnRfFeJ+9JEmSpOaZ1EiSJElqmuVnkiRJUtPS7JLOG4ozNZIkSZKaZlIjSZIkqWmWn0mSJEktc/UzZ2okSZIktc2kRpIkSVLTLD+TJEmSWufqZ5IkSZLULpMaSZIkSU2z/EySJElqWeLqZ6MOQJIkSZLWh0mNJEmSpKZZfiZJkiS1ztXPJEmSJKldJjWSJEmSmmb5mSRJktSy4Opnow5AkiRJktaHSY0kSZKkpll+JkmSJDUtrn426gAkSZIkaX2Y1EiSJElqmuVnkiRJUssCLBvvuYrxPntJkiRJzTOpkSRJktQ0y88kSZKk1i139TNJkiRJapZJjSRJkqSmWX4mSZIktSxx9bNRByBJkiRJ68OkRpIkSVLTLD+TJEmSWrfM1c8kSZIkqVkmNZIkSZKaZvmZJEmS1LLgwzdHHYAkSZIkrQ+TGkmSJElNs/xMG9Qlv92FZ1945qjDGEsfP+hJow5hrB169JdHHYIkaZz58E1JkiRJapdJjSRJkqSmWX4mSZIktSxhwodvSpIkSVK7TGokSZIkNc3yM0mSJKlhBUy4+pkkSZIktcukRpIkSVLTLD+TJEmSGufqZ5IkSZLUMJMaSZIkSU2z/EySJElqWCWsXT7ecxXjffaSJEmSmmdSI0mSJKlplp9JkiRJjXP1M0mSJElqmEmNJEmSpKZZfiZJkiS1LFDLxnuuYrzPXpIkSVLzTGokSZIkNc3yM0mSJKlhhaufOVMjSZIkqWkmNZIkSZKaZvmZJEmS1LLE8rNRByBJkiRJ68OkRpIkSVLTLD+TJEmSGtatfjbecxXjffaSJEmSmmdSI0mSJKlplp9JkiRJjXP1M0mSJElqmEmNJEmSpKZZfiZJkiQ1rBLWZrznKsb77CVJkiQ1z6RGkiRJUtMsP5MkSZIa5+pnkiRJktQwkxpJkiRJTbP8TJIkSWpcq+VnSY4H9gWurqoH99veCjwZ+B1wMfC8qrp2tn6cqZEkSZI0KicA+0zZdirw4Kp6CPBD4NVzdWJSI0mSJGkkquprwJop275SVbf0H88E7jVXP5afSZIkSQ2rQC273c5VPB/45FyNTGokSZIkLZStkqwc+HxcVR03zIFJDgNuAU6cq61JjSRJkqSFsrqqVsz3oCQH0i0g8Piqqrnam9RIkiRJTUuzq59NJ8k+wCuBx1bVb4Y5poniuyTvSPJPA59PSfKBgc9HJzm0f39IkhuTbD6wf+8kJ0/T72lJVvTvd0zyoyR/Ntg+yUFJJpI8ZOC4C5Ps2L/fJMm/Jbk4yXeSnJPkb2Y5l9vEkuSEJM8YiOkHSc5LcnqSnfvt+/b9n5fku0n+NslhSc7tX2sH3r90oO/zkpw05HhnJ9ljoN3zk1yQ5Pz+nJ8603lJkiRJ89V/Tz0D2DnJ5UleABwDbAqc2n+3fd9c/bQyU/NN4JnAO5MsA7YCNhvYvxcwmfTsD5wNPJ1uibg5JbkXcArwsqo6JcneU5pcDhwG7DfN4R8AfgLsVFUTSbamu6FpfRxQVSuTHAy8NclfAccBj6iqy5PcCdixqn4AHNmfww1VtcdgJ0l2oUtcH5PkrlX16znGex7wVuCJ/TU5DNizqq5Lsgmw9XqelyRJkvS/qmr/aTZ/cL79NDFTA5xOl7gA7ApcCFyfZIv+C/4uwHeS3B/YBDicLrkZxj2BrwCHV9XnZ2hzMrDr5KzJpH68R/THTgBU1aqqevPwpzarrwF/QJepbgRc049xU5/QzOXZwEfpzu8pQ7Q/A9i+f78NcD1wQz/mDVV1ybyilyRJ0sILTCxbtiRfi6WJpKaqrgRuSXIfuuTmDOAs4NHACuD8qvodXSJzEvB1uimsbYbo/iPAMVX16VnaTABvAV4zZfuuwHmTCc0CeDJwQVWtAT4PXJrkpCQH9DNWc9mPbgm8kxguydsH+I/+/XnAL4BLknwoyZNnOijJwUlWJll54y2rhhhGkiRJ2nCaSGp6k7M1k0nNGQOfv9m3eRbwiT7J+Cxdydpc/gt4bpKN52j3ceBRSe47U4OBe1yunKWfmVZvGNx+YpJzgT8EXg5QVS8EHg98q992/GzBJnk4sKqqLgX+G9gzyRYzND8xyeV0N2S9px9vLV2S8wy6J7m+I8kR0wZedVxVraiqFXfeyAo1SZIkLa6Wkppv0iUwu9GVn51JN1OzF3B6fyP/TnQ3FP2ULsEZZnbiLXSzPp9OMuM9Rv1TTY+m++I/6bvA7pOzJlV1ZH9fy2bTdDHpGmBqcnF3YPXA5wOqao+qelpVXTYQwwVV9Q7gicBfzXFe+wMP7K/FxX1MMx1zAHBfusTt2IHxqqq+VVVH0V3PucaUJEnSIitgIlmSr8XSUlJzOt1a1Wuqam1fknU3usTmDLov8UdU1Y79aztg+yQ7DNH3IcCvgA8ms179E4An0N8wX1U/BlYCb0yyHCDJnYHZ+vgRsF1/Ez99fLsD5850QL/C2t4Dm/YALp2l/TK6WaqHTF4P4KnMkuRV1c109yI9KskuSbZLsuewY0qSJEmj0lJScwHdqmdnTtl2XVWtpptJ+NyUYz7Xbwd4fL9M3OTr0ZON+gf6HAhsSzdzM63+vp13091EP+mFwJbAj5OcQ1fO9sppDp/s4ybgOcCH+hKzzwAvrKrrZjzzLkl6Rb/08rnA64GDZmn/GOCKqrpiYNvXgAcl2XaW2H5LNxv1cuAOwNuSfL8fcz/gH2cZU5IkSRqJDPGATmloW26yov5sj2+NOoyx9PEn//moQxhrhx795VGHMLY2W3X7eeCcpDYcxwqurJVL5n98dnvwtvW5f3/BqMOY1k4PPPKcqlqx0OO0NFMjSZIkSbfRysM3m5NkN7pnxAy6qaoeOYp4JEmSpNsrk5oFUlUX0N1cL0mSJC2YShb1QZdL0XifvSRJkqTmmdRIkiRJaprlZ5IkSVLj1i7igy6XImdqJEmSJDXNpEaSJElS0yw/kyRJkhpW4Opnow5AkiRJktaHSY0kSZKkpll+JkmSJDUtlKufSZIkSVK7TGokSZIkNc3yM0mSJKllgYlllp9JkiRJUrNMaiRJkiQ1zfIzSZIkqWEFTGS85yrG++wlSZIkNc+kRpIkSVLTLD+TJEmSGufqZ5IkSZLUMJMaSZIkSU1Bxq9ZAAAgAElEQVSz/EySJElqWcJELD+TJEmSpGaZ1EiSJElqmuVnkiRJUsMKWLtsvOcqxvvsJUmSJDXPpEaSJElS0yw/kyRJkhrn6meSJEmS1DCTGkmSJElNs/xMkiRJalhh+ZkzNZIkSZKaZlIjSZIkqWmWn0mSJEktSygfvilJkiRJ7TKpkSRJktQ0y88kSZKkxrn6mSRJkiQ1zKRGkiRJUtMsP9MGtdFNcM8fjvf056g8/+NfGnUIY+09n/qHUYcwtt78hGNHHcJYW7bW/82XRs2HbzpTI0mSJKlxJjWSJEmSmmb5mSRJktQ4y88kSZIkqWEmNZIkSZKaZvmZJEmS1LBKmMh4z1WM99lLkiRJap5JjSRJkqSmWX4mSZIkNc7VzyRJkiSpYSY1kiRJkppm+ZkkSZLUsALWLrP8TJIkSZKaZVIjSZIkqWmWn0mSJEkt8+GbztRIkiRJaptJjSRJkqSmWX4mSZIkNa58+KYkSZIktcukRpIkSVLTLD+TJEmSGlbABJafSZIkSVKzTGokSZIkNc3yM0mSJKlxE65+JkmSJEntMqmRJEmS1DTLzyRJkqSmhYmM91zFeJ+9JEmSpOaZ1EiSJElqmuVnkiRJUsMKVz9zpkaSJElS00xqJEmSJI1EkuOTXJ3kwoFtd09yapIf9f9uMVc/JjWSJElSywJrkyX5GsIJwD5Ttr0K+O+q2gn47/7zrExqJEmSJI1EVX0NWDNl81OBD/fvPww8ba5+TGokSZIkLSX3qKqfA/T/bjPXAa5+JkmSJDVsia9+tlWSlQOfj6uq4zb0ICY1kiRJkhbK6qpaMc9jfpFk26r6eZJtgavnOsDyM0mSJElLyeeBA/v3BwL//1wHOFMjSZIkNS1MNDpXkeQkYG+6MrXLgdcB/wp8KskLgJ8Bz5yrH5MaSZIkSSNRVfvPsOvx8+mnzZROkiRJknrO1EiSJEmNq6W7+tmicKZGkiRJUtNMaiRJkiQ1bcGSmiTvSPJPA59PSfKBgc9HJzm0f39IkhuTbD6wf+8kJ0/T72lJVvTvd0zyoyR/Ntg+yUFJJpI8ZOC4C5Ps2L/fJMm/Jbk4yXeSnJPkb2Y5lx2T/LZv+70k30py4JQ2T0tyfpLvJ7kgydP67bsnOXeg3f5JfpPkDv3n3ZKcP3BuKwfarkhyWv9+4yQn9n1fmOQbSXZIcm7/uirJFQOf79gf9/QkleSBU87nwoHrfF1/bt9P8raBdvdIcnKS85J8N8mXZrpGkiRJGo3Jh28uxddiWciZmm8CewEkWQZsBew6sH8v4PT+/f7A2cDTh+08yb2AU4CXVdUp0zS5HDhshsM/APwS2KmqHgrsA9x9jiEvrqqHVtUuwLOAQ5I8r49ld+BtwFOr6oHAU4C39UnVBcAOSTbt+9kL+D7w0IHPpw+Ms02SJ00z/j8Cv6iq3arqwcALgKuqao+q2gN4H/COyc9V9bv+uP2Bb/Qxz+Tr/XV4KLBvkj/st78BOLWqdq+qBwGvmuMaSZIkSYtuIZOa0+mTGrpk5kLg+iRbJLkTsAvwnST3BzYBDqf7Aj6MewJfAQ6vqs/P0OZkYNckOw9u7Md7RH/sBEBVraqqNw97YlX1E+BQ4KX9ppcDb6qqS/r9lwBHAf/cj3E28Mi+7cOAY/n9tdmLLgGc9Fa6azHVtsAVAzH8oKpumi3OJJsAf0iXAM2W1Ez2+VvgXGD7gTEvH9h//lx9SJIkSYttwZKaqroSuCXJfei+uJ8BnAU8GlgBnN/PJuwPnAR8Hdg5yTZDdP8R4Jiq+vQsbSaAtwCvmbJ9V+C8yYRmPXwbmCzp2hU4Z8r+lfx+ZuqbwF5J7trHdRq3TmoGZ2rOAG5K8rgp/R0PvDLJGUnemGSnIWJ8GvCfVfVDYE2SPWdrnGQLYCfga/2mY4EPJvm/SQ5Lst0Mxx2cZGWSlb+dWDVEWJIkSdqQJsiSfC2WhV4oYHK2ZjKpOWPg8+TsxLOAT/RJxmcZ4omhwH8Bz02y8RztPg48Ksl9Z2rQf1k/N8mVQ4x7q0OnvK9p9k9um7wOjwDOrqqLgT9IsjWwST/zM+iNTJmtqapzgfvRzeTcHTg7yS5zxLg/8In+/SeYeSbsj/v7eq4CTq6qq/oxT+nHfD9dAvedPuZbqarjqmpFVa24y7Lb7JYkSZIW1EInNZP31exGV352Jt1MzV7A6f09JzsBpyb5KV2CM0wJ2lvoZn0+nWTGZ+1U1S3A0cArBzZ/F9i9v8+Hqjqyvydls/mdGg8Fvte/v4hu9mnQnv1Y0J33w4E/okvsoCvreha3Lj2bjPt/gDsDj5qy/Yaq+mxV/T3wMeDPZwouyZbAnwAf6K/tPwP7JdPesfX1qnoI3c/pRUn2GBhzTVV9vKqeS1dG95iZxpQkSZJGYTFmavYF1lTV2qpaA9yNLrE5gy6BOaKqduxf2wHbJ9lhiL4PAX5FVx4129zWCcATgK0BqurHdKVhb0yyHCDJnWH4+bF+FbW3Ae/pN70NePXA6mo70pW9Hd2PeT1wGXAQv09qzgD+iWmSmt6RwCsGxvzDvjyMfmWzBwGXzhLmM4CPVNUO/bW9N3AJXWI1rb5M7Sj6JDDJn0zOhvULHdwf+NksY0qSJGmRFWEiy5bka7Es9EgX0K16duaUbddV1Wq6mYrPTTnmc/z+pvbHJ7l84PXoyUZVVcCBdDezv2WmAPr7dt4NDN6r80JgS+DHSc6hK2d75TSHD7r/5JLOwKeA91TVh/oxzu2P/0KS7wNfAF7Rb590OnCnqrqs/3wGXWnXtElNVX0JGLxB5f7AV5NcAHyHLjH791ni3Z/bXtt/B549x3m+D3hMX7L3MGBlX5p2BvCBqjp7juMlSZKkRZUuN5A2jHvcYUUdsIV5zyhcu63/LY/Se9714lGHMLbe/IRjRx3CWFu2dvFuBJaWiuNYwZW1csn88u/4sPvVa8/6l1GHMa0X3OE551TV1Ns0NrgZ70eRJEmS1IbFXGlsKTKpGZBkN+CjUzbfVFWPnK69JEmSpNEzqRlQVRcAe8zZUJIkSdKSYVIjSZIkNawCE7MuBnz7t3jrrEmSJEnSAjCpkSRJktQ0y88kSZKkxq0d89XPnKmRJEmS1DSTGkmSJElNs/xMkiRJalgRVz8bdQCSJEmStD5MaiRJkiQ1zfIzSZIkqXHl6meSJEmS1C6TGkmSJElNs/xMkiRJatxExnuuYrzPXpIkSVLzTGokSZIkNW3G8rMkm812YFX9asOHI0mSJGk+CpgY89XPZrun5iK6azR4hSY/F3CfBYxLkiRJkoYyY1JTVfdezEAkSZIkaV0MtfpZkmcB96uqNyW5F3CPqjpnYUOTJEmSNLeMffnZnAsFJDkGeBzw3H7Tb4D3LWRQkiRJkjSsYWZq9qqqPZN8B6Cq1iS54wLHJUmSJElDGSapuTnJMrrFAUiyJTCxoFFJkiRJGprlZ3M7Fvh3YOskrwe+Abx5QaOSJEmSpCHNOVNTVR9Jcg7whH7TM6vqwoUNS5IkSZKGM9TqZ8By4Ga6ErRhZnckSZIkLYIC1sbys1klOQw4CdgOuBfw8SSvXujAJEmSJGkYw8zUPAd4WFX9BiDJkcA5wFELGZgkSZIkDWOYpObSKe02An6yMOFIkiRJmq9xX/1sxqQmyTvoSvR+A1yU5JT+85/SrYAmSZIkSSM320zN5ApnFwFfHNh+5sKFI0mSJEnzM2NSU1UfXMxAJEmSJM1fESbGfIHiOe+pSXJ/4EjgQcCdJ7dX1QMWMC5JkiRJGsowKd0JwIeAAE8CPgV8YgFjkiRJkqShDZPUbFxVpwBU1cVVdTjwuIUNS5IkSdKwiizJ12IZZknnm5IEuDjJ3wFXANssbFiSJEmSNJxhkppDgE2Al9LdW7M58PyFDEqSJEmShjVnUlNVZ/Vvrweeu7DhSJIkSZovH745gySfo3vY5rSq6i8XJCJJkiRJmofZZmqOWbQodLux/BbYbNV4/6VgVDa+btQRjLe3Pu69ow5hbL3uyy8cdQhjbdvL3j7qEMbW3x+82ahDkJaM2R6++d+LGYgkSZKk+SssPxvvR49KkiRJap5JjSRJkqSmDbOkMwBJ7lRVNy1kMJIkSZLmz/KzOSR5RJILgB/1n3dP8p4Fj0ySJEmShjBM+dm7gX2BawCq6jzgcQsZlCRJkiQNa5jys2VVdWlyqymttQsUjyRJkqR5KMLaMS8/GyapuSzJI4BKshx4CfDDhQ1LkiRJkoYzTPnZi4BDgfsAvwAe1W+TJEmSpJGbc6amqq4GnrUIsUiSJElaB2X52eySvJ/uQaW3UlUHL0hEkiRJkjQPw9xT818D7+8MPB24bGHCkSRJkqT5Gab87JODn5N8FDh1wSKSJEmSNC8+fHP+7gvssKEDkSRJkqR1Mcw9Nb/k9/fULAPWAK9ayKAkSZIkaVizJjXpnri5O3BFv2miqm6zaIAkSZKk0ShgbVl+NqM+gflcVa3tXyY0kiRJkpaUYe6p+VaSPRc8EkmSJElaBzOWnyXZqKpuAf4I+JskFwO/BkI3iWOiI0mSJC0B47762Wz31HwL2BN42iLFIkmSJEnzNltSE4CquniRYpEkSZKkeZstqdk6yaEz7ayqty9APJIkSZLmoQhl+dmMlgObwJhfIUmSJElL2mxJzc+r6g2LFokkSZIkrYM576mRJEmStLRNDPWkltuv2c7+8YsWhSRJkqSxk+SQJBcluTDJSUnuvC79zJjUVNWadQ9PkiRJkmaWZHvgpcCKqnow3T39z1qXvmYrP5MkSZLUgIlq9s6RjYC7JLkZ2Bi4cl06Ge/iO0mSJEkjUVVXAG8Dfgb8HLiuqr6yLn2Z1EiSJElaKFslWTnwOnhyR5ItgKcC9wW2A+6a5DnrMojlZ5IkSVLDCli7dBcuXl1VK2bY9wTgkqpaBZDks8BewMfmO4gzNZIkSZJG4WfAo5JsnCR0qy9/b106MqmRJEmStOiq6izgM8C3gQvocpPj1qUvy88kSZKkpoVqdPWzqnod8Lr17ceZGkmSJElNM6mRJEmS1DTLzyRJkqSGFTCxdFc/WxTO1EiSJElqmkmNJEmSpKZZfiZJkiS1rGBto6ufbSjO1EiSJElqmkmNJEmSpKZZfiZJkiQ1zNXPnKmRJEmS1DiTmiUsyWFJLkpyfpJzkzwyyWlJViQ5q9/2sySr+vfnJvnFDNt3TPLTJFv1fVeSowfGenmSIwY+P6cf96Ik5yX5QJK7jeAySJIkSbOy/GyJSvJoYF9gz6q6qU9G7ji5v6oe2bc7CFhRVS+ecvxttie3mpa8CfjLJEdV1eopx+4DHAI8qaquSLIcOBC4B3DtBjtJSZIkbRDl6mdaorYFVlfVTQBVtbqqrtyA/d8CHEeXvEx1GPDyqrqiH3ttVR1fVT/YgONLkiRJG4RJzdL1FeDeSX6Y5L1JHrsAYxwLHJBk8ynbdwW+PWwnSQ5OsjLJyt+waoMGKEmSJM3FpGaJqqobgIcBBwOrgE/2JWUbcoxfAR8BXjpTmyS79ffkXJxkvxn6Oa6qVlTVio3ZekOGKEmSpDmFiSX6WiwmNUtYX/Z1WlW9Dngx8FcLMMw7gRcAdx3YdhGwZx/DBVW1B/Bl4C4LML4kSZK0XkxqlqgkOyfZaWDTHsClG3qcqloDfIousZl0FPC2JPca2GZCI0mSpCXJ1c+Wrk2A9/TLKN8C/JiuFO0zCzDW0XQzQQBU1ZeSbA18uV/57FrgQuCUBRhbkiRJ66GAiTFf/cykZomqqnOAvabZtfeUdicAJ0xz/G22V9WOA+83GXj/C2DjKW0/DHx4flFLkiRJi8/yM0mSJElNc6ZGkiRJatzaMS8/c6ZGkiRJUtNMaiRJkiQ1zfIzSZIkqXG1iA+6XIqcqZEkSZLUNJMaSZIkSU2z/EySJElqmA/fdKZGkiRJUuNMaiRJkiQ1zfIzSZIkqWUVH7456gAkSZIkaX2Y1EiSJElqmuVnkiRJUsO61c9GHcVoOVMjSZIkqWkmNZIkSZKaZvmZJEmS1Lhy9TNJkiRJapdJjSRJkqSmWX4mSZIkNaxb/czyM0mSJElqlkmNJEmSpKZZfiZJkiQ1bgLLzyRJkiSpWSY1kiRJkppm+ZkkSZLUsALWuvqZJEmSJLXLpEaSJElS0yw/kyRJklpWoSw/kyRJkqR2mdRIkiRJaprlZ5IkSVLjJiYsP5MkSZKkZpnUSJIkSWqa5WeSJElSw3z4pjM1kiRJkhpnUiNJkiSpaZafSZIkSS0rmLD8TJIkSZLaZVIjSZIkqWmWn0mSJEmNK8vPJEmSJKldztRItxMTy0cdgTQaD/vqMaMOYaxdvfqPRh3C2DqClaMOQVoynKmRJEmS1DRnaiRJkqSGFXFJ51EHIEmSJEnrw6RGkiRJUtMsP5MkSZIaN1GjjmC0nKmRJEmS1DSTGkmSJElNs/xMkiRJalgVrJ1w9TNJkiRJapZJjSRJkqSmWX4mSZIkNa58+KYkSZIktcukRpIkSVLTLD+TJEmSGjdh+ZkkSZIktcukRpIkSVLTLD+TJEmSGlb48E1naiRJkiQ1zaRGkiRJ0sgkuVuSzyT5fpLvJXn0fPuw/EySJElqWaX11c/eBfxnVT0jyR2BjefbgUmNJEmSpJFIshnwGOAggKr6HfC7+fZj+ZkkSZKkUbkfsAr4UJLvJPlAkrvOtxOTGkmSJKlhBdTE0nwBWyVZOfA6eEr4GwF7Av9WVQ8Ffg28ar7XwPIzSZIkSQtldVWtmGX/5cDlVXVW//kzrENS40yNJEmSpJGoqquAy5Ls3G96PPDd+fbjTI0kSZLUuMZXP3sJcGK/8tn/a+++wy2r63uPvz8zSlEpURCRIoLYKAIORbgBFWJMRIzt4pgYUZRoYkEDFqwxYgMxxhATLEFjri2IFyuoFEWKDoQqCjYUhAvYKDaY+d4/1jqyPZwZmHPO3mvWWe/X8+yHvdbae+3PWTPMzHf/vuv3+z7wnNU9gUWNJEmSpM5U1QXAqlrU7pTtZ5IkSZJ6zZEaSZIkqc8KVqzodfvZnDlSI0mSJKnXLGokSZIk9ZrtZ5IkSVKPFbC837OfzZkjNZIkSZJ6zaJGkiRJUq/ZfiZJkiT1XDn7mSRJkiT1l0WNJEmSpF6z/UySJEnqsQJWVNcpuuVIjSRJkqRes6iRJEmS1Gu2n0mSJEl9VmG5s59JkiRJUn9Z1EiSJEnqNYuaCUnymiSXJrkoyQVJTmv/+90kv2yfX5Bkz/b1Gye5NcnfTDvPD5OcMLL9tCTHt88PSnJ9kv9JckWSk6fO1x4/PsnT2uenJ1k2cmxJktNHtndrX3NFkvOTfC7JDuO6PpIkSZqdAlasyBr5mBTvqZmAJI8C9gd2qarfJtkIWKuqfpLk0cBhVbX/tLc9HTgHWAr8+7RjS5JsV1WXzvBxH6+qF7Wf+xjgU0keU1WXzfDa+yb5s6r6wrS8mwCfAJ5ZVWe1+/4XsA1w8Wr86JIkSdLYOVIzGZsCN1TVbwGq6oaq+smdvGcp8PfA5kk2m3bsaOCIO/vQqjoNOA44ZCUvOQp47Qz7XwR8aKqgac91ZlV9+s4+U5IkSZo0i5rJOAXYIsnlSf41yT6renGSLYD7VdU3aEZMDpz2kk8AuyR50F347POBh67k2NnAb9sRnVHbte+TJElSD1RljXxMikXNBFTVzcAjaUZMrgc+nuSgVbzlGTSFC8DHaEZtRi2nGWV59V34+Dv73fRmZh6tuf0EyblJLkvy7pUcPyTJsiTLfsX1dyGSJEmSNH8saiakqpZX1elV9Qaa9q6nruLlS4GDkvwQOAl4RJJtp73mP4G9gS3v5KN3Bma6n2Yq16nAOsAeI7svBXYZec3uwOuADVZyjuOqaklVLbkHG99JHEmSJGl+WdRMQJKHTCtKdgKuXNlrgXtW1WZVtVVVbQW8lWb05veq6lbgXcChq/jcfWhGh953JxGPBF4xsn0sTVG158i+e9zJOSRJktSFghUr1szHpDj72WTcC3hPkg2B24DvsvKb95cCJ07bdwJNG9o/Ttv/Ae7YOnZgO1PZPYAfAE9dycxnv1dVn09y/cj2tUkOBN7eTlJwHXAD8KZVnUeSJEnqgkXNBFTVecCeKzl2OnD6yPYbZ3jNRcDD2+dbjez/LXD/ke3jgeNXkeOgkeePnnbskdO2zwFWOaGBJEmStCawqJEkSZJ6bGrxzSHznhpJkiRJvWZRI0mSJKnXbD+TJEmS+qxgue1nkiRJktRfFjWSJEmSes32M0mSJKnHijj7WdcBJEmSJGkuLGokSZIk9ZrtZ5IkSVLP1YquE3TLkRpJkiRJvWZRI0mSJKnXbD+TJEmS+qxgeTn7mSRJkiT1lkWNJEmSpF6z/UySJEnqsQIX3+w6gCRJkiTNhUWNJEmSpF6z/UySJEnquRUuvilJkiRJ/WVRI0mSJKnXbD+TJEmS+qygnP1MkiRJkvrLokaSJElSr9l+JkmSJPWYi286UiNJkiSp5yxqJEmSJPWa7WeSJElSnxUsd/FNSZIkSeovixpJkiRJvWb7mSRJktRjRZz9rOsAkiRJkjQXFjWSJEmSes32M0mSJKnPCmq57WeSJEmS1FsWNZIkSZJ6zfYzSZIkqccKF990pEaSJElSr1nUSJIkSeo1288kSZKknnPxTUmSJEnqMYsaSZIkSb1m+5m0QNzwgK4TDNv9v911guH6i7et03WEQXsjy7qOMFhvZNjtRl36bNcBpitY4exnkiRJktRfFjWSJEmSes32M0mSJKnnsobOflYT+hxHaiRJkiT1mkWNJEmSpF6z/UySJEnqs4LFy9fM9rPbJvQ5jtRIkiRJ6jWLGkmSJEm9ZvuZJEmS1GMBFrn4piRJkiT1l0WNJEmSpF6z/UySJEnqswqL1tDFNyfFkRpJkiRJnUmyOMn/JPnsbM9hUSNJkiSpSy8FLpvLCWw/kyRJknouy7tOMDtJNgeeABwJvHy253GkRpIkSVJX/gl4BTCnSaktaiRJkiSNy0ZJlo08Dpk6kGR/4LqqOm+uH2L7mSRJktRjKVi85s5+dkNVLVnJsb2AA5L8ObAOsH6Sj1TVX63uhzhSI0mSJGniqurVVbV5VW0FPAM4dTYFDVjUSJIkSeo5288kSZKknls0p9vsu1dVpwOnz/b9jtRIkiRJ6jWLGkmSJEm9ZvuZJEmS1GMpWLR8jZ39bCIcqZEkSZLUaxY1kiRJknrN9jNJkiSp57LmLr45EY7USJIkSeo1ixpJkiRJvWb7mSRJktRjKVi8vOsU3XKkRpIkSVKvWdRIkiRJ6jXbzyRJkqReC4uc/UySJEmS+suiRpIkSVKv2X4mSZIk9VnBImc/kyRJkqT+sqiRJEmS1Gu2n0mSJEk9FiDOfiZJkiRJ/WVRI0mSJKnXbD+TJEmS+qxgsbOfSZIkSVJ/WdT0RJKbV3HswiQfHdk+JMnHR7bXT/K9JA9McnySp7X7T0+ybOR1S5KcPrK9W/uaK5Kcn+RzSXaY9x9OkiRJmgOLmp5L8jCaX8e9k9yz3f0+YPMk+7XbbwI+WFU/mOEU903yZzOcdxPgE8ARVbVtVe0CvBXYZt5/CEmSJM1agEUr1szHpFjU9N8zgf8ETgEOAKiqAl4I/FOSJcC+wFEref9RwGtn2P8i4ENVddbUjqo6s6o+PY/ZJUmSpDmzqOm/A4GPAx8Flk7trKqLgJOBrwAvqarfreT9ZwO/TfKYafu3A86f/7iSJEnS/LKo6bEkuwLXV9WVNMXLLkn+aOQlxwJXV9Vpd3KqNzPzaM3oZ52b5LIk757h2CFJliVZ9iuuX82fQpIkSXNSsGh51sjHpFjU9NtS4KFJfgh8D1gfeOrI8RXtY5Wq6lRgHWCPkd2XAruMvGZ34HXABjO8/7iqWlJVS+7BxrP4MSRJkqTZs6jpqSSLgKcDO1bVVlW1FfAkRlrQVtORwCtGto8FDkqy58i+e8zy3JIkSdLYuPhmf9wjyVUj28fQtJZdPbLvq8DDk2xaVdeszsmr6vNJrh/ZvjbJgcDbk2wGXAfcQDOTmiRJktYgmeBMY2sii5qeqKqZRtWOmfaa5cCmI9s/BLaf9pqDRp4/etqxR07bPgfYZ5aRJUmSpImw/UySJElSrzlSI0mSJPVYChZPcKaxNZEjNZIkSZJ6zaJGkiRJUq/ZfiZJkiT13KLlXSfoliM1kiRJknrNokaSJElSr9l+JkmSJPVYChatcPYzSZIkSeotixpJkiRJvWb7mSRJktRzcfYzSZIkSeovixpJkiRJvWb7mSRJktRnFRYvd/YzSZIkSeotixpJkiRJvWb7mSRJktRjKVjk7GeSJEmS1F8WNZIkSZJ6zfYzSZIkqecWreg6QbccqZEkSZLUaxY1kiRJknrN9jNJkiSpzwri4puSJEmS1F8WNZIkSZJ6zfYzSZIkqccCLHbxTUmSJEnqL4saSZIkSb1m+5kkSZLUZwWLbD+TJEmSpP6yqJEkSZLUa7afSZIkST0WYJGLb0qSJElSf1nUSJIkSeo1288kSZKkPivIiq5DdMuRGkmSJEm9ZlEjSZIkqddsP5MkSZJ6LMBiF9+UJEmSpP6yqJEkSZLUa7afSZIkSX1WLr7pSI0kSZKkXrOokSRJktRrtp9pXl3DeTf8A7my6xxzsBFwQ9chZuXbXQeYs/5e+4XB698dr313en3t/6HrAHPX5+v/gK4D/IGCRQOf/cyiRvOqqjbuOsNcJFlWVUu6zjFEXvtuef2747Xvjte+W15/zSfbzyRJkiT1miM1kiRJUo8F288cqZH+0HFdBxgwr323vP7d8dp3x2vfLa+/5k2qqusMkiRJkmZp/Y0eWbvuf07XMWZ06ofWOm8S907ZfiZJkiT1WY8X30yyBfBh4H7ACuC4qnr36p7HokaSJElSV24D/r6qzk+yHnBeki9V1bdW5yTeUyNJkiSpE1V1TVWd3z6/CfdVnYQAAB0YSURBVLgM2Gx1z+NIjaSJS/JE4KKqurLdfj3wVOBK4KVV9YMu8w1JkvsAewM/qqrzus6z0CVZH9ikqq5ot58OrNsePrmq/l9n4ST11kKZ/SzJVsDOwLmr+15HajRISQ5OcvjI9tVJbkxyU5IXdpltII4ErgdIsj/wV8BzgZOAf+sw14KX5LNJtm+fbwpcQnPt/zPJoZ2GG4ajgb1Gtt8K7EpTWC6ABeLXXEm2S3LAyPa7knywfezSZbYh8PoP2kZJlo08DpnpRUnuBZwAHFpVN67uh1jUaKheAHxwZPu6qlof2BhY2k2kQamq+lX7/CnAB6rqvKp6P82vgcbngVV1Sfv8OcCXquqJwO40xY3Ga1fgQyPbN1XVi6vqecD2HWUaircBN4xs/ynwOeA04PWdJBoWr/9w3VBVS0Yed5jKO8ndaQqa/6qqT83mQ2w/01Atqqqfjmx/EqCqfpNk3ZW8R/Mn7TcyvwL2Bf515Ng63UQajFtHnu8LvA+aPuYkK7qJNCh3qz9cS+FZI883nHSYgdm0qs4a2b6xqk4ASPI3HWUaEq//OFV/28+SBPgAcFlVHTPb81jUaKg2GN2oqrcAJFkE3KeTRMPyT8AFwI00f4gtA0iyM3BNl8EG4MdJXgxcBewCfBGgLebv3mWwgViR5H5VdS3A1KhZks1opjLV+Kw3ulFVe4xs3nfCWYbI66+V2YvmC56Lk1zQ7juiqj6/Oiex/UxDdUqSN8+w/03AKZMOMzRV9UFgH+Bg4M9HDl0LHNRFpgE5GNiO5jofWFW/aPfvAfxHV6EG5CjgM0n2TrJe+9gH+HR7TOPzkyS7T9+ZZA/gJx3kGRqvv2ZUVWdWVapqx6raqX2sVkEDjtRouA4H3p/ku8CF7b5HAMuA53WWakCq6mrg6mm71wcOA54/+UTDUFXX0dxTNn3/aUm+30GkQamqjyS5AXgzTXEJzWQNr6+qL3SXbBBeCXw8yfHA+e2+RwLPBg7sKtSAeP3HqcftZ/PFokaDVFW3AEuTbM3t/7D4VlV9r8NYg5FkR5pZoO5P8w31e2juq9kdeGeH0QYhyaNo1gD4alVd1/56vAr4Y2CLTsMNQFV9kbbtT5NTVd9oRwX+jttHhC8F9nAq7fHz+mvcLGo0SEm2bJ/exu0jNb/fX1U/6iLXgLwPeC9wNvB4mm/t/g/wl1X1my6DLXRJjgL2p7mn6ZVJPgv8LfAWnP1s7No1mVamquofJxZmgNp/PDvTVke8/honixoN1eeAolmvakrRTCd8X2BxF6EGZO2qOr59/p0khwGvqqqBD55PxBOAnduZ/v6Ippd9x6nFIDV2t8yw75409zrdB7CoGZMkp9H8OT+Tqqp9J5lnaLz+4xXCouW58xcuYBY1GqSq2mF0u13B9pXAfjTfWGu81mlnOpv6E/hmYMd2Wkeq6vyVvlNz9eup0bCq+nmS71jQTE5V/b69Msl6wEtp1gv6GLZejtthM+zbA3gFcN2EswyR119jZVGjQUuyLfAabr+X4yVVdeuq36V5cC1wzEq2C3jsxBMNxzZJThrZ3mp0u6oOmOE9mkdJ7g28HPhLmoU4d6mqn3ebauGrqvOmnrczzr0OWBt4gZM0jJ/XX+NmUaNBSrI9TTGzHfAO4GBbnyanqh7ddYYBe9K0bUcHJqi9p+kpwHHADlV1c8eRBiXJn9L8Y/o3wJFVdVrHkQbF6z9Gzn5G/nBhY2kYkiwHfkxzb80d/hioqpdMPNSAJHnKqo5X1acmlUWapCQrgN/STFIy+hdwaO4rWL+TYAOQ5Js0900eRTNJyR+w7XW8vP7jteGGS2qfvc/tOsaMTvrM3c6rqiXj/hxHajRUB7PyGxY1fk9cxbECLGrGJMnFzPx7f+of1TtOONKgVJWLXnfnFpr7957WPkbZ9jp+Xn+NlUWNBmlk5i11oKqes7JjSTaZZJYB2r/rAEPW3k+zUlX1s0llGRrbXrvl9R+v2H5mUaNhSvIZVjFS483Sk5VkA+CpwDOBh9EsDKkxqKorZ9qfZC+a6/93k000OOdxx+nkpxSw9WTjDIdtr93y+mvcLGo0VEd3HWDokqwLHEDzD+ldgPWAvwC+2mWuIUmyE831/9/AD7DtbxIevbLCUmNn22u3vP4aK4saDdVaVfWlmQ4keTtwxoTzDEqS/wL2Bk4B/gU4FfhuVZ3eZa4hSPJg4BnAUuCnwMdpJo15TKfBhuNEmiJeE7aqtldNxBst6Mdr6O1n3rCooTo2yRNGdyRZlOR44BHdRBqU7YGfA5cB326n03bihsn4NrAv8MSq+l9V9R5mmAFQYzPsJb87luQhSd6Z5HPt4+i20Nf4fSXJq5L4hbrGwt9YGqrHAV9MsnZVfapthfokcCOrHiLXPKiqRyR5KE3r05eTXAesl+R+VXVtx/EWuqfSjNScluSLNCvZ+w/tydksyT+v7KDTyY9PkkfRtDgd1z4C7AycnuQpVXVOl/kGYGfgTcB5SV5cVbYaa15Z1GiQquqHSfYDTk5yX+BZwLlV9fKOow1GVX0beD3w+iRLaNqhvpHkqqras9t0C1dVnQicmOSeNPcwvQzYJMl7gROr6pROAy58v6aZLECT93pg6bQ2108nORV4A/BnnaQaiKq6CXhZkkfSjNpcBazA6eTnRTP72bC/n3LxTQ1Skqme9k2BDwNfAt4xddxFwMYryYuq6l9m2B9g76rynqYxSXK3qrpt2r57A08HDqwq14oYoyTnV5X31HQgyeVVNWOrWZLvVNVDJp1paJI8Fng3cDJwLE1RA6x8ZkbdNfdef0ntu/s3uo4xo//+8mIX35TG6J0jzy8CNhnZ5yJg4/dcmgkC/kA137JY0IzXN5h2o3q7Nsq/tw+N16ZdBxiwm1Zx7JaJpRioJB+jma7/mVV1cdd5tPBY1GiQVjXTU5I9JplFmrBh9yd0z3vGurPFSu5nCq6NNQlfqar3zXQgySZV9f8mHWihGfrsZxY10h19Atiy6xAL3I5Jbpxh/1Rv9fqTDjQgGydZ6b1jVXXMJMMMkD3f3Tl8FceWTSzFQE0vaFx0WfPNoka6I7/JHr+Lq2rnrkMM1GLgXvj7vCubO/tZN6rqQ11nGDoXXdY4WdRId+Q3qVrIrqmqN3UdYsCc/awjSf6Dlf/5XlV18CTzDI2LLo9Z2X5mUaNBSvIZZv7LLcB9JhxniD7ZdYABc4SmWz91xKAzn51h35bAoTQjmBqvOyy6nMQvETVvLGo0VEfP8pjmx/VJtq2qK9ppnD9I01v9Q+Agp9QeqycluXtV3QrNCuvAnwNXVtWnuo02CL/rOsBQVdUJU8+TbA0cQTNy8DbgA13lGgoXXda4WdRokFa2DkqSLWhWW3da4fF6KXB8+3wpsCPwQJoVp98N/HE3sQbhI8DBwBVJHgScDfwXsH+SXavq1Z2mW/j+bmSdrDuwoB+vJA8DXkPzZ81RwAumr9uk8XHR5fGJ7WcWNVKSjWgWHlxKM/vKid0mGoTbpkYKgP2BD1fVT2m+vXvHKt6nufujqrqiff5s4KNV9eIka9Hc62FRM15H07S+TrUBTm+/cY2sMUnySWAJza/By4DlwPrNYPHv12vShFTVMmBZksNpvuiS5sSiRoOUZD3gyTTD4A+mKWS2rqrNOw02HCuSbErTX70vcOTIsXW7iTQYo/+IfizNt9VU1e+SrJj5LZpHrwR+XFXXACR5Nre3Xr6xu1iDsCvN7//DgL9v940Wl1t3EWroqmpFkpcB7+o6i/rNokZDdR3NyuqvBc6sqkry5I4zDcnradaFWAycVFWXAiTZB/h+l8EG4KIkRwNXAw+imYmIJBt2mmo4/g3YDyDJ3sBbgRcDOwHHAU/rLtrCVlVbdZ1BK+UEJvNg6O1ni7oOIHXkCGAd4L3Aq5Ns03GeQamqzwIPAB5WVc8fObQMOLCbVIPxfOAGYCvgcVX1q3b/w3GSjElYPNLmdCBwXFWdUFWvoykyNUFJtknymiSXdJ1l4JwFTXNmUaNBqqp3VdXuNIuABfg0cP8kr0zy4G7TLXxJtgX+G/hako8m2Qygqm6pqpu7TbewVdWvq+ptVfXSqrpwZP9ZVfWfXWYbiMVJprok9qVZq2OK3RMTkGTTJIcm+QZwKc11X9pxrAUvyU1JbpzhcRNw/67zqf/8A1SDlORQ4Ezggqo6EjgyyQ40f7F9AXDkZrw+CHyYZhXpA4D3AE/pNNFAJDmNVS9AuO8k8wzQR4EzktxAsxDn1wDameh+2WWwhS7J82n+jN8c+ATwPOD/VtU/dBpsIKpqva4zLGTN7GfD7uKzqNFQbQ78M/DQJBcBZwFfB46uqiM6TTYM61XV+9rnRyVxGtvJOWyGfXsAr6C510xjVFVHJvkKsClwSlVNFZiLaO6t0fgcSzOF+TPbmbdw8Udp4bCo0SBV1WEA7TS2S4A9gecC70vyi6p6eJf5BmCdJDtz+82h645uu1bH+FTVeVPP24kZXgesTbNexxc6CzYgVXXODPsu7yLLwNyfZvr+Y5JsQjNac/duI0maLxY1Grp1gfWBDdrHT4CLO000DNcCx6xku3CtjrFK8qc0xcxvgCOr6rSOI0ljV1U30EwO894km9MstHxdksuAEx2lV98NffYzixoNUpLjgO2Am4BzadrPjqmqn3cabCCq6tFdZxiqJN8ENqZZn+bsdt/vV7h3lEwLVZI9pkbJquoqmtn+jk7yEJoCR1KPWdRoqLakabm5gma9jquAX3SaaECSTJ8UoGimGb6gqm7qINKQ3ALcTLMeyvQ1URwl00L2r8Au03dW1XcAJwuQes6iRoNUVY9PEprRmj1pVpfePsnPgLOr6g2dBlz4njjDvnsDOyY5uKpOneG45oGjZJK0AJXtZxY1Gqx21qFLkvyCZirVXwL7A7sBFjVjVFXPmWl/kgfQ3Ly7+2QTDUeSC2mmMz8L+HpV/bDbRNLEbJ3kpJUdrKoDJhlG0vyyqNEgJXkJzQjNXsCtNNM5n02zfooTBXSkqq5M4mxE4/WXNL/3/wR4Q5J70hQ4ZwFnVdW5XYaTxuh64J1dh5A0HhY1GqqtaFa0f1lVXdNxFrXaG3Z/23WOhayqLgEuAY4DSLIRzU3Sh9LcOL24u3TSWN1cVWd0HUIah9h+ZlGjYaqql3edYciSfIY7rmp/b5oFCf9q8omGI8liYGduH6nchmayjPfTzoYmLVA/T3K/qroWIMlfA08FrgTeWFU/6zSdpDmxqJHUhaOnbRfwU+CKqvpdB3mG5EbgMprV1V9VVT/oOI80KRsCvwNIsjfwNuDFwE40I5fTZwOU1CMWNZIm7q62gCQ5u6oeNe48A/M84FHtf5/TrltzNs2sf1d3mkwar0UjozEHAsdV1QnACUku6DCXNC9sP5OkNdc6XQdYaKrqo8BHAZLcg2a2v72AtyZZq6oe0GU+aYzuluRuVXUbsC9wyOixjjJJmif+TyxpTTb9vhvNg3bGs925/b6aXYEf08wCKC1UHwXOSHID8GvgawBJHkQzpb+kHrOokaQBSfI/wJbAMpppnN8JnFNVN3caTBqzqjoyyVdoJiQ5pV2rDGARzb01Um85+5lFjaQ1W7oOsAA9G7h45B900mBU1Tkz7Lu8iyyS5teirgNI0io8q+sAC01VXQRsl+RDSZYl+Wb7fMeus0mSNFsWNZImLsnBSQ4f2b46yY1Jbkrywqn97UKRmkdJngScCJwBPJdmFrQzaGaAelKX2SRJs1Sw6LY18zEptp9J6sILgMePbF9XVZslWQc4BXhvN7EG4U3An1TVD0f2XZjkVOD/tg9JknrFkRpJXVhUVT8d2f4kQFX9Bli3m0iDcfdpBQ0A7b67TzyNJEnzwJEaSV3YYHSjqt4CkGQRcJ9OEg3HrUm2rKofje5M8gBggo0CkqT5tGj5sOfWcaRGUhdOSfLmGfa/iab9TOPzBuDLSQ5KskOS7ZM8h+a6v77jbJIkzYojNZK6cDjw/iTfBS5s9z2CZu2U53WWagCq6tNJfgD8Pc3aHAEuBf53VV24yjdLkrSGsqiRNHFVdQuwNMnWwHbt7m9V1fc6jDUYbfHy113nkCTNDxfftP1MUgeSbJlkS5p7OC5sH7eO7NcYJXl2kvOS3NI+liWxyJEk9ZYjNZK68DmgaFqfphSwMXBfYHEXoYagLV4OBV4OnE/za7ALcFQSqurDXeaTJGk2LGokTVxV7TC6nWQr4JXAfsBbOog0JH8LPHnatM6nJnkq8DHAokaSesj2M0nqSJJtkxwPfAE4D3h4Vb2n21QL3vqrWKdm/YmnkSRpHjhSI2nikmwPvIZmkoB3AAdX1cC/Y5qYX8/ymCRJayyLGklduBD4Mc29NbsBuyW3315TVS/pKNcQPCzJRTPsD7D1pMNIkubO2c8saiR142CaiQE0eQ/rOoAkSfPNokbSxFXV8V1nGKqquvKuvC7J2VX1qHHnkSRpPljUSJq4JJ9hFSM1VXXABONoZut0HUCSdBfZfmZRI6kTR3cdQHfK9kBJUm9Y1EjqwlpV9aWZDiR5O3DGhPNIkqQes6iR1IVjk7ysqj43tSPJIuCDwP26i6URufOXSJLWFLafSdLkPQ74YpK1q+pTSdYFPgncCDyx22hqPavrAJIk3VWLug4gaXja1ev3A/4xyQuALwOXV9Uzq+rWTsMtcEkOTnL4yPbVSW5MclOSF07tr6pLukkoSdLqc6RG0sQl2aV9+grgw8CXgI9M7a+q87vKNgAvAB4/sn1dVW2WZB3gFOC93cSSJM2Wi29a1EjqxjtHnl8EbDKyr4DHTjzRcCyqqp+ObH8SoKp+07YBSpLUOxY1kiauqh6zsmNJ9phklgHaYHSjqt4Cv5+o4T6dJJIkaY4saiStaT4BbNl1iAXslCRvrqrXTtv/Jpr2M0lS3xQsuq3rEN2yqJG0pnEq4fE6HHh/ku8CF7b7HgEsA57XWSpJkubAokbSmsaV7Meoqm4BlibZGtiu3f2tqvpeh7EkSZoTixpJE5fkM8xcvATv6xirJFOtfbdx+0jN7/dX1Y+6yCVJmr3g7GcWNZK6cPQsj2nuPkdTUI62+RWwMXBfYHEXoSRJmguLGkkTV1VnzLQ/yRbAM4AZj2vuqmqH0e0kWwGvpFkM9S0dRJIkac4saiR1KslGwNOBpcBmwIndJhqGJNsCrwF2p1kj6CVVdWu3qSRJs+LimxY1kiYvyXrAk4FnAg+mKWS2rqrNOw02AEm2pylmtgPeARxcVQP/q1CS1HcWNZK6cB3wDeC1wJlVVUme3HGmobgQ+DHNvTW7Abslt99eU1Uv6SiXJGmAkjweeDfNPZ3vr6q3zeY8FjWSunAEzb0z7wX+T5KPd5xnSA7GabMlacHpY/tZksXAscCfAFcB30xyUlV9a3XPZVEjaeKq6l3Au9q1UpYCnwbun+SVwIlVdXmnARewqjq+6wySJLV2A75bVd8HSPIx4EmARY2kNV+SQ4EzgQuq6kjgyCQ70BQ4XwC26TLfQraKNYIAqKoDJhhHkjRsm9G0RE+5imYCm9VmUSOpC5sD/ww8NMlFwFnA14Gjq+qITpMtfK4DJEkLzDWcd/IbyUZd51iJdZIsG9k+rqqOa59nhtfPqkXaokbSxFXVYQBJ1gKWAHsCzwXel+QXVfXwLvMtcGtV1ZdmOpDk7bhGkCT1TlU9vusMs3QVsMXI9ubAT2ZzokXzEkeSZmddYH1gg/bxE+DcThMtfMcmecLojiSLkhwPPKKbSJKkgfomsG2SB7ZfdD4DOGk2J3KkRtLEJTmOZp2Um2iKmLOAY6rq550GG4bHAV9MsnZVfSrJusAngRuBJ3YbTZI0JFV1W5IXASfTTOn8waq6dDbnsqiR1IUtgbWBK4CraYaff9FpooGoqh8m2Q84Ocl9gWcB51bVyzuOJkkaoKr6PPD5uZ4nVS5XIGny0qz4uB3N/TR7AtsDPwPOrqo3dJltIUuyS/t0U+DDwJeAd0wdr6rzu8glSdJcWNRI6lSSzYG9aAqb/YH7VNWG3aZauJKctorDVVWPnVgYSZLmiUWNpIlL8hKaImYv4Faa6ZzPbv97cVWt6DDeYCXZo6rO6TqHJEmry6JG0sQlOYZ2bZqquqbrPGok+VFVbdl1DkmSVpdFjSQJgCQ/rqot7vyVkiStWVynRpI0xW+5JEm95JTOkjQgST7DzMVLgPtMOI4kSfPC9jNJGpAk+6zqeFWdMakskiTNF4saSRJJtgCeUVVHdZ1FkqTV5T01kjRQSTZK8sIkXwVOBzbpOJIkSbPiPTWSNCBJ1gOeDDwTeDBwIrB1VW3eaTBJkubA9jNJGpAkvwa+AbwWOLOqKsn3q2rrjqNJkjRrtp9J0rAcAawDvBd4dZJtOs4jSdKcOVIjSQOUZGtgKfAMYFvgDcCJVXV5p8EkSZoFixpJGpAkhwJnAhdU1W3tvh1oCpwDq8qRG0lS71jUSNKAJDka2BN4KHARcBbwdeDsqvpZl9kkSZotixpJGqAkawFLaAqcR7WPX1TVwzsNJknSLDilsyQN07rA+sAG7eMnwMWdJpIkaZYcqZGkAUlyHLAdcBNwLnAOcE5V/bzTYJIkzYFTOkvSsGwJrA1cC1wNXAX8otNEkiTNkSM1kjQwSUIzWrNn+9ge+BnNZAFv6DKbJEmzYVEjSQOVZHNgL5rCZn/gPlW1YbepJElafRY1kjQgSV5CU8TsBdxKO51z+9+Lq2pFh/EkSZoVZz+TpGHZCvhv4GVVdU3HWSRJmheO1EiSJEnqNWc/kyRJktRrFjWSJEmSes2iRpK0WpIsT3JBkkuSfDLJPeZwrkcn+Wz7/IAkr1rFazdM8rez+Iw3Jjnsru6f9prjkzxtNT5rqySXrG5GSdLcWNRIklbXr6tqp6raHvgd8ILRg2ms9t8vVXVSVb1tFS/ZEFjtokaStPBZ1EiS5uJrwIPaEYrLkvwrcD6wRZLHJTk7yfntiM69AJI8Psm3k5wJPGXqREkOSvIv7fNNkpyY5ML2sSfwNmCbdpToqPZ1hyf5ZpKLkvzDyLlek+Q7Sb4MPOTOfogkz2/Pc2GSE6aNPu2X5GtJLk+yf/v6xUmOGvnsv5nrhZQkzZ5FjSRpVpLcDfgz4OJ210OAD1fVzsAtwGuB/apqF2AZ8PIk6wDvA54I/DFwv5Wc/p+BM6rqEcAuwKXAq4DvtaNEhyd5HLAtsBuwE/DIJHsneSTwDGBnmqJp17vw43yqqnZtP+8y4OCRY1sB+wBPAP6t/RkOBn5ZVbu2539+kgfehc+RJI2B69RIklbXukkuaJ9/DfgAcH/gyqo6p92/B/Bw4OtJANaiWeTzocAPquoKgCQfAQ6Z4TMeC/w1QFUtB36Z5I+mveZx7eN/2u170RQ56wEnVtWv2s846S78TNsneTNNi9u9gJNHjn2iXZT0iiTfb3+GxwE7jtxvs0H72Zffhc+SJM0zixpJ0ur6dVXtNLqjLVxuGd0FfKmqlk573U7AfC2QFuCtVfXv0z7j0Fl8xvHAX1TVhUkOAh49cmz6uar97BdX1WjxQ5KtVvNzJUnzwPYzSdI4nAPsleRBAEnukeTBwLeBBybZpn3d0pW8/yvAC9v3Lk6yPnATzSjMlJOB547cq7NZkvsCXwWenGTdJOvRtLrdmfWAa5LcHfjLaceenmRRm3lr4DvtZ7+wfT1JHpzknnfhcyRJY+BIjSRp3lXV9e2Ix0eTrN3ufm1VXZ7kEOBzSW4AzgS2n+EULwWOS3IwsBx4YVWdneTr7ZTJX2jvq3kYcHY7UnQz8FdVdX6SjwMXAFfStMjdmdcB57avv5g/LJ6+A5wBbAK8oKp+k+T9NPfanJ/mw68H/uKuXR1J0nxL1Xx1AUiSJEnS5Nl+JkmSJKnXLGokSZIk9ZpFjSRJkqRes6iRJEmS1GsWNZIkSZJ6zaJGkiRJUq9Z1EiSJEnqNYsaSZIkSb32/wF42v2ekCTA9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "f = plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "f.savefig(\"cnn_conf.pdf\", bbox_inches='tight')\n",
    "f.savefig(\"cnn_conf.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
