{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "WARNING:tensorflow:From c:\\users\\edwar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-235ff3d07f57>:194: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.426562, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3549656867980957, Accuracy = 0.15982355177402496\n",
      "Training iter #30000:   Batch Loss = 1.202447, Accuracy = 0.6946666836738586\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2348134517669678, Accuracy = 0.5554801225662231\n",
      "Training iter #60000:   Batch Loss = 0.817100, Accuracy = 0.8140000104904175\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0145211219787598, Accuracy = 0.7777400612831116\n",
      "Training iter #90000:   Batch Loss = 0.706162, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.022066354751587, Accuracy = 0.80861896276474\n",
      "Training iter #120000:   Batch Loss = 0.592792, Accuracy = 0.9226666688919067\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.203288197517395, Accuracy = 0.7611129879951477\n",
      "Training iter #150000:   Batch Loss = 0.516418, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9733090400695801, Accuracy = 0.7953851222991943\n",
      "Training iter #180000:   Batch Loss = 0.455209, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.076472282409668, Accuracy = 0.7899559140205383\n",
      "Training iter #210000:   Batch Loss = 0.449536, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1504497528076172, Accuracy = 0.7913131713867188\n",
      "Optimization Finished!\n",
      "Training time is: 126.32524728775024 seconds\n",
      "FINAL RESULT: Batch Loss = 1.2390209436416626, Accuracy = 0.7858839631080627\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.510724, Accuracy = 0.15133333206176758\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.373626708984375, Accuracy = 0.17000339925289154\n",
      "Training iter #30000:   Batch Loss = 1.603356, Accuracy = 0.5019999742507935\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5759363174438477, Accuracy = 0.501526951789856\n",
      "Training iter #60000:   Batch Loss = 0.937808, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.01474130153656, Accuracy = 0.7265015244483948\n",
      "Training iter #90000:   Batch Loss = 0.677190, Accuracy = 0.878000020980835\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7812082171440125, Accuracy = 0.8425517678260803\n",
      "Training iter #120000:   Batch Loss = 0.532080, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7243513464927673, Accuracy = 0.8690193295478821\n",
      "Training iter #150000:   Batch Loss = 0.477394, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7020394802093506, Accuracy = 0.8832710981369019\n",
      "Training iter #180000:   Batch Loss = 0.444054, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6809254884719849, Accuracy = 0.8900576829910278\n",
      "Training iter #210000:   Batch Loss = 0.407158, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7053549289703369, Accuracy = 0.882253110408783\n",
      "Training iter #240000:   Batch Loss = 0.404727, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6553492546081543, Accuracy = 0.898201584815979\n",
      "Training iter #270000:   Batch Loss = 0.371765, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6709507703781128, Accuracy = 0.894808292388916\n",
      "Training iter #300000:   Batch Loss = 0.375950, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6637027859687805, Accuracy = 0.8992195725440979\n",
      "Training iter #330000:   Batch Loss = 0.365216, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6469237208366394, Accuracy = 0.9022734761238098\n",
      "Training iter #360000:   Batch Loss = 0.337552, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6571739912033081, Accuracy = 0.8944689631462097\n",
      "Training iter #390000:   Batch Loss = 0.322463, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6579012870788574, Accuracy = 0.8985409140586853\n",
      "Training iter #420000:   Batch Loss = 0.316668, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6377679109573364, Accuracy = 0.9012554883956909\n",
      "Optimization Finished!\n",
      "Training time is: 249.3763711452484 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6488886475563049, Accuracy = 0.8985409140586853\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.685859, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.534871816635132, Accuracy = 0.18221920728683472\n",
      "Training iter #30000:   Batch Loss = 1.665562, Accuracy = 0.5839999914169312\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6069267988204956, Accuracy = 0.6077367067337036\n",
      "Training iter #60000:   Batch Loss = 0.995813, Accuracy = 0.7540000081062317\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9081277847290039, Accuracy = 0.7980997562408447\n",
      "Training iter #90000:   Batch Loss = 0.694052, Accuracy = 0.8980000019073486\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7991501092910767, Accuracy = 0.8595181703567505\n",
      "Training iter #120000:   Batch Loss = 0.662257, Accuracy = 0.8986666798591614\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.724036455154419, Accuracy = 0.8853070735931396\n",
      "Training iter #150000:   Batch Loss = 0.554145, Accuracy = 0.9480000138282776\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6651386022567749, Accuracy = 0.8968442678451538\n",
      "Training iter #180000:   Batch Loss = 0.512135, Accuracy = 0.968666672706604\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6546238660812378, Accuracy = 0.9026128053665161\n",
      "Training iter #210000:   Batch Loss = 0.515212, Accuracy = 0.9626666903495789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6666832566261292, Accuracy = 0.8968442678451538\n",
      "Training iter #240000:   Batch Loss = 0.444549, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.680066704750061, Accuracy = 0.8927723169326782\n",
      "Training iter #270000:   Batch Loss = 0.436058, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6072707176208496, Accuracy = 0.9097387194633484\n",
      "Training iter #300000:   Batch Loss = 0.439655, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6053688526153564, Accuracy = 0.9148286581039429\n",
      "Training iter #330000:   Batch Loss = 0.413910, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6133298277854919, Accuracy = 0.9189005494117737\n",
      "Training iter #360000:   Batch Loss = 0.401043, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6135772466659546, Accuracy = 0.9185612201690674\n",
      "Training iter #390000:   Batch Loss = 0.378239, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6217949390411377, Accuracy = 0.9168646335601807\n",
      "Training iter #420000:   Batch Loss = 0.364530, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6040523052215576, Accuracy = 0.9185612201690674\n",
      "Training iter #450000:   Batch Loss = 0.356847, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6146231293678284, Accuracy = 0.9151679873466492\n",
      "Training iter #480000:   Batch Loss = 0.346393, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6195420026779175, Accuracy = 0.9151679873466492\n",
      "Training iter #510000:   Batch Loss = 0.338682, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6264799237251282, Accuracy = 0.9144893288612366\n",
      "Training iter #540000:   Batch Loss = 0.323775, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6147143840789795, Accuracy = 0.9178826212882996\n",
      "Training iter #570000:   Batch Loss = 0.317060, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6359354257583618, Accuracy = 0.9144893288612366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #600000:   Batch Loss = 0.312185, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5976765751838684, Accuracy = 0.9195792078971863\n",
      "Training iter #630000:   Batch Loss = 0.304169, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.625015139579773, Accuracy = 0.9124533534049988\n",
      "Training iter #660000:   Batch Loss = 0.296204, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6085476875305176, Accuracy = 0.9178826212882996\n",
      "Optimization Finished!\n",
      "Training time is: 370.1370780467987 seconds\n",
      "FINAL RESULT: Batch Loss = 0.6129117012023926, Accuracy = 0.9165253043174744\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.858523, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.710886240005493, Accuracy = 0.16661010682582855\n",
      "Training iter #30000:   Batch Loss = 1.659389, Accuracy = 0.5406666398048401\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5995546579360962, Accuracy = 0.5670173168182373\n",
      "Training iter #60000:   Batch Loss = 0.877558, Accuracy = 0.8186666369438171\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.865088939666748, Accuracy = 0.8092975616455078\n",
      "Training iter #90000:   Batch Loss = 0.562321, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7226420640945435, Accuracy = 0.854428231716156\n",
      "Training iter #120000:   Batch Loss = 0.527559, Accuracy = 0.9279999732971191\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6505345702171326, Accuracy = 0.8788598775863647\n",
      "Training iter #150000:   Batch Loss = 0.474914, Accuracy = 0.9493333101272583\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6184998154640198, Accuracy = 0.8934509754180908\n",
      "Training iter #180000:   Batch Loss = 0.451788, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5795783996582031, Accuracy = 0.9080420732498169\n",
      "Training iter #210000:   Batch Loss = 0.428896, Accuracy = 0.9646666646003723\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5735064744949341, Accuracy = 0.9155073165893555\n",
      "Training iter #240000:   Batch Loss = 0.384107, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5728371143341064, Accuracy = 0.9141499996185303\n",
      "Training iter #270000:   Batch Loss = 0.389456, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5627919435501099, Accuracy = 0.9175432920455933\n",
      "Training iter #300000:   Batch Loss = 0.365467, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6065374612808228, Accuracy = 0.9063454270362854\n",
      "Training iter #330000:   Batch Loss = 0.360649, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5649648904800415, Accuracy = 0.9127926826477051\n",
      "Training iter #360000:   Batch Loss = 0.337118, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5489445924758911, Accuracy = 0.9202578663825989\n",
      "Training iter #390000:   Batch Loss = 0.314854, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5538036227226257, Accuracy = 0.9165253043174744\n",
      "Training iter #420000:   Batch Loss = 0.332472, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5584532022476196, Accuracy = 0.9124533534049988\n",
      "Training iter #450000:   Batch Loss = 0.303391, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5681878328323364, Accuracy = 0.910417377948761\n",
      "Training iter #480000:   Batch Loss = 0.311278, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5631203651428223, Accuracy = 0.9087207317352295\n",
      "Training iter #510000:   Batch Loss = 0.290428, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5179845690727234, Accuracy = 0.9168646335601807\n",
      "Training iter #540000:   Batch Loss = 0.272164, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.558211088180542, Accuracy = 0.9093993902206421\n",
      "Training iter #570000:   Batch Loss = 0.270844, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5622649192810059, Accuracy = 0.9029521346092224\n",
      "Training iter #600000:   Batch Loss = 0.262928, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5611470937728882, Accuracy = 0.9097387194633484\n",
      "Training iter #630000:   Batch Loss = 0.260855, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5783672332763672, Accuracy = 0.9039701223373413\n",
      "Training iter #660000:   Batch Loss = 0.247472, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5605347752571106, Accuracy = 0.903630793094635\n",
      "Training iter #690000:   Batch Loss = 0.238833, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5920354723930359, Accuracy = 0.8992195725440979\n",
      "Training iter #720000:   Batch Loss = 0.230515, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5759078860282898, Accuracy = 0.8978622555732727\n",
      "Training iter #750000:   Batch Loss = 0.239470, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5586888790130615, Accuracy = 0.9090600609779358\n",
      "Training iter #780000:   Batch Loss = 0.233593, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5350409746170044, Accuracy = 0.9121140241622925\n",
      "Training iter #810000:   Batch Loss = 0.222290, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5067828893661499, Accuracy = 0.917203962802887\n",
      "Training iter #840000:   Batch Loss = 0.214988, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5936787724494934, Accuracy = 0.9056667685508728\n",
      "Training iter #870000:   Batch Loss = 0.208716, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5369089841842651, Accuracy = 0.9134713411331177\n",
      "Optimization Finished!\n",
      "Training time is: 492.6169419288635 seconds\n",
      "FINAL RESULT: Batch Loss = 0.5350266098976135, Accuracy = 0.9107567071914673\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.349392, Accuracy = 0.13733333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.250687837600708, Accuracy = 0.30539530515670776\n",
      "Training iter #30000:   Batch Loss = 1.323983, Accuracy = 0.6759999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.251556158065796, Accuracy = 0.721072256565094\n",
      "Training iter #60000:   Batch Loss = 0.886714, Accuracy = 0.8013333082199097\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.799013078212738, Accuracy = 0.8371224999427795\n",
      "Training iter #90000:   Batch Loss = 0.581150, Accuracy = 0.9179999828338623\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6754952073097229, Accuracy = 0.876823902130127\n",
      "Training iter #120000:   Batch Loss = 0.503861, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6001976728439331, Accuracy = 0.9046487808227539\n",
      "Training iter #150000:   Batch Loss = 0.458036, Accuracy = 0.9580000042915344\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5517923831939697, Accuracy = 0.9141499996185303\n",
      "Training iter #180000:   Batch Loss = 0.441357, Accuracy = 0.9620000123977661\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5598070621490479, Accuracy = 0.9209365248680115\n",
      "Training iter #210000:   Batch Loss = 0.446685, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5796078443527222, Accuracy = 0.917203962802887\n",
      "Training iter #240000:   Batch Loss = 0.423256, Accuracy = 0.9679999947547913\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5541648268699646, Accuracy = 0.9239904880523682\n",
      "Training iter #270000:   Batch Loss = 0.373626, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5649521946907043, Accuracy = 0.9239904880523682\n",
      "Training iter #300000:   Batch Loss = 0.340845, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.544106125831604, Accuracy = 0.9263657927513123\n",
      "Training iter #330000:   Batch Loss = 0.333713, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5414619445800781, Accuracy = 0.9300984144210815\n",
      "Training iter #360000:   Batch Loss = 0.349136, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5315122604370117, Accuracy = 0.9297590851783752\n",
      "Training iter #390000:   Batch Loss = 0.343835, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5201156139373779, Accuracy = 0.9328130483627319\n",
      "Training iter #420000:   Batch Loss = 0.333839, Accuracy = 0.9833333492279053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5061836242675781, Accuracy = 0.9311164021492004\n",
      "Training iter #450000:   Batch Loss = 0.299806, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.511863112449646, Accuracy = 0.9263657927513123\n",
      "Training iter #480000:   Batch Loss = 0.296450, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4883592128753662, Accuracy = 0.9358670115470886\n",
      "Training iter #510000:   Batch Loss = 0.277511, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47694650292396545, Accuracy = 0.9402782320976257\n",
      "Training iter #540000:   Batch Loss = 0.276887, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4821263551712036, Accuracy = 0.9382422566413879\n",
      "Training iter #570000:   Batch Loss = 0.275461, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47415032982826233, Accuracy = 0.9379029273986816\n",
      "Training iter #600000:   Batch Loss = 0.271016, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.462136447429657, Accuracy = 0.9402782320976257\n",
      "Training iter #630000:   Batch Loss = 0.257646, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4635664224624634, Accuracy = 0.9419748783111572\n",
      "Training iter #660000:   Batch Loss = 0.273912, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47742387652397156, Accuracy = 0.9307770729064941\n",
      "Training iter #690000:   Batch Loss = 0.243432, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.436454176902771, Accuracy = 0.9433321952819824\n",
      "Training iter #720000:   Batch Loss = 0.237856, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.422315388917923, Accuracy = 0.9375635981559753\n",
      "Training iter #750000:   Batch Loss = 0.245560, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.420017808675766, Accuracy = 0.9399389028549194\n",
      "Training iter #780000:   Batch Loss = 0.241576, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4215216636657715, Accuracy = 0.9402782320976257\n",
      "Training iter #810000:   Batch Loss = 0.219675, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4183187782764435, Accuracy = 0.9436715245246887\n",
      "Training iter #840000:   Batch Loss = 0.208752, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4099801778793335, Accuracy = 0.9416355490684509\n",
      "Training iter #870000:   Batch Loss = 0.201779, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40705955028533936, Accuracy = 0.9419748783111572\n",
      "Training iter #900000:   Batch Loss = 0.198626, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.409249871969223, Accuracy = 0.9419748783111572\n",
      "Training iter #930000:   Batch Loss = 0.202522, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41168013215065, Accuracy = 0.9409568905830383\n",
      "Training iter #960000:   Batch Loss = 0.198603, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3868155777454376, Accuracy = 0.9450288414955139\n",
      "Training iter #990000:   Batch Loss = 0.294316, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.51435387134552, Accuracy = 0.9005768299102783\n",
      "Training iter #1020000:   Batch Loss = 0.207732, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.38762736320495605, Accuracy = 0.9416355490684509\n",
      "Training iter #1050000:   Batch Loss = 0.190597, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3674987554550171, Accuracy = 0.9443501830101013\n",
      "Training iter #1080000:   Batch Loss = 0.186877, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35079991817474365, Accuracy = 0.9433321952819824\n",
      "Optimization Finished!\n",
      "Training time is: 614.8972370624542 seconds\n",
      "FINAL RESULT: Batch Loss = 0.3383646011352539, Accuracy = 0.944010853767395\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.515888, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3635876178741455, Accuracy = 0.16661010682582855\n",
      "Training iter #30000:   Batch Loss = 1.403779, Accuracy = 0.659333348274231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2988908290863037, Accuracy = 0.7193756103515625\n",
      "Training iter #60000:   Batch Loss = 0.693140, Accuracy = 0.8813333511352539\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7313711643218994, Accuracy = 0.8741092681884766\n",
      "Training iter #90000:   Batch Loss = 0.608591, Accuracy = 0.9139999747276306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.658726155757904, Accuracy = 0.8849677443504333\n",
      "Training iter #120000:   Batch Loss = 0.672085, Accuracy = 0.8759999871253967\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6089320778846741, Accuracy = 0.9087207317352295\n",
      "Training iter #150000:   Batch Loss = 0.611959, Accuracy = 0.8880000114440918\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5791134238243103, Accuracy = 0.9189005494117737\n",
      "Training iter #180000:   Batch Loss = 0.497899, Accuracy = 0.9459999799728394\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5468311309814453, Accuracy = 0.9270444512367249\n",
      "Training iter #210000:   Batch Loss = 0.440130, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5131196975708008, Accuracy = 0.937224268913269\n",
      "Training iter #240000:   Batch Loss = 0.451437, Accuracy = 0.9613333344459534\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5139093995094299, Accuracy = 0.937224268913269\n",
      "Training iter #270000:   Batch Loss = 0.443717, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.473727822303772, Accuracy = 0.9545300602912903\n",
      "Training iter #300000:   Batch Loss = 0.436482, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.463691771030426, Accuracy = 0.950797438621521\n",
      "Training iter #330000:   Batch Loss = 0.417410, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4622679650783539, Accuracy = 0.9494401216506958\n",
      "Training iter #360000:   Batch Loss = 0.355579, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44215643405914307, Accuracy = 0.9528334140777588\n",
      "Training iter #390000:   Batch Loss = 0.356300, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4232337772846222, Accuracy = 0.9606379270553589\n",
      "Training iter #420000:   Batch Loss = 0.356287, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4173378646373749, Accuracy = 0.9569053053855896\n",
      "Training iter #450000:   Batch Loss = 0.377355, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4273160696029663, Accuracy = 0.950797438621521\n",
      "Training iter #480000:   Batch Loss = 0.330995, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41065168380737305, Accuracy = 0.9599592685699463\n",
      "Training iter #510000:   Batch Loss = 0.308651, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.39453059434890747, Accuracy = 0.9643705487251282\n",
      "Training iter #540000:   Batch Loss = 0.301908, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3887621760368347, Accuracy = 0.9579232931137085\n",
      "Training iter #570000:   Batch Loss = 0.311306, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4189761281013489, Accuracy = 0.9450288414955139\n",
      "Training iter #600000:   Batch Loss = 0.319974, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.37016093730926514, Accuracy = 0.9650492072105408\n",
      "Training iter #630000:   Batch Loss = 0.288284, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3761146068572998, Accuracy = 0.9592806100845337\n",
      "Training iter #660000:   Batch Loss = 0.266244, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.36160677671432495, Accuracy = 0.9647098779678345\n",
      "Training iter #690000:   Batch Loss = 0.272065, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3647024631500244, Accuracy = 0.9609772562980652\n",
      "Training iter #720000:   Batch Loss = 0.283842, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.350480854511261, Accuracy = 0.9650492072105408\n",
      "Training iter #750000:   Batch Loss = 0.331311, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.32959824800491333, Accuracy = 0.9670851826667786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #780000:   Batch Loss = 0.259662, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33716991543769836, Accuracy = 0.9640312194824219\n",
      "Training iter #810000:   Batch Loss = 0.239980, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34438061714172363, Accuracy = 0.9599592685699463\n",
      "Training iter #840000:   Batch Loss = 0.238651, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30412986874580383, Accuracy = 0.972175121307373\n",
      "Training iter #870000:   Batch Loss = 0.233248, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30237719416618347, Accuracy = 0.9667458534240723\n",
      "Training iter #900000:   Batch Loss = 0.282568, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30759677290916443, Accuracy = 0.9650492072105408\n",
      "Training iter #930000:   Batch Loss = 0.235384, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3205040693283081, Accuracy = 0.9592806100845337\n",
      "Training iter #960000:   Batch Loss = 0.212052, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3033159375190735, Accuracy = 0.9650492072105408\n",
      "Training iter #990000:   Batch Loss = 0.271985, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.29190176725387573, Accuracy = 0.9653885364532471\n",
      "Training iter #1020000:   Batch Loss = 0.204040, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2853384017944336, Accuracy = 0.9633525609970093\n",
      "Training iter #1050000:   Batch Loss = 0.202645, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2874544858932495, Accuracy = 0.9643705487251282\n",
      "Training iter #1080000:   Batch Loss = 0.205892, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2879892587661743, Accuracy = 0.9592806100845337\n",
      "Training iter #1110000:   Batch Loss = 0.188911, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2697591781616211, Accuracy = 0.9647098779678345\n",
      "Training iter #1140000:   Batch Loss = 0.196793, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.28255438804626465, Accuracy = 0.9640312194824219\n",
      "Training iter #1170000:   Batch Loss = 0.194132, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2823900878429413, Accuracy = 0.9670851826667786\n",
      "Training iter #1200000:   Batch Loss = 0.194588, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2732757031917572, Accuracy = 0.9667458534240723\n",
      "Training iter #1230000:   Batch Loss = 0.179022, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.26898932456970215, Accuracy = 0.9681031703948975\n",
      "Training iter #1260000:   Batch Loss = 0.170852, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.26047483086586, Accuracy = 0.9670851826667786\n",
      "Training iter #1290000:   Batch Loss = 0.168926, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.26352497935295105, Accuracy = 0.9647098779678345\n",
      "Training iter #1320000:   Batch Loss = 0.180752, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.264426589012146, Accuracy = 0.9643705487251282\n",
      "Optimization Finished!\n",
      "Training time is: 736.4559714794159 seconds\n",
      "FINAL RESULT: Batch Loss = 0.2672640085220337, Accuracy = 0.9619952440261841\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.461490, Accuracy = 0.1966666728258133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.42785382270813, Accuracy = 0.16830675303936005\n",
      "Training iter #30000:   Batch Loss = 1.520413, Accuracy = 0.5360000133514404\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4922184944152832, Accuracy = 0.5734645128250122\n",
      "Training iter #60000:   Batch Loss = 0.861567, Accuracy = 0.862666666507721\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8825112581253052, Accuracy = 0.8316932320594788\n",
      "Training iter #90000:   Batch Loss = 0.607922, Accuracy = 0.9200000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7034701108932495, Accuracy = 0.8717339634895325\n",
      "Training iter #120000:   Batch Loss = 0.621344, Accuracy = 0.8893333077430725\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6634459495544434, Accuracy = 0.8754665851593018\n",
      "Training iter #150000:   Batch Loss = 0.635875, Accuracy = 0.8806666731834412\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6193393468856812, Accuracy = 0.8897183537483215\n",
      "Training iter #180000:   Batch Loss = 0.592374, Accuracy = 0.9100000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5873012542724609, Accuracy = 0.9063454270362854\n",
      "Training iter #210000:   Batch Loss = 0.546411, Accuracy = 0.9193333387374878\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5718172192573547, Accuracy = 0.9056667685508728\n",
      "Training iter #240000:   Batch Loss = 0.486665, Accuracy = 0.9440000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.570969820022583, Accuracy = 0.9080420732498169\n",
      "Training iter #270000:   Batch Loss = 0.429390, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5242593288421631, Accuracy = 0.9229725003242493\n",
      "Training iter #300000:   Batch Loss = 0.431772, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5064482092857361, Accuracy = 0.9334917068481445\n",
      "Training iter #330000:   Batch Loss = 0.444605, Accuracy = 0.9539999961853027\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49559473991394043, Accuracy = 0.9389209151268005\n",
      "Training iter #360000:   Batch Loss = 0.415657, Accuracy = 0.9639999866485596\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4936879873275757, Accuracy = 0.9392602443695068\n",
      "Training iter #390000:   Batch Loss = 0.421040, Accuracy = 0.9633333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4947238862514496, Accuracy = 0.9307770729064941\n",
      "Training iter #420000:   Batch Loss = 0.370945, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47140800952911377, Accuracy = 0.9436715245246887\n",
      "Training iter #450000:   Batch Loss = 0.345669, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4796122610569, Accuracy = 0.9382422566413879\n",
      "Training iter #480000:   Batch Loss = 0.394385, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4916902780532837, Accuracy = 0.9365456104278564\n",
      "Training iter #510000:   Batch Loss = 0.352014, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45850497484207153, Accuracy = 0.9450288414955139\n",
      "Training iter #540000:   Batch Loss = 0.325086, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4855421185493469, Accuracy = 0.9389209151268005\n",
      "Training iter #570000:   Batch Loss = 0.329886, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5256531238555908, Accuracy = 0.9331523776054382\n",
      "Training iter #600000:   Batch Loss = 0.309965, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46911609172821045, Accuracy = 0.9402782320976257\n",
      "Training iter #630000:   Batch Loss = 0.346871, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49571698904037476, Accuracy = 0.9426535367965698\n",
      "Training iter #660000:   Batch Loss = 0.297337, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46809303760528564, Accuracy = 0.9453681707382202\n",
      "Training iter #690000:   Batch Loss = 0.292090, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.484669029712677, Accuracy = 0.9419748783111572\n",
      "Training iter #720000:   Batch Loss = 0.280658, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4671934247016907, Accuracy = 0.9409568905830383\n",
      "Training iter #750000:   Batch Loss = 0.286619, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5025038719177246, Accuracy = 0.9365456104278564\n",
      "Training iter #780000:   Batch Loss = 0.270893, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4364980459213257, Accuracy = 0.9460468292236328\n",
      "Training iter #810000:   Batch Loss = 0.261059, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45001453161239624, Accuracy = 0.9436715245246887\n",
      "Training iter #840000:   Batch Loss = 0.268467, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45189815759658813, Accuracy = 0.9419748783111572\n",
      "Training iter #870000:   Batch Loss = 0.258724, Accuracy = 0.9926666617393494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4212356209754944, Accuracy = 0.9491007924079895\n",
      "Training iter #900000:   Batch Loss = 0.268038, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45383572578430176, Accuracy = 0.9446895122528076\n",
      "Training iter #930000:   Batch Loss = 0.256712, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4435201585292816, Accuracy = 0.9429928660392761\n",
      "Training iter #960000:   Batch Loss = 0.230926, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.417975515127182, Accuracy = 0.9477434754371643\n",
      "Training iter #990000:   Batch Loss = 0.228713, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.431074857711792, Accuracy = 0.9457074999809265\n",
      "Training iter #1020000:   Batch Loss = 0.232611, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45531514286994934, Accuracy = 0.9412962198257446\n",
      "Training iter #1050000:   Batch Loss = 0.220920, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4520457684993744, Accuracy = 0.9433321952819824\n",
      "Training iter #1080000:   Batch Loss = 0.240519, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4349210262298584, Accuracy = 0.9467254877090454\n",
      "Training iter #1110000:   Batch Loss = 0.224503, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4405176341533661, Accuracy = 0.9375635981559753\n",
      "Training iter #1140000:   Batch Loss = 0.203715, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40944868326187134, Accuracy = 0.9484221339225769\n",
      "Training iter #1170000:   Batch Loss = 0.203391, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43788084387779236, Accuracy = 0.9412962198257446\n",
      "Training iter #1200000:   Batch Loss = 0.223069, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43598008155822754, Accuracy = 0.9216151833534241\n",
      "Training iter #1230000:   Batch Loss = 0.212398, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.365150511264801, Accuracy = 0.9450288414955139\n",
      "Training iter #1260000:   Batch Loss = 0.212311, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3516356647014618, Accuracy = 0.9565659761428833\n",
      "Training iter #1290000:   Batch Loss = 0.200924, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35240983963012695, Accuracy = 0.9548693299293518\n",
      "Training iter #1320000:   Batch Loss = 0.186230, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3482641875743866, Accuracy = 0.9572446346282959\n",
      "Training iter #1350000:   Batch Loss = 0.189864, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3397025465965271, Accuracy = 0.9535120725631714\n",
      "Training iter #1380000:   Batch Loss = 0.186910, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33844172954559326, Accuracy = 0.9572446346282959\n",
      "Training iter #1410000:   Batch Loss = 0.180168, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3602084517478943, Accuracy = 0.9457074999809265\n",
      "Training iter #1440000:   Batch Loss = 0.187530, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34336596727371216, Accuracy = 0.9494401216506958\n",
      "Training iter #1470000:   Batch Loss = 0.180189, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33420586585998535, Accuracy = 0.9552086591720581\n",
      "Training iter #1500000:   Batch Loss = 0.166316, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3429777920246124, Accuracy = 0.9497794508934021\n",
      "Training iter #1530000:   Batch Loss = 0.164761, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33869385719299316, Accuracy = 0.9514760971069336\n",
      "Optimization Finished!\n",
      "Training time is: 856.327957868576 seconds\n",
      "FINAL RESULT: Batch Loss = 0.3360779285430908, Accuracy = 0.9487614631652832\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.750146, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.6249475479125977, Accuracy = 0.18221920728683472\n",
      "Training iter #30000:   Batch Loss = 1.773418, Accuracy = 0.39133334159851074\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7157468795776367, Accuracy = 0.41194435954093933\n",
      "Training iter #60000:   Batch Loss = 0.956029, Accuracy = 0.7613333463668823\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9109482765197754, Accuracy = 0.8011537194252014\n",
      "Training iter #90000:   Batch Loss = 0.762144, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7916393280029297, Accuracy = 0.8357651829719543\n",
      "Training iter #120000:   Batch Loss = 0.631280, Accuracy = 0.9066666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6845918893814087, Accuracy = 0.8829317688941956\n",
      "Training iter #150000:   Batch Loss = 0.589459, Accuracy = 0.9213333129882812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.632813036441803, Accuracy = 0.8995589017868042\n",
      "Training iter #180000:   Batch Loss = 0.569099, Accuracy = 0.9206666946411133\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5949629545211792, Accuracy = 0.9100780487060547\n",
      "Training iter #210000:   Batch Loss = 0.585684, Accuracy = 0.9153333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5936224460601807, Accuracy = 0.9097387194633484\n",
      "Training iter #240000:   Batch Loss = 0.532625, Accuracy = 0.9399999976158142\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5334048867225647, Accuracy = 0.9304377436637878\n",
      "Training iter #270000:   Batch Loss = 0.505468, Accuracy = 0.95333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5135293006896973, Accuracy = 0.9412962198257446\n",
      "Training iter #300000:   Batch Loss = 0.495388, Accuracy = 0.9553333520889282\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5216774940490723, Accuracy = 0.937224268913269\n",
      "Training iter #330000:   Batch Loss = 0.478373, Accuracy = 0.9546666741371155\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5099399089813232, Accuracy = 0.9365456104278564\n",
      "Training iter #360000:   Batch Loss = 0.423332, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47661879658699036, Accuracy = 0.950797438621521\n",
      "Training iter #390000:   Batch Loss = 0.412513, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.47986164689064026, Accuracy = 0.950797438621521\n",
      "Training iter #420000:   Batch Loss = 0.403221, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4666562080383301, Accuracy = 0.9531727433204651\n",
      "Training iter #450000:   Batch Loss = 0.397163, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4618825316429138, Accuracy = 0.954190731048584\n",
      "Training iter #480000:   Batch Loss = 0.397370, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45087939500808716, Accuracy = 0.9545300602912903\n",
      "Training iter #510000:   Batch Loss = 0.383242, Accuracy = 0.9766666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4592490494251251, Accuracy = 0.9501187801361084\n",
      "Training iter #540000:   Batch Loss = 0.372846, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4418673813343048, Accuracy = 0.9538514018058777\n",
      "Training iter #570000:   Batch Loss = 0.358714, Accuracy = 0.981333315372467\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4301360547542572, Accuracy = 0.9552086591720581\n",
      "Training iter #600000:   Batch Loss = 0.343614, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4715162515640259, Accuracy = 0.935188353061676\n",
      "Training iter #630000:   Batch Loss = 0.346098, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42017853260040283, Accuracy = 0.9565659761428833\n",
      "Training iter #660000:   Batch Loss = 0.347959, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.45448482036590576, Accuracy = 0.9392602443695068\n",
      "Training iter #690000:   Batch Loss = 0.328746, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4249243140220642, Accuracy = 0.9477434754371643\n",
      "Training iter #720000:   Batch Loss = 0.306124, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4210166335105896, Accuracy = 0.9494401216506958\n",
      "Training iter #750000:   Batch Loss = 0.299714, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4489654004573822, Accuracy = 0.9392602443695068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #780000:   Batch Loss = 0.302345, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41527897119522095, Accuracy = 0.9558873176574707\n",
      "Training iter #810000:   Batch Loss = 0.299554, Accuracy = 0.984000027179718\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3994405269622803, Accuracy = 0.9535120725631714\n",
      "Training iter #840000:   Batch Loss = 0.271092, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.373835951089859, Accuracy = 0.9609772562980652\n",
      "Training iter #870000:   Batch Loss = 0.279405, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41698962450027466, Accuracy = 0.9450288414955139\n",
      "Training iter #900000:   Batch Loss = 0.279187, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3811950385570526, Accuracy = 0.9511367678642273\n",
      "Training iter #930000:   Batch Loss = 0.280778, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3683550953865051, Accuracy = 0.9558873176574707\n",
      "Training iter #960000:   Batch Loss = 0.256831, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3598518371582031, Accuracy = 0.9579232931137085\n",
      "Training iter #990000:   Batch Loss = 0.251714, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35894104838371277, Accuracy = 0.9582626223564148\n",
      "Training iter #1020000:   Batch Loss = 0.266752, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3829740881919861, Accuracy = 0.9457074999809265\n",
      "Training iter #1050000:   Batch Loss = 0.244733, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35861191153526306, Accuracy = 0.9538514018058777\n",
      "Training iter #1080000:   Batch Loss = 0.244369, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3471171259880066, Accuracy = 0.95961993932724\n",
      "Training iter #1110000:   Batch Loss = 0.243921, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3599426746368408, Accuracy = 0.9521547555923462\n",
      "Training iter #1140000:   Batch Loss = 0.222695, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3368179500102997, Accuracy = 0.9555479884147644\n",
      "Training iter #1170000:   Batch Loss = 0.229396, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35668158531188965, Accuracy = 0.9528334140777588\n",
      "Training iter #1200000:   Batch Loss = 0.233507, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3563190698623657, Accuracy = 0.9504581093788147\n",
      "Training iter #1230000:   Batch Loss = 0.228444, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3376484215259552, Accuracy = 0.9524940848350525\n",
      "Training iter #1260000:   Batch Loss = 0.215379, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34487634897232056, Accuracy = 0.9487614631652832\n",
      "Training iter #1290000:   Batch Loss = 0.232365, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3477635085582733, Accuracy = 0.9504581093788147\n",
      "Training iter #1320000:   Batch Loss = 0.206965, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30229824781417847, Accuracy = 0.9613165855407715\n",
      "Training iter #1350000:   Batch Loss = 0.199487, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3271220922470093, Accuracy = 0.9518154263496399\n",
      "Training iter #1380000:   Batch Loss = 0.204623, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3147605061531067, Accuracy = 0.9579232931137085\n",
      "Training iter #1410000:   Batch Loss = 0.203675, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3298150897026062, Accuracy = 0.9511367678642273\n",
      "Training iter #1440000:   Batch Loss = 0.235017, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30812638998031616, Accuracy = 0.9572446346282959\n",
      "Training iter #1470000:   Batch Loss = 0.182388, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.32832181453704834, Accuracy = 0.9480828046798706\n",
      "Training iter #1500000:   Batch Loss = 0.182574, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3167279064655304, Accuracy = 0.9491007924079895\n",
      "Training iter #1530000:   Batch Loss = 0.189035, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31171301007270813, Accuracy = 0.9504581093788147\n",
      "Training iter #1560000:   Batch Loss = 0.198330, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3068303167819977, Accuracy = 0.9501187801361084\n",
      "Training iter #1590000:   Batch Loss = 0.175075, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31663909554481506, Accuracy = 0.9467254877090454\n",
      "Training iter #1620000:   Batch Loss = 0.166747, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3011532723903656, Accuracy = 0.9545300602912903\n",
      "Training iter #1650000:   Batch Loss = 0.174267, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.37878209352493286, Accuracy = 0.9314557313919067\n",
      "Training iter #1680000:   Batch Loss = 0.186946, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3586651682853699, Accuracy = 0.9341703653335571\n",
      "Training iter #1710000:   Batch Loss = 0.182483, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3128766119480133, Accuracy = 0.947404146194458\n",
      "Training iter #1740000:   Batch Loss = 0.164630, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2923792600631714, Accuracy = 0.9535120725631714\n",
      "Optimization Finished!\n",
      "Training time is: 976.7088627815247 seconds\n",
      "FINAL RESULT: Batch Loss = 0.31044039130210876, Accuracy = 0.9429928660392761\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.965874, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.7699217796325684, Accuracy = 0.16661010682582855\n",
      "Training iter #30000:   Batch Loss = 1.730478, Accuracy = 0.5099999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.6552218198776245, Accuracy = 0.6586359143257141\n",
      "Training iter #60000:   Batch Loss = 0.915091, Accuracy = 0.7953333258628845\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8242746591567993, Accuracy = 0.8313539028167725\n",
      "Training iter #90000:   Batch Loss = 0.668945, Accuracy = 0.8700000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7178199887275696, Accuracy = 0.8540889024734497\n",
      "Training iter #120000:   Batch Loss = 0.627514, Accuracy = 0.890666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6418478488922119, Accuracy = 0.8781812191009521\n",
      "Training iter #150000:   Batch Loss = 0.562596, Accuracy = 0.9173333048820496\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6047812104225159, Accuracy = 0.891414999961853\n",
      "Training iter #180000:   Batch Loss = 0.503636, Accuracy = 0.9473333358764648\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5620248317718506, Accuracy = 0.9141499996185303\n",
      "Training iter #210000:   Batch Loss = 0.484523, Accuracy = 0.9433333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5319761633872986, Accuracy = 0.9222938418388367\n",
      "Training iter #240000:   Batch Loss = 0.471542, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5663684606552124, Accuracy = 0.910417377948761\n",
      "Training iter #270000:   Batch Loss = 0.477550, Accuracy = 0.9559999704360962\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5361414551734924, Accuracy = 0.917203962802887\n",
      "Training iter #300000:   Batch Loss = 0.407836, Accuracy = 0.9666666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5149638652801514, Accuracy = 0.9212758541107178\n",
      "Training iter #330000:   Batch Loss = 0.387854, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5025509595870972, Accuracy = 0.9277231097221375\n",
      "Training iter #360000:   Batch Loss = 0.397804, Accuracy = 0.9673333168029785\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49721086025238037, Accuracy = 0.92840176820755\n",
      "Training iter #390000:   Batch Loss = 0.366232, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5014705657958984, Accuracy = 0.9246691465377808\n",
      "Training iter #420000:   Batch Loss = 0.359574, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48685890436172485, Accuracy = 0.9311164021492004\n",
      "Training iter #450000:   Batch Loss = 0.341035, Accuracy = 0.9906666874885559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5148635506629944, Accuracy = 0.9236511588096619\n",
      "Training iter #480000:   Batch Loss = 0.338446, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4993925094604492, Accuracy = 0.9256871342658997\n",
      "Training iter #510000:   Batch Loss = 0.331801, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46751710772514343, Accuracy = 0.9389209151268005\n",
      "Training iter #540000:   Batch Loss = 0.326178, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.48600903153419495, Accuracy = 0.9345096945762634\n",
      "Training iter #570000:   Batch Loss = 0.332226, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4925338327884674, Accuracy = 0.9338310360908508\n",
      "Training iter #600000:   Batch Loss = 0.309586, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4438784122467041, Accuracy = 0.9470648169517517\n",
      "Training iter #630000:   Batch Loss = 0.330035, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4351797103881836, Accuracy = 0.9453681707382202\n",
      "Training iter #660000:   Batch Loss = 0.287633, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4355021119117737, Accuracy = 0.944010853767395\n",
      "Training iter #690000:   Batch Loss = 0.273037, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4251062870025635, Accuracy = 0.9491007924079895\n",
      "Training iter #720000:   Batch Loss = 0.264316, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42130911350250244, Accuracy = 0.950797438621521\n",
      "Training iter #750000:   Batch Loss = 0.267108, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.429097443819046, Accuracy = 0.9480828046798706\n",
      "Training iter #780000:   Batch Loss = 0.263191, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43098533153533936, Accuracy = 0.944010853767395\n",
      "Training iter #810000:   Batch Loss = 0.263007, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5187118649482727, Accuracy = 0.9107567071914673\n",
      "Training iter #840000:   Batch Loss = 0.247789, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4828760325908661, Accuracy = 0.9307770729064941\n",
      "Training iter #870000:   Batch Loss = 0.251479, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.38842761516571045, Accuracy = 0.9487614631652832\n",
      "Training iter #900000:   Batch Loss = 0.236959, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42146894335746765, Accuracy = 0.9419748783111572\n",
      "Training iter #930000:   Batch Loss = 0.237569, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3941432237625122, Accuracy = 0.9501187801361084\n",
      "Training iter #960000:   Batch Loss = 0.239828, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4139333665370941, Accuracy = 0.9419748783111572\n",
      "Training iter #990000:   Batch Loss = 0.267991, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4241277575492859, Accuracy = 0.9429928660392761\n",
      "Training iter #1020000:   Batch Loss = 0.216419, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3953322470188141, Accuracy = 0.9423142075538635\n",
      "Training iter #1050000:   Batch Loss = 0.215822, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41859495639801025, Accuracy = 0.9402782320976257\n",
      "Training iter #1080000:   Batch Loss = 0.204924, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4274106025695801, Accuracy = 0.940617561340332\n",
      "Training iter #1110000:   Batch Loss = 0.211074, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34173429012298584, Accuracy = 0.9531727433204651\n",
      "Training iter #1140000:   Batch Loss = 0.206686, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3819809556007385, Accuracy = 0.9433321952819824\n",
      "Training iter #1170000:   Batch Loss = 0.223604, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.39396780729293823, Accuracy = 0.937224268913269\n",
      "Training iter #1200000:   Batch Loss = 0.211343, Accuracy = 0.9853333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3822760581970215, Accuracy = 0.9443501830101013\n",
      "Training iter #1230000:   Batch Loss = 0.192304, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3962322473526001, Accuracy = 0.9409568905830383\n",
      "Training iter #1260000:   Batch Loss = 0.186511, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4114753305912018, Accuracy = 0.937224268913269\n",
      "Training iter #1290000:   Batch Loss = 0.181882, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4065401554107666, Accuracy = 0.9368849396705627\n",
      "Training iter #1320000:   Batch Loss = 0.187615, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4312041997909546, Accuracy = 0.9345096945762634\n",
      "Training iter #1350000:   Batch Loss = 0.209836, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3997233211994171, Accuracy = 0.935188353061676\n",
      "Training iter #1380000:   Batch Loss = 0.229489, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46079379320144653, Accuracy = 0.9185612201690674\n",
      "Training iter #1410000:   Batch Loss = 0.185030, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3726978003978729, Accuracy = 0.940617561340332\n",
      "Training iter #1440000:   Batch Loss = 0.201407, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3570554256439209, Accuracy = 0.9467254877090454\n",
      "Training iter #1470000:   Batch Loss = 0.173694, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.36243534088134766, Accuracy = 0.9433321952819824\n",
      "Training iter #1500000:   Batch Loss = 0.292525, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3560580611228943, Accuracy = 0.944010853767395\n",
      "Training iter #1530000:   Batch Loss = 0.164714, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35211342573165894, Accuracy = 0.9433321952819824\n",
      "Training iter #1560000:   Batch Loss = 0.160897, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.348459929227829, Accuracy = 0.9446895122528076\n",
      "Training iter #1590000:   Batch Loss = 0.154089, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.32231828570365906, Accuracy = 0.9528334140777588\n",
      "Training iter #1620000:   Batch Loss = 0.158189, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.37072083353996277, Accuracy = 0.9392602443695068\n",
      "Training iter #1650000:   Batch Loss = 0.160448, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3397243618965149, Accuracy = 0.9419748783111572\n",
      "Training iter #1680000:   Batch Loss = 0.149221, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3406345248222351, Accuracy = 0.9426535367965698\n",
      "Training iter #1710000:   Batch Loss = 0.146883, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3302322328090668, Accuracy = 0.9443501830101013\n",
      "Training iter #1740000:   Batch Loss = 0.149817, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.35049188137054443, Accuracy = 0.9385815858840942\n",
      "Training iter #1770000:   Batch Loss = 0.156734, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3494940996170044, Accuracy = 0.9426535367965698\n",
      "Training iter #1800000:   Batch Loss = 0.141518, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4162752628326416, Accuracy = 0.9314557313919067\n",
      "Training iter #1830000:   Batch Loss = 0.458423, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43268144130706787, Accuracy = 0.884628415107727\n",
      "Training iter #1860000:   Batch Loss = 0.177716, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.32231754064559937, Accuracy = 0.9412962198257446\n",
      "Training iter #1890000:   Batch Loss = 0.178264, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3238492012023926, Accuracy = 0.9463861584663391\n",
      "Training iter #1920000:   Batch Loss = 0.161592, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33921879529953003, Accuracy = 0.9450288414955139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1950000:   Batch Loss = 0.152586, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3508039712905884, Accuracy = 0.9429928660392761\n",
      "Training iter #1980000:   Batch Loss = 0.150254, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34123629331588745, Accuracy = 0.9463861584663391\n",
      "Optimization Finished!\n",
      "Training time is: 1089.9778270721436 seconds\n",
      "FINAL RESULT: Batch Loss = 0.34025001525878906, Accuracy = 0.9460468292236328\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.548447, Accuracy = 0.15733332931995392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.396347999572754, Accuracy = 0.15982355177402496\n",
      "Training iter #30000:   Batch Loss = 1.432490, Accuracy = 0.6113333106040955\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4416236877441406, Accuracy = 0.6762809753417969\n",
      "Training iter #60000:   Batch Loss = 0.959783, Accuracy = 0.7353333234786987\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8630799651145935, Accuracy = 0.8113335371017456\n",
      "Training iter #90000:   Batch Loss = 0.736357, Accuracy = 0.8573333621025085\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7166934013366699, Accuracy = 0.8676620125770569\n",
      "Training iter #120000:   Batch Loss = 0.629246, Accuracy = 0.906000018119812\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6750587224960327, Accuracy = 0.8795385360717773\n",
      "Training iter #150000:   Batch Loss = 0.581997, Accuracy = 0.9240000247955322\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6384168267250061, Accuracy = 0.891414999961853\n",
      "Training iter #180000:   Batch Loss = 0.575013, Accuracy = 0.9313333630561829\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6178402900695801, Accuracy = 0.9009161591529846\n",
      "Training iter #210000:   Batch Loss = 0.614890, Accuracy = 0.8939999938011169\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6100484132766724, Accuracy = 0.9005768299102783\n",
      "Training iter #240000:   Batch Loss = 0.548471, Accuracy = 0.9259999990463257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5561290383338928, Accuracy = 0.9212758541107178\n",
      "Training iter #270000:   Batch Loss = 0.504360, Accuracy = 0.9506666660308838\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5336883664131165, Accuracy = 0.9297590851783752\n",
      "Training iter #300000:   Batch Loss = 0.459364, Accuracy = 0.9653333425521851\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5192896127700806, Accuracy = 0.931795060634613\n",
      "Training iter #330000:   Batch Loss = 0.429564, Accuracy = 0.9739999771118164\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5106406211853027, Accuracy = 0.9348490238189697\n",
      "Training iter #360000:   Batch Loss = 0.423446, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4897555708885193, Accuracy = 0.9460468292236328\n",
      "Training iter #390000:   Batch Loss = 0.406253, Accuracy = 0.972000002861023\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5007798075675964, Accuracy = 0.9450288414955139\n",
      "Training iter #420000:   Batch Loss = 0.391983, Accuracy = 0.9746666550636292\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4738602340221405, Accuracy = 0.9491007924079895\n",
      "Training iter #450000:   Batch Loss = 0.388482, Accuracy = 0.9693333506584167\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4732598662376404, Accuracy = 0.9491007924079895\n",
      "Training iter #480000:   Batch Loss = 0.371482, Accuracy = 0.9726666808128357\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4599628150463104, Accuracy = 0.954190731048584\n",
      "Training iter #510000:   Batch Loss = 0.348197, Accuracy = 0.9773333072662354\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.43886157870292664, Accuracy = 0.9569053053855896\n",
      "Training iter #540000:   Batch Loss = 0.345531, Accuracy = 0.9800000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4352656304836273, Accuracy = 0.954190731048584\n",
      "Training iter #570000:   Batch Loss = 0.343364, Accuracy = 0.9793333411216736\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.42981454730033875, Accuracy = 0.956226646900177\n",
      "Training iter #600000:   Batch Loss = 0.354168, Accuracy = 0.9753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4214652180671692, Accuracy = 0.9586019515991211\n",
      "Training iter #630000:   Batch Loss = 0.298461, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4192681908607483, Accuracy = 0.9575839638710022\n",
      "Training iter #660000:   Batch Loss = 0.293827, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41929715871810913, Accuracy = 0.954190731048584\n",
      "Training iter #690000:   Batch Loss = 0.309521, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4437599778175354, Accuracy = 0.9487614631652832\n",
      "Training iter #720000:   Batch Loss = 0.279981, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40622764825820923, Accuracy = 0.9538514018058777\n",
      "Training iter #750000:   Batch Loss = 0.276817, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3935513496398926, Accuracy = 0.9606379270553589\n",
      "Training iter #780000:   Batch Loss = 0.271517, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.38787633180618286, Accuracy = 0.9552086591720581\n",
      "Training iter #810000:   Batch Loss = 0.278652, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.38909846544265747, Accuracy = 0.9552086591720581\n",
      "Training iter #840000:   Batch Loss = 0.262107, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3996105194091797, Accuracy = 0.9467254877090454\n",
      "Training iter #870000:   Batch Loss = 0.257244, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40542447566986084, Accuracy = 0.9511367678642273\n",
      "Training iter #900000:   Batch Loss = 0.260028, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4235159754753113, Accuracy = 0.9484221339225769\n",
      "Training iter #930000:   Batch Loss = 0.257222, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3428342640399933, Accuracy = 0.9636918902397156\n",
      "Training iter #960000:   Batch Loss = 0.246256, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3512881398200989, Accuracy = 0.9569053053855896\n",
      "Training iter #990000:   Batch Loss = 0.226775, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.38098594546318054, Accuracy = 0.9453681707382202\n",
      "Training iter #1020000:   Batch Loss = 0.234968, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3501157760620117, Accuracy = 0.956226646900177\n",
      "Training iter #1050000:   Batch Loss = 0.221711, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33261221647262573, Accuracy = 0.9623345732688904\n",
      "Training iter #1080000:   Batch Loss = 0.224928, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.32804518938064575, Accuracy = 0.9626739025115967\n",
      "Training iter #1110000:   Batch Loss = 0.215808, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31523823738098145, Accuracy = 0.966406524181366\n",
      "Training iter #1140000:   Batch Loss = 0.211563, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.322659969329834, Accuracy = 0.9643705487251282\n",
      "Training iter #1170000:   Batch Loss = 0.205935, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3461456000804901, Accuracy = 0.9487614631652832\n",
      "Training iter #1200000:   Batch Loss = 0.209925, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3466363847255707, Accuracy = 0.9491007924079895\n",
      "Training iter #1230000:   Batch Loss = 0.194140, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33005446195602417, Accuracy = 0.9535120725631714\n",
      "Training iter #1260000:   Batch Loss = 0.194180, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.36667609214782715, Accuracy = 0.9460468292236328\n",
      "Training iter #1290000:   Batch Loss = 0.208107, Accuracy = 0.987333357334137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31825482845306396, Accuracy = 0.9565659761428833\n",
      "Training iter #1320000:   Batch Loss = 0.214848, Accuracy = 0.984666645526886\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31908562779426575, Accuracy = 0.9586019515991211\n",
      "Training iter #1350000:   Batch Loss = 0.195453, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3029631972312927, Accuracy = 0.9613165855407715\n",
      "Training iter #1380000:   Batch Loss = 0.215309, Accuracy = 0.9793333411216736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33225584030151367, Accuracy = 0.9480828046798706\n",
      "Training iter #1410000:   Batch Loss = 0.188520, Accuracy = 0.9953333139419556\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34259462356567383, Accuracy = 0.9497794508934021\n",
      "Training iter #1440000:   Batch Loss = 0.183997, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33674541115760803, Accuracy = 0.9548693299293518\n",
      "Training iter #1470000:   Batch Loss = 0.204469, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3330428898334503, Accuracy = 0.9602985978126526\n",
      "Training iter #1500000:   Batch Loss = 0.177326, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31635618209838867, Accuracy = 0.9609772562980652\n",
      "Training iter #1530000:   Batch Loss = 0.250660, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.34285205602645874, Accuracy = 0.9545300602912903\n",
      "Training iter #1560000:   Batch Loss = 0.170948, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3031715452671051, Accuracy = 0.9619952440261841\n",
      "Training iter #1590000:   Batch Loss = 0.167114, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2921241521835327, Accuracy = 0.9619952440261841\n",
      "Training iter #1620000:   Batch Loss = 0.158143, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30309367179870605, Accuracy = 0.9572446346282959\n",
      "Training iter #1650000:   Batch Loss = 0.158022, Accuracy = 0.9973333477973938\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31647390127182007, Accuracy = 0.956226646900177\n",
      "Training iter #1680000:   Batch Loss = 0.176795, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2815718650817871, Accuracy = 0.9681031703948975\n",
      "Training iter #1710000:   Batch Loss = 0.162324, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2949168086051941, Accuracy = 0.9619952440261841\n",
      "Training iter #1740000:   Batch Loss = 0.145906, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2870332598686218, Accuracy = 0.9633525609970093\n",
      "Training iter #1770000:   Batch Loss = 0.182437, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30383458733558655, Accuracy = 0.9514760971069336\n",
      "Training iter #1800000:   Batch Loss = 0.145163, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.29593002796173096, Accuracy = 0.956226646900177\n",
      "Training iter #1830000:   Batch Loss = 0.175449, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2838340997695923, Accuracy = 0.9538514018058777\n",
      "Training iter #1860000:   Batch Loss = 0.188985, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.28551018238067627, Accuracy = 0.95961993932724\n",
      "Training iter #1890000:   Batch Loss = 0.162410, Accuracy = 0.9926666617393494\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2933841943740845, Accuracy = 0.9565659761428833\n",
      "Training iter #1920000:   Batch Loss = 0.186066, Accuracy = 0.9806666374206543\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2955801486968994, Accuracy = 0.9575839638710022\n",
      "Training iter #1950000:   Batch Loss = 0.148198, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.28188633918762207, Accuracy = 0.9569053053855896\n",
      "Training iter #1980000:   Batch Loss = 0.137913, Accuracy = 0.9986666440963745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2808082401752472, Accuracy = 0.9575839638710022\n",
      "Training iter #2010000:   Batch Loss = 0.142444, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2640984356403351, Accuracy = 0.9613165855407715\n",
      "Training iter #2040000:   Batch Loss = 0.152468, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2459065318107605, Accuracy = 0.9667458534240723\n",
      "Training iter #2070000:   Batch Loss = 0.148250, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.26170602440834045, Accuracy = 0.9643705487251282\n",
      "Training iter #2100000:   Batch Loss = 0.152501, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2610219717025757, Accuracy = 0.9640312194824219\n",
      "Training iter #2130000:   Batch Loss = 0.129478, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2854990065097809, Accuracy = 0.9558873176574707\n",
      "Training iter #2160000:   Batch Loss = 0.126792, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2785065770149231, Accuracy = 0.9545300602912903\n",
      "Training iter #2190000:   Batch Loss = 0.133434, Accuracy = 0.9980000257492065\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.30416953563690186, Accuracy = 0.9528334140777588\n",
      "Optimization Finished!\n",
      "Training time is: 1212.9846336841583 seconds\n",
      "FINAL RESULT: Batch Loss = 0.2671588957309723, Accuracy = 0.954190731048584\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[126.32624459266663, 249.3763711452484, 370.1370780467987, 492.6169419288635, 614.8972370624542, 736.4559714794159, 856.327957868576, 976.7088627815247, 1089.9778270721436, 1212.9846336841583]\n",
      "[0.78588396, 0.8985409, 0.9165253, 0.9107567, 0.94401085, 0.96199524, 0.94876146, 0.94299287, 0.9460468, 0.95419073]\n",
      "[1.239021, 0.64888865, 0.6129117, 0.5350266, 0.3383646, 0.267264, 0.33607793, 0.3104404, 0.34025002, 0.2671589]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 of End-to-end training with the CNNLSTM model model 3.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset under same number of training iterations.\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN LSTM model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def cnn_LSTM_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    conv2 = tf.transpose(conv2, [1, 0, 2])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, conv2, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 30000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = cnn_LSTM_net(x,weights,biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.596522, Accuracy = 0.14533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.5457639694213867, Accuracy = 0.1425178200006485\n",
      "Training iter #300000:   Batch Loss = 0.500984, Accuracy = 0.9326666593551636\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5463629961013794, Accuracy = 0.9155073165893555\n",
      "Training iter #600000:   Batch Loss = 0.334970, Accuracy = 0.9826666712760925\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.467978298664093, Accuracy = 0.9477434754371643\n",
      "Training iter #900000:   Batch Loss = 0.257485, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.41076499223709106, Accuracy = 0.9521547555923462\n",
      "Training iter #1200000:   Batch Loss = 0.219698, Accuracy = 0.9860000014305115\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.390073299407959, Accuracy = 0.9446895122528076\n",
      "Training iter #1500000:   Batch Loss = 0.184680, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3629838228225708, Accuracy = 0.9399389028549194\n",
      "Training iter #1800000:   Batch Loss = 0.144679, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.31475287675857544, Accuracy = 0.9528334140777588\n",
      "Training iter #2100000:   Batch Loss = 0.155715, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3004613518714905, Accuracy = 0.9477434754371643\n",
      "Training iter #2400000:   Batch Loss = 0.172630, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3313223123550415, Accuracy = 0.9426535367965698\n",
      "Training iter #2700000:   Batch Loss = 0.120562, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.29019540548324585, Accuracy = 0.944010853767395\n",
      "Optimization Finished!\n",
      "Training time is: 2790.304626941681 seconds\n",
      "FINAL RESULT: Batch Loss = 0.27040907740592957, Accuracy = 0.956226646900177\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment 2 Part 1 of End-to-end training with the CNNLSTM model model 3.\n",
    "# Finding the optimal number of iterations for Model 3\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN LSTM model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def cnn_LSTM_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu) \n",
    "    conv2 = tf.transpose(conv2, [1, 0, 2])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, conv2, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 400  # Loop 400 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = cnn_LSTM_net(x,weights,biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(np.arange(0,len(train_losses),1), train_losses, 'r', np.arange(0,len(test_losses),1), test_losses, 'b')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(np.arange(0,len(train_accuracies),1), train_accuracies, 'r', np.arange(0,len(test_accuracies),1), test_accuracies, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save above results for manuscript plotting later.\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[train_accuracies[i], test_accuracies[i],train_losses[i],test_losses[i]] for i in range(0,len(train_accuracies))])\n",
    "sheet.save_as(\"CNN_LSTM_Training_ACC_LOSS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(735, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.510424, Accuracy = 0.20399999618530273\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4413578510284424, Accuracy = 0.16830675303936005\n",
      "Optimization Finished!\n",
      "Training time is: 86.01405668258667 seconds\n",
      "FINAL RESULT: Batch Loss = 0.940360963344574, Accuracy = 0.7964031100273132\n",
      "(7352, 1)\n",
      "(1470, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.850456, Accuracy = 0.15333333611488342\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.746682643890381, Accuracy = 0.15982355177402496\n",
      "Optimization Finished!\n",
      "Training time is: 167.23733615875244 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7051725387573242, Accuracy = 0.8758059144020081\n",
      "(7352, 1)\n",
      "(2206, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.488543, Accuracy = 0.1653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.379026174545288, Accuracy = 0.18221920728683472\n",
      "Training iter #300000:   Batch Loss = 0.392660, Accuracy = 0.9833333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7733074426651001, Accuracy = 0.8870037198066711\n",
      "Optimization Finished!\n",
      "Training time is: 250.78299975395203 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7165746688842773, Accuracy = 0.8995589017868042\n",
      "(7352, 1)\n",
      "(2941, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.485046, Accuracy = 0.1860000044107437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.452120542526245, Accuracy = 0.18086189031600952\n",
      "Training iter #300000:   Batch Loss = 0.368870, Accuracy = 0.9779999852180481\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7639238238334656, Accuracy = 0.9043094515800476\n",
      "Optimization Finished!\n",
      "Training time is: 342.0062355995178 seconds\n",
      "FINAL RESULT: Batch Loss = 0.7529195547103882, Accuracy = 0.8890396952629089\n",
      "(7352, 1)\n",
      "(3676, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 3.152811, Accuracy = 0.15733332931995392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 3.034594774246216, Accuracy = 0.19681032001972198\n",
      "Training iter #300000:   Batch Loss = 0.408197, Accuracy = 0.9900000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5648451447486877, Accuracy = 0.9348490238189697\n",
      "Training iter #600000:   Batch Loss = 0.324766, Accuracy = 0.9906666874885559\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4971059560775757, Accuracy = 0.9429928660392761\n",
      "Optimization Finished!\n",
      "Training time is: 422.47657561302185 seconds\n",
      "FINAL RESULT: Batch Loss = 0.45579731464385986, Accuracy = 0.9460468292236328\n",
      "(7352, 1)\n",
      "(4411, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.512673, Accuracy = 0.15733332931995392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3903088569641113, Accuracy = 0.15982355177402496\n",
      "Training iter #300000:   Batch Loss = 0.449751, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4630783200263977, Accuracy = 0.9602985978126526\n",
      "Training iter #600000:   Batch Loss = 0.356984, Accuracy = 0.9713333249092102\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.40863484144210815, Accuracy = 0.9670851826667786\n",
      "Optimization Finished!\n",
      "Training time is: 588.8813662528992 seconds\n",
      "FINAL RESULT: Batch Loss = 0.35652047395706177, Accuracy = 0.9619952440261841\n",
      "(7352, 1)\n",
      "(5146, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.345883, Accuracy = 0.15733332931995392\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.286691665649414, Accuracy = 0.15982355177402496\n",
      "Training iter #300000:   Batch Loss = 0.416089, Accuracy = 0.9700000286102295\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5385448336601257, Accuracy = 0.9358670115470886\n",
      "Training iter #600000:   Batch Loss = 0.280896, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4758915305137634, Accuracy = 0.9484221339225769\n",
      "Training iter #900000:   Batch Loss = 0.246021, Accuracy = 0.9893333315849304\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4446337819099426, Accuracy = 0.9480828046798706\n",
      "Optimization Finished!\n",
      "Training time is: 665.489280462265 seconds\n",
      "FINAL RESULT: Batch Loss = 0.4367775619029999, Accuracy = 0.9457074999809265\n",
      "(7352, 1)\n",
      "(5882, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.689781, Accuracy = 0.14533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.679107427597046, Accuracy = 0.1425178200006485\n",
      "Training iter #300000:   Batch Loss = 0.413747, Accuracy = 0.9760000109672546\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5416560173034668, Accuracy = 0.9270444512367249\n",
      "Training iter #600000:   Batch Loss = 0.302557, Accuracy = 0.9879999756813049\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.49511921405792236, Accuracy = 0.9263657927513123\n",
      "Training iter #900000:   Batch Loss = 0.235594, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.39924871921539307, Accuracy = 0.9423142075538635\n",
      "Optimization Finished!\n",
      "Training time is: 684.1157515048981 seconds\n",
      "FINAL RESULT: Batch Loss = 0.37667298316955566, Accuracy = 0.9409568905830383\n",
      "(7352, 1)\n",
      "(6617, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.529468, Accuracy = 0.14533333480358124\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.4731292724609375, Accuracy = 0.1425178200006485\n",
      "Training iter #300000:   Batch Loss = 0.402153, Accuracy = 0.9733333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4825781583786011, Accuracy = 0.944010853767395\n",
      "Training iter #600000:   Batch Loss = 0.307709, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4268856346607208, Accuracy = 0.9528334140777588\n",
      "Training iter #900000:   Batch Loss = 0.237231, Accuracy = 0.996666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.345686674118042, Accuracy = 0.9582626223564148\n",
      "Training iter #1200000:   Batch Loss = 0.202694, Accuracy = 0.9940000176429749\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.33816003799438477, Accuracy = 0.9555479884147644\n",
      "Optimization Finished!\n",
      "Training time is: 755.1538782119751 seconds\n",
      "FINAL RESULT: Batch Loss = 0.3047422170639038, Accuracy = 0.9558873176574707\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.405074, Accuracy = 0.30000001192092896\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3697452545166016, Accuracy = 0.16864608228206635\n",
      "Training iter #300000:   Batch Loss = 0.425230, Accuracy = 0.9706666469573975\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.4996069669723511, Accuracy = 0.9345096945762634\n",
      "Training iter #600000:   Batch Loss = 0.360378, Accuracy = 0.9660000205039978\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44291579723358154, Accuracy = 0.9491007924079895\n",
      "Training iter #900000:   Batch Loss = 0.251930, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.36865290999412537, Accuracy = 0.9589412808418274\n",
      "Training iter #1200000:   Batch Loss = 0.202805, Accuracy = 0.9919999837875366\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3364206552505493, Accuracy = 0.9555479884147644\n",
      "Optimization Finished!\n",
      "Training time is: 892.9882638454437 seconds\n",
      "FINAL RESULT: Batch Loss = 0.2905753254890442, Accuracy = 0.9606379270553589\n",
      "------------------------\n",
      "FINAL RESULTS LIST:\n",
      "[86.01405668258667, 167.2383337020874, 250.78299975395203, 342.0062355995178, 422.47657561302185, 588.8813662528992, 665.4902803897858, 684.1167492866516, 755.1538782119751, 892.9892616271973]\n",
      "[0.7964031, 0.8758059, 0.8995589, 0.8890397, 0.9460468, 0.96199524, 0.9457075, 0.9409569, 0.9558873, 0.9606379]\n",
      "[0.94036096, 0.70517254, 0.71657467, 0.75291955, 0.4557973, 0.35652047, 0.43677756, 0.37667298, 0.30474222, 0.29057533]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 Part 2 of End-to-end training with the CNNLSTM model model 3.\n",
    "# Try to find the time and accuracy of the models for different sizes of the dataset \n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN LSTM model is written by myself\n",
    "# The credit of this code goes to Guillaume Chevalier under MIT License.\n",
    "# Python version 3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def cnn_LSTM_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu)\n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu)\n",
    "    conv2 = tf.transpose(conv2, [1, 0, 2])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, conv2, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "runing_time_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 200  # Loop 200 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = cnn_LSTM_net(x,weights,biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss, acc = sess.run(\n",
    "                [cost, accuracy], \n",
    "                feed_dict={\n",
    "                    x: X_test,\n",
    "                    y: one_hot(y_test)\n",
    "                }\n",
    "            )\n",
    "            test_losses.append(loss)\n",
    "            test_accuracies.append(acc)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "    runing_time_list.append(time.time() - start_time)\n",
    "    # Accuracy for test data\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    test_losses.append(final_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(final_loss)\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"FINAL RESULTS LIST:\")\n",
    "print(runing_time_list)\n",
    "print(accuracy_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save above results for manuscript plotting later.\n",
    "\n",
    "import pyexcel as pe\n",
    "\n",
    "sheet = pe.Sheet([[runing_time_list[i], accuracy_list[i],loss_list[i]] for i in range(0,len(runing_time_list))])\n",
    "sheet.save_as(\"CNN_LSTM_Mod_Time_ACC_LOSS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in the data\n",
      "(7352, 1)\n",
      "(7352, 128, 9)\n",
      "WARNING:tensorflow:From c:\\users\\edwar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-dbce89bcb383>:191: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "start training\n",
      "Training iter #1500:   Batch Loss = 2.410534, Accuracy = 0.14933332800865173\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3403375148773193, Accuracy = 0.16661010682582855\n",
      "Training iter #300000:   Batch Loss = 0.400829, Accuracy = 0.9819999933242798\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.626539945602417, Accuracy = 0.9155073165893555\n",
      "Training iter #600000:   Batch Loss = 0.326681, Accuracy = 0.9886666536331177\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5435868501663208, Accuracy = 0.9267051219940186\n",
      "Training iter #900000:   Batch Loss = 0.249756, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5159831047058105, Accuracy = 0.9328130483627319\n",
      "Training iter #1200000:   Batch Loss = 0.207193, Accuracy = 0.9946666955947876\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44152384996414185, Accuracy = 0.9379029273986816\n",
      "Training iter #1500000:   Batch Loss = 0.183540, Accuracy = 0.9933333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.44781213998794556, Accuracy = 0.9331523776054382\n",
      "Training iter #1800000:   Batch Loss = 0.153472, Accuracy = 0.9993333220481873\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3899304270744324, Accuracy = 0.940617561340332\n",
      "Training iter #2100000:   Batch Loss = 0.151778, Accuracy = 0.9913333058357239\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.366237998008728, Accuracy = 0.9402782320976257\n",
      "Training iter #2400000:   Batch Loss = 0.149646, Accuracy = 0.9866666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3309790790081024, Accuracy = 0.9446895122528076\n",
      "Training iter #2700000:   Batch Loss = 0.120634, Accuracy = 0.9959999918937683\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.3033648133277893, Accuracy = 0.9460468292236328\n",
      "Optimization Finished!\n",
      "Training time is: 3218.08304977417 seconds\n",
      "FINAL RESULT: Batch Loss = 0.32435375452041626, Accuracy = 0.9443501830101013\n",
      "Testing Accuracy: 94.43501830101013%\n",
      "\n",
      "Precision: 94.97778032107566%\n",
      "Recall: 94.43501866304716%\n",
      "f1_score: 94.49339541689905%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[459   5  30   1   0   1]\n",
      " [  0 443  28   0   0   0]\n",
      " [  3  10 407   0   0   0]\n",
      " [  0   0   0 434  57   0]\n",
      " [  0   0   0   0 532   0]\n",
      " [  0   0   0   2  27 508]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[15.575162    0.16966406  1.0179844   0.03393281  0.          0.03393281]\n",
      " [ 0.         15.032236    0.9501188   0.          0.          0.        ]\n",
      " [ 0.10179844  0.3393281  13.810656    0.          0.          0.        ]\n",
      " [ 0.          0.          0.         14.726841    1.9341704   0.        ]\n",
      " [ 0.          0.          0.          0.         18.052256    0.        ]\n",
      " [ 0.          0.          0.          0.06786563  0.916186   17.23787   ]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix to take a look\n",
    "# The structure based on Guillaume Chevalier https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "# The structure include read in of the input, formatting the input and the conversion of the one-hot encoding.\n",
    "# The CNN model is written by myself\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "# Pathes to dataset\n",
    "DATASET_PATH = \"UCI HAR Dataset/\"\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_for_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_for_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"read in the data\")\n",
    "\n",
    "# randomly shuffle the array\n",
    "def rand_shuffle_data(train_X, train_Y):\n",
    "    item_size = np.array(train_X).shape\n",
    "    sample_size = item_size[0]\n",
    "    np.random.seed(42)\n",
    "    value_list = np.arange(sample_size)\n",
    "    np.random.shuffle(value_list)\n",
    "    output_X = np.zeros(np.array(train_X).shape)\n",
    "    output_Y = np.zeros(np.array(train_Y).shape)\n",
    "    print(output_Y.shape)\n",
    "    cnt = 0\n",
    "    for idx in value_list:\n",
    "        output_X[cnt,:,:] = train_X[idx,:,:]\n",
    "        output_Y[cnt,:] = train_Y[idx,:]\n",
    "        cnt += 1\n",
    "    return value_list, output_X, output_Y\n",
    "\n",
    "def cnn_LSTM_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, [-1, n_steps, n_input]) \n",
    "    # convlution layers\n",
    "    conv1 = tf.layers.separable_conv1d(_X, 32, 5, activation=tf.nn.relu) \n",
    "    conv1 = tf.layers.max_pooling1d(conv1, 20, 2)\n",
    "    conv2 = tf.layers.separable_conv1d(conv1, 64, 5, activation=tf.nn.relu)\n",
    "    conv2 = tf.transpose(conv2, [1, 0, 2])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, conv2, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "percentage_list = [1]\n",
    "\n",
    "for percent_train in percentage_list:\n",
    "    \n",
    "    tf.reset_default_graph() # reset my graph\n",
    "    \n",
    "    rand_shuffle_idx, train_X, train_y = rand_shuffle_data(X_for_train, y_for_train)\n",
    "    pick_samples = int(round((np.array(X_for_train).shape[0])*percent_train))\n",
    "    X_train = X_for_train[0:pick_samples, :, :]\n",
    "    y_train = y_for_train[0:pick_samples,:]\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    # Input data and LSTM internal structure\n",
    "    training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "    test_data_count = len(X_test)  # 2947 testing series\n",
    "    n_height = 1\n",
    "    n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])\n",
    "    n_hidden = 32\n",
    "    n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.0025\n",
    "    lambda_loss_amount = 0.0015\n",
    "    training_iters = training_data_count * 400  # Loop 500 times on the dataset\n",
    "    batch_size = 1500\n",
    "    display_iter = 300000  # To show test set accuracy during training\n",
    "    \n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = cnn_LSTM_net(x,weights,biases)\n",
    "\n",
    "    # Loss, optimizer and evaluation\n",
    "    l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "    ) # L2 loss prevents this overkill neural network to overfit the data\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2# Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # To keep track of training's performance\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"start training\")\n",
    "    start_time = time.time()\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "    step = 1\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "        batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run(\n",
    "            [optimizer, cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: batch_xs, \n",
    "                y: batch_ys\n",
    "            }\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "        \n",
    "        loss2, acc2 = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss2)\n",
    "        test_accuracies.append(acc2)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "            # To not spam console, show training accuracy/loss in this \"if\"\n",
    "            print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "                  \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "\n",
    "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "                  \"Batch Loss = {}\".format(loss2) + \\\n",
    "                  \", Accuracy = {}\".format(acc2))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Training time is: \" + str(time.time() - start_time) + \" seconds\")\n",
    "\n",
    "    one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "        [pred, accuracy, cost],\n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"FINAL RESULT: \" + \\\n",
    "          \"Batch Loss = {}\".format(final_loss) + \\\n",
    "          \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    predictions = one_hot_predictions.argmax(1)\n",
    "    \n",
    "    print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "    print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    print(confusion_matrix)\n",
    "    normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "    print(normalised_confusion_matrix)\n",
    "    print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "    print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "    \n",
    "    # Plot Results: \n",
    "    width = 12\n",
    "    height = 12\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(\n",
    "        normalised_confusion_matrix, \n",
    "        interpolation='nearest', \n",
    "        cmap=plt.cm.rainbow\n",
    "    )\n",
    "    plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(n_classes)\n",
    "    plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "    plt.yticks(tick_marks, LABELS)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "f = plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "f.savefig(\"cnnlstm_conf.pdf\", bbox_inches='tight')\n",
    "f.savefig(\"cnnlstm_conf.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
